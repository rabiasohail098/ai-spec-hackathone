"use strict";(globalThis.webpackChunkbook_ui=globalThis.webpackChunkbook_ui||[]).push([[561],{570:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapter-6/autonomous-humanoid-capstone","title":"Chapter 6: The Autonomous Humanoid Capstone Project","description":"Overview","source":"@site/docs/chapter-6/autonomous-humanoid-capstone.mdx","sourceDirName":"chapter-6","slug":"/chapter-6/autonomous-humanoid-capstone","permalink":"/ai-spec-hackathone/docs/chapter-6/autonomous-humanoid-capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/rabiasohail098/ai-spec-kit-book/tree/main/docs/chapter-6/autonomous-humanoid-capstone.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Chapter 6: The Autonomous Humanoid Capstone Project","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Vision-Language-Action (VLA)","permalink":"/ai-spec-hackathone/docs/chapter-5/vision-language-action"},"next":{"title":"Chapter 7: AI for Robotics","permalink":"/ai-spec-hackathone/docs/chapter-7/ai-for-robotics"}}');var o=t(4848),i=t(8453),a=(t(9815),t(7561)),r=t(3113);const l={title:"Chapter 6: The Autonomous Humanoid Capstone Project",sidebar_position:6},c="Chapter 6: The Autonomous Humanoid Capstone Project",p={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Capstone Architecture Overview",id:"capstone-architecture-overview",level:2},{value:"Complete System Architecture",id:"complete-system-architecture",level:3},{value:"System Integration Challenges",id:"system-integration-challenges",level:3},{value:"Voice Command to Action Pipeline",id:"voice-command-to-action-pipeline",level:2},{value:"Complete Voice Processing System",id:"complete-voice-processing-system",level:3},{value:"Intent-Based Command Processing",id:"intent-based-command-processing",level:3},{value:"Environmental Perception System",id:"environmental-perception-system",level:2},{value:"Multi-Sensor Perception Integration",id:"multi-sensor-perception-integration",level:3},{value:"Path Planning and Navigation System",id:"path-planning-and-navigation-system",level:2},{value:"Advanced Navigation for Humanoids",id:"advanced-navigation-for-humanoids",level:3},{value:"Manipulation System for Humanoids",id:"manipulation-system-for-humanoids",level:2},{value:"Grasp Planning and Execution",id:"grasp-planning-and-execution",level:3},{value:"Complete Capstone System Integration",id:"complete-capstone-system-integration",level:2},{value:"Main Capstone Node",id:"main-capstone-node",level:3},{value:"Mission Planning and Execution",id:"mission-planning-and-execution",level:2},{value:"Hierarchical Task Planner",id:"hierarchical-task-planner",level:3},{value:"System Validation and Testing",id:"system-validation-and-testing",level:2},{value:"Comprehensive Testing Framework",id:"comprehensive-testing-framework",level:3},{value:"Deployment and Optimization Strategies",id:"deployment-and-optimization-strategies",level:2},{value:"Performance Optimization and Deployment",id:"performance-optimization-and-deployment",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Practice Questions",id:"practice-questions",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2}];function u(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-6-the-autonomous-humanoid-capstone-project",children:"Chapter 6: The Autonomous Humanoid Capstone Project"})}),"\n",(0,o.jsx)(a.A,{chapterId:"chapter-6"}),"\n",(0,o.jsx)(r.A,{chapterId:"chapter-6"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"The Autonomous Humanoid Capstone Project represents the culmination of all concepts explored throughout this course: from the foundational ROS 2 middleware to advanced NVIDIA Isaac perception, and from Vision-Language-Action integration to full autonomous operation. This chapter guides you through the complete implementation of an autonomous humanoid robot system capable of receiving voice commands, planning paths, navigating obstacles, identifying objects using computer vision, and manipulating them. This capstone project integrates all the technological components covered in previous chapters into a cohesive, functioning autonomous system."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate all previous course components into a complete autonomous humanoid system"}),"\n",(0,o.jsx)(n.li,{children:"Implement end-to-end voice-to-action pipeline for humanoid robots"}),"\n",(0,o.jsx)(n.li,{children:"Design and implement a complete mission planning and execution system"}),"\n",(0,o.jsx)(n.li,{children:"Deploy perception, navigation, and manipulation capabilities simultaneously"}),"\n",(0,o.jsx)(n.li,{children:"Test and validate the integrated autonomous system"}),"\n",(0,o.jsx)(n.li,{children:"Understand challenges and solutions in full-system integration"}),"\n",(0,o.jsx)(n.li,{children:"Develop debugging and optimization strategies for complex robotic systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"capstone-architecture-overview",children:"Capstone Architecture Overview"}),"\n",(0,o.jsx)(n.h3,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "User Interface Layer"\n        A[Voice Command] --\x3e B[Speech Recognition]\n        C[Visual Command] --\x3e D[Gesture Recognition]\n    end\n    \n    subgraph "Intelligence Layer"\n        B --\x3e E[Cognitive Planner]\n        D --\x3e E\n        F[Environmental Perception] --\x3e E\n        G[Robot State Monitor] --\x3e E\n    end\n    \n    subgraph "Perception Layer"\n        F --\x3e H[Camera Processing]\n        F --\x3e I[Lidar Processing] \n        F --\x3e J[IMU Processing]\n        F --\x3e K[Tactile Sensors]\n    end\n    \n    subgraph "Planning Layer"\n        E --\x3e L[Path Planner]\n        E --\x3e M[Manipulation Plan]\n        E --\x3e N[Task Scheduler]\n    end\n    \n    subgraph "Execution Layer"\n        L --\x3e O[Navigation Control]\n        M --\x3e P[Manipulation Control]\n        N --\x3e Q[State Machine]\n    end\n    \n    subgraph "Hardware Layer"\n        O --\x3e R[Humanoid Robot]\n        P --\x3e R\n        Q --\x3e R\n        R --\x3e F\n    end\n    \n    R --\x3e S[Sensors Feedback]\n    S --\x3e F\n'})}),"\n",(0,o.jsx)(n.h3,{id:"system-integration-challenges",children:"System Integration Challenges"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project addresses several complex integration challenges:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Temporal Synchronization"}),": Coordinating perception, planning, and action at appropriate frequencies"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Management"}),": Efficiently allocating computational resources across subsystems"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"State Consistency"}),": Maintaining consistent robot state across distributed components"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Failure Recovery"}),": Handling failures gracefully while maintaining system operation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Assurance"}),": Ensuring safe operation in dynamic human environments"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-to-action-pipeline",children:"Voice Command to Action Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"complete-voice-processing-system",children:"Complete Voice Processing System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# voice_pipeline.py\nimport openai\nimport speech_recognition as sr\nimport threading\nimport queue\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport time\n\n@dataclass\nclass VoiceCommand:\n    \"\"\"Represents a processed voice command\"\"\"\n    text: str\n    confidence: float\n    timestamp: float\n    intent: str\n    objects: List[str]\n    location: Optional[str]\n\nclass VoiceCommandProcessor:\n    def __init__(self, api_key: str):\n        openai.api_key = api_key\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.command_queue = queue.Queue()\n        self.is_listening = False\n        self.listening_thread = None\n        \n        # Initialize with environmental noise consideration\n        self.recognizer.energy_threshold = 300\n        self.recognizer.dynamic_energy_threshold = True\n        self.recognizer.pause_threshold = 0.8  # Adjust for natural speech\n\n    def start_listening(self):\n        \"\"\"Start the voice command listening loop\"\"\"\n        self.is_listening = True\n        self.listening_thread = threading.Thread(target=self._listen_loop)\n        self.listening_thread.daemon = True\n        self.listening_thread.start()\n        print(\"Voice command processor started\")\n\n    def stop_listening(self):\n        \"\"\"Stop the voice command listening loop\"\"\"\n        self.is_listening = False\n        if self.listening_thread:\n            self.listening_thread.join()\n        print(\"Voice command processor stopped\")\n\n    def _listen_loop(self):\n        \"\"\"Continuous listening loop with error handling\"\"\"\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source, duration=1.0)\n            print(\"Listening for commands... (Say 'stop' to quit)\")\n\n            while self.is_listening:\n                try:\n                    # Listen with timeout\n                    audio = self.recognizer.listen(source, timeout=5.0, phrase_time_limit=10.0)\n                    \n                    # Transcribe using Whisper\n                    transcript = self._transcribe_audio(audio)\n                    \n                    if transcript:\n                        # Process transcript for intent and entities\n                        command = self._process_transcript(transcript)\n                        \n                        # Add to processing queue\n                        self.command_queue.put(command)\n                        \n                        print(f\"Command recognized: {transcript} (Confidence: {command.confidence:.2f})\")\n\n                except sr.WaitTimeoutError:\n                    # Continue listening if no speech detected\n                    continue\n                except sr.RequestError:\n                    print(\"API unavailable - check connection and API key\")\n                except sr.UnknownValueError:\n                    # Could not understand audio\n                    print(\"Could not understand audio\")\n                except Exception as e:\n                    print(f\"Error in voice processing: {e}\")\n\n    def _transcribe_audio(self, audio) -> Optional[str]:\n        \"\"\"Transcribe audio using OpenAI Whisper\"\"\"\n        try:\n            # Save audio for Whisper API\n            audio_data = audio.get_raw_data()\n            with open(\"temp_audio.webm\", \"wb\") as f:\n                f.write(audio.get_wav_data())  # Convert to WAV format\n\n            # Transcribe with Whisper API\n            with open(\"temp_audio.webm\", \"rb\") as audio_file:\n                result = openai.Audio.transcribe(\n                    model=\"whisper-1\",\n                    file=audio_file,\n                    response_format=\"text\"\n                )\n            \n            return result.strip()\n        except Exception as e:\n            print(f\"Error transcribing audio: {e}\")\n            return None\n\n    def _process_transcript(self, transcript: str) -> VoiceCommand:\n        \"\"\"Process transcript to extract intent and entities\"\"\"\n        # Analyze intent\n        intent = self._classify_intent(transcript)\n        \n        # Extract objects\n        objects = self._extract_objects(transcript)\n        \n        # Extract location\n        location = self._extract_location(transcript)\n        \n        return VoiceCommand(\n            text=transcript,\n            confidence=0.9,  # Placeholder confidence\n            timestamp=time.time(),\n            intent=intent,\n            objects=objects,\n            location=location\n        )\n\n    def _classify_intent(self, text: str) -> str:\n        \"\"\"Classify the intent of the voice command\"\"\"\n        text_lower = text.lower()\n        \n        intent_keywords = {\n            'navigation': ['go to', 'navigate', 'move to', 'walk to', 'travel to'],\n            'manipulation': ['pick up', 'grasp', 'take', 'get', 'bring', 'place', 'put'],\n            'cleaning': ['clean', 'tidy', 'organize', 'clear'],\n            'search': ['find', 'locate', 'look for', 'where is'],\n            'assist': ['help', 'assist', 'can you', 'could you'],\n            'stop': ['stop', 'halt', 'cease', 'quit']\n        }\n        \n        for intent, keywords in intent_keywords.items():\n            if any(keyword in text_lower for keyword in keywords):\n                return intent\n        \n        return 'unknown'\n\n    def _extract_objects(self, text: str) -> List[str]:\n        \"\"\"Extract object references from text\"\"\"\n        # Common household objects that the robot should recognize\n        object_keywords = [\n            'cup', 'bottle', 'glass', 'plate', 'bowl', 'fork', 'spoon', 'knife',\n            'book', 'pen', 'pencil', 'paper', 'notebook', 'phone', 'tablet',\n            'chair', 'table', 'sofa', 'bed', 'desk', 'cabinet', 'drawer',\n            'ball', 'toy', 'box', 'bag', 'laptop', 'remote', 'keys', 'wallet'\n        ]\n        \n        found_objects = []\n        text_lower = text.lower()\n        \n        for obj in object_keywords:\n            if obj in text_lower:\n                found_objects.append(obj)\n        \n        return found_objects\n\n    def _extract_location(self, text: str) -> Optional[str]:\n        \"\"\"Extract location references from text\"\"\"\n        location_keywords = [\n            'kitchen', 'living room', 'bedroom', 'bathroom', 'office', \n            'dining room', 'hallway', 'garage', 'garden', 'outside',\n            'table', 'counter', 'desk', 'shelf', 'cabinet', 'bed', 'sofa'\n        ]\n        \n        text_lower = text.lower()\n        for location in location_keywords:\n            if location in text_lower:\n                return location\n        \n        return None\n\n# Example usage in capstone system\nclass CapstoneVoiceSystem:\n    def __init__(self, api_key: str):\n        self.processor = VoiceCommandProcessor(api_key)\n        self.active_commands = []\n        \n    def start_system(self):\n        \"\"\"Start the voice command system\"\"\"\n        self.processor.start_listening()\n        \n    def get_next_command(self) -> Optional[VoiceCommand]:\n        \"\"\"Get the next available command\"\"\"\n        try:\n            return self.processor.command_queue.get_nowait()\n        except queue.Empty:\n            return None\n    \n    def stop_system(self):\n        \"\"\"Stop the voice command system\"\"\"\n        self.processor.stop_listening()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"intent-based-command-processing",children:"Intent-Based Command Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# intent_processor.py\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport re\n\nclass TaskPriority(Enum):\n    HIGH = 3\n    MEDIUM = 2\n    LOW = 1\n\n@dataclass\nclass Task:\n    \"\"\"Represents an executable task\"\"\"\n    id: str\n    type: str\n    description: str\n    parameters: Dict\n    priority: TaskPriority\n    dependencies: List[str] = None\n\nclass IntentProcessor:\n    def __init__(self):\n        self.task_counter = 0\n        self.current_tasks = []\n    \n    def process_command(self, command: VoiceCommand) -> List[Task]:\n        \"\"\"Process voice command and generate executable tasks\"\"\"\n        intent = command.intent\n        objects = command.objects\n        location = command.location\n        \n        if intent == 'navigation':\n            return self._process_navigation_command(command)\n        elif intent == 'manipulation':\n            return self._process_manipulation_command(command)\n        elif intent == 'cleaning':\n            return self._process_cleaning_command(command)\n        elif intent == 'search':\n            return self._process_search_command(command)\n        else:\n            # For unknown commands, try to parse based on content\n            return self._infer_command(command)\n    \n    def _process_navigation_command(self, command: VoiceCommand) -> List[Task]:\n        \"\"\"Process navigation-related commands\"\"\"\n        tasks = []\n        \n        # Example: \"Go to the kitchen\"\n        if command.location:\n            tasks.append(self._create_task(\n                'navigation',\n                f'Navigate to {command.location}',\n                {'target_location': command.location},\n                TaskPriority.MEDIUM\n            ))\n        else:\n            # Try to infer destination from objects\n            for obj in command.objects:\n                tasks.append(self._create_task(\n                    'search_and_navigate',\n                    f'Find and navigate to {obj}',\n                    {'target_object': obj},\n                    TaskPriority.MEDIUM\n                ))\n        \n        return tasks\n    \n    def _process_manipulation_command(self, command: VoiceCommand) -> List[Task]:\n        \"\"\"Process manipulation-related commands\"\"\"\n        tasks = []\n        \n        for obj in command.objects:\n            # Approach object\n            tasks.append(self._create_task(\n                'approach_object',\n                f'Approach {obj}',\n                {'target_object': obj},\n                TaskPriority.HIGH\n            ))\n            \n            # Grasp object\n            tasks.append(self._create_task(\n                'grasp_object',\n                f'Grasp {obj}',\n                {'target_object': obj},\n                TaskPriority.HIGH\n            ))\n        \n        # If there's a destination (e.g., \"put the cup on the table\")\n        location = self._extract_destination(command.text)\n        if location:\n            for obj in command.objects:\n                tasks.append(self._create_task(\n                    'navigate_to_place',\n                    f'Navigate to {location} and place {obj}',\n                    {'target_location': location, 'object': obj},\n                    TaskPriority.MEDIUM\n                ))\n        \n        return tasks\n    \n    def _process_cleaning_command(self, command: VoiceCommand) -> List[Task]:\n        \"\"\"Process cleaning-related commands\"\"\"\n        tasks = []\n        \n        # Scan area for cleaning targets\n        tasks.append(self._create_task(\n            'scan_cleaning_area',\n            'Scan area for cleaning targets',\n            {'scan_radius': 3.0, 'area': command.location},\n            TaskPriority.MEDIUM\n        ))\n        \n        # Plan cleaning path\n        tasks.append(self._create_task(\n            'plan_cleaning_path',\n            'Plan systematic cleaning path',\n            {'coverage_type': 'grid', 'area': command.location},\n            TaskPriority.MEDIUM\n        ))\n        \n        # Execute cleaning\n        tasks.append(self._create_task(\n            'execute_cleaning',\n            'Execute planned cleaning path',\n            {'area': command.location},\n            TaskPriority.LOW\n        ))\n        \n        return tasks\n    \n    def _process_search_command(self, command: VoiceCommand) -> List[Task]:\n        \"\"\"Process search-related commands\"\"\"\n        tasks = []\n        \n        for obj in command.objects:\n            # Navigate and search for object\n            tasks.append(self._create_task(\n                'search_for_object',\n                f'Search for {obj}',\n                {'target_object': obj, 'search_area': command.location},\n                TaskPriority.HIGH\n            ))\n        \n        return tasks\n    \n    def _infer_command(self, command: VoiceCommand) -> List[Task]:\n        \"\"\"Infer command type when intent is unknown\"\"\"\n        # Look for patterns in the text\n        text_lower = command.text.lower()\n        \n        if any(word in text_lower for word in ['help', 'can you', 'could you']):\n            # Likely an assistive command\n            return self._process_assistive_command(command)\n        \n        # Default to search if objects mentioned\n        if command.objects:\n            return self._process_search_command(command)\n        \n        # Default to navigation if location mentioned\n        if command.location:\n            return self._process_navigation_command(command)\n        \n        # Unknown command\n        return []\n    \n    def _process_assistive_command(self, command: VoiceCommand) -> List[Task]:\n        \"\"\"Process assistive commands\"\"\"\n        # For now, assume assistive commands involve manipulation\n        return self._process_manipulation_command(command)\n    \n    def _extract_destination(self, text: str) -> Optional[str]:\n        \"\"\"Extract destination from phrases like 'put X on Y'\"\"\"\n        # Pattern: put/place X on/in/at Y\n        patterns = [\n            r'put\\s+.*?\\s+on\\s+(\\w+)',\n            r'place\\s+.*?\\s+on\\s+(\\w+)',\n            r'put\\s+.*?\\s+in\\s+(\\w+)',\n            r'place\\s+.*?\\s+in\\s+(\\w+)',\n            r'to\\s+(\\w+)',  # \"take this to kitchen\"\n        ]\n        \n        text_lower = text.lower()\n        for pattern in patterns:\n            match = re.search(pattern, text_lower)\n            if match:\n                destination = match.group(1)\n                # Verify it's a location (not an object)\n                if destination in ['table', 'kitchen', 'bedroom', 'living room', 'desk', 'cabinet']:\n                    return destination\n        \n        return None\n    \n    def _create_task(self, task_type: str, description: str, \n                     parameters: Dict, priority: TaskPriority) -> Task:\n        \"\"\"Create a new task with unique ID\"\"\"\n        self.task_counter += 1\n        task_id = f\"task_{self.task_counter:04d}\"\n        \n        return Task(\n            id=task_id,\n            type=task_type,\n            description=description,\n            parameters=parameters,\n            priority=priority\n        )\n"})}),"\n",(0,o.jsx)(n.h2,{id:"environmental-perception-system",children:"Environmental Perception System"}),"\n",(0,o.jsx)(n.h3,{id:"multi-sensor-perception-integration",children:"Multi-Sensor Perception Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# perception_system.py\nimport cv2\nimport numpy as np\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu, PointCloud2\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PointStamped\nfrom visualization_msgs.msg import MarkerArray\nfrom cv_bridge import CvBridge\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\nimport tf2_ros\nimport open3d as o3d\n\nclass MultiSensorPerceptionSystem(Node):\n    def __init__(self):\n        super().__init__(\'multi_sensor_perception\')\n        \n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n        \n        # Create subscribers for all sensors\n        self.rgb_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10\n        )\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.lidar_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10\n        )\n        \n        # Publishers\n        self.object_pub = self.create_publisher(\n            String, \'/detected_objects\', 10\n        )\n        self.map_pub = self.create_publisher(\n            PointCloud2, \'/environment_map\', 10\n        )\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n        \n        # Internal state\n        self.current_rgb = None\n        self.current_depth = None\n        self.current_lidar = None\n        self.current_imu = None\n        self.object_detector = self._initialize_object_detector()\n        self.spatial_mapper = SpatialMapper()\n        \n        # Processing timer\n        self.process_timer = self.create_timer(0.1, self.process_sensors)\n        \n        self.get_logger().info(\'Multi-sensor perception system initialized\')\n\n    def _initialize_object_detector(self):\n        """Initialize object detection system"""\n        # This would initialize a YOLO, DETR, or similar detector\n        # For now, using placeholder\n        class PlaceholderDetector:\n            def detect(self, image):\n                # Return mock detections\n                h, w = image.shape[:2]\n                return [\n                    {\'class\': \'bottle\', \'bbox\': [w//2-25, h//2-50, w//2+25, h//2+50], \'confidence\': 0.9}\n                ]\n        return PlaceholderDetector()\n\n    def rgb_callback(self, msg):\n        """Process RGB camera data"""\n        try:\n            self.current_rgb = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f\'Error processing RGB: {e}\')\n\n    def depth_callback(self, msg):\n        """Process depth camera data"""\n        try:\n            self.current_depth = self.cv_bridge.imgmsg_to_cv2(msg, "32FC1")\n        except Exception as e:\n            self.get_logger().error(f\'Error processing depth: {e}\')\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data"""\n        self.current_lidar = msg\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        self.current_imu = msg\n\n    def process_sensors(self):\n        """Process all sensor data and create unified perception"""\n        if self.current_rgb is not None and self.current_depth is not None:\n            # Perform object detection\n            detections = self.object_detector.detect(self.current_rgb)\n            \n            # Create 3D positions for detected objects\n            detected_objects = []\n            for detection in detections:\n                obj_3d_pos = self._convert_2d_to_3d(\n                    detection[\'bbox\'], \n                    self.current_depth\n                )\n                \n                detected_objects.append({\n                    \'class\': detection[\'class\'],\n                    \'position\': obj_3d_pos,\n                    \'bbox\': detection[\'bbox\'],\n                    \'confidence\': detection[\'confidence\']\n                })\n            \n            # Publish detected objects\n            self._publish_detected_objects(detected_objects)\n            \n            # Update spatial map\n            self.spatial_mapper.update_with_detections(\n                detected_objects, \n                self.current_rgb, \n                self.current_depth\n            )\n            \n            # Publish updated map\n            self._publish_environment_map()\n\n    def _convert_2d_to_3d(self, bbox, depth_image):\n        """Convert 2D bounding box to 3D position using depth"""\n        x1, y1, x2, y2 = bbox\n        center_x = (x1 + x2) // 2\n        center_y = (y1 + y2) // 2\n        \n        # Get depth at center point\n        depth = depth_image[center_y, center_x]\n        \n        # Convert to 3D coordinates (simplified)\n        # This would require proper camera calibration in reality\n        x_3d = (center_x - 320) * depth / 600  # Placeholder focal length\n        y_3d = (center_y - 240) * depth / 600\n        z_3d = depth\n        \n        return [x_3d, y_3d, z_3d]\n\n    def _publish_detected_objects(self, objects):\n        """Publish detected objects"""\n        obj_string = String()\n        obj_string.data = str([obj[\'class\'] for obj in objects])\n        self.object_pub.publish(obj_string)\n\n    def _publish_environment_map(self):\n        """Publish environment map"""\n        # This would publish a real PointCloud2 message\n        pass\n\nclass SpatialMapper:\n    """Maintains spatial understanding of the environment"""\n    def __init__(self):\n        self.map_points = []\n        self.objects = {}\n        self.visited_regions = set()\n        \n    def update_with_detections(self, detections, rgb_image, depth_image):\n        """Update spatial map with new detections"""\n        for detection in detections:\n            obj_id = detection[\'class\'] + str(len(self.map_points))\n            \n            # Update object tracking\n            if obj_id in self.objects:\n                # Update existing object position with temporal filtering\n                old_pos = self.objects[obj_id][\'position\']\n                new_pos = detection[\'position\']\n                filtered_pos = self._temporal_filter(old_pos, new_pos)\n                self.objects[obj_id][\'position\'] = filtered_pos\n            else:\n                # Add new object\n                self.objects[obj_id] = detection\n        \n        # Update spatial map with new points\n        self._update_point_cloud(detections)\n    \n    def _temporal_filter(self, old_pos, new_pos, alpha=0.7):\n        """Simple temporal filtering for object positions"""\n        return [alpha * old_val + (1 - alpha) * new_val \n                for old_val, new_val in zip(old_pos, new_pos)]\n    \n    def _update_point_cloud(self, detections):\n        """Update the spatial point cloud with new information"""\n        # This would add points to a maintained 3D map\n        pass\n    \n    def get_nearest_object(self, obj_class, reference_point):\n        """Get nearest object of specified class to reference point"""\n        min_dist = float(\'inf\')\n        nearest_obj = None\n        \n        for obj_id, obj_data in self.objects.items():\n            if obj_data[\'class\'] == obj_class:\n                dist = self._euclidean_distance(\n                    obj_data[\'position\'], \n                    reference_point\n                )\n                if dist < min_dist:\n                    min_dist = dist\n                    nearest_obj = obj_data\n        \n        return nearest_obj\n    \n    def _euclidean_distance(self, point1, point2):\n        """Calculate Euclidean distance between two 3D points"""\n        return np.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n'})}),"\n",(0,o.jsx)(n.h2,{id:"path-planning-and-navigation-system",children:"Path Planning and Navigation System"}),"\n",(0,o.jsx)(n.h3,{id:"advanced-navigation-for-humanoids",children:"Advanced Navigation for Humanoids"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# navigation_system.py\nimport numpy as np\nimport heapq\nfrom typing import List, Tuple, Dict, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass Node:\n    """Node for path planning"""\n    x: float\n    y: float\n    z: float  # For 3D navigation\n    cost: float\n    heuristic: float\n    parent: Optional[\'Node\'] = None\n    \n    def __lt__(self, other):\n        return (self.cost + self.heuristic) < (other.cost + other.heuristic)\n\nclass HumanoidPathPlanner:\n    def __init__(self):\n        self.map_resolution = 0.1  # meters per cell\n        self.step_height_limit = 0.15  # maximum step height for humanoid\n        self.step_width_limit = 0.4   # maximum step width\n        self.balance_margin = 0.1     # safety margin for balance\n        self.collision_threshold = 0.3 # minimum distance from obstacles\n        \n    def plan_path(self, start: Tuple[float, float, float], \n                  goal: Tuple[float, float, float], \n                  occupancy_grid: np.ndarray) -> Optional[List[Tuple[float, float, float]]]:\n        """Plan path for humanoid considering physical constraints"""\n        \n        # Convert continuous coordinates to grid coordinates\n        start_grid = self._world_to_grid(start)\n        goal_grid = self._world_to_grid(goal)\n        \n        # Check if start and goal are valid\n        if not self._is_valid_position(start_grid, occupancy_grid):\n            return None\n        if not self._is_valid_position(goal_grid, occupancy_grid):\n            return None\n        \n        # Run A* pathfinding with humanoid constraints\n        path = self._a_star_search(start_grid, goal_grid, occupancy_grid)\n        \n        if path is None:\n            return None\n        \n        # Convert back to world coordinates\n        world_path = [self._grid_to_world(pos) for pos in path]\n        \n        # Smooth the path considering humanoid dynamics\n        smoothed_path = self._smooth_path(world_path, occupancy_grid)\n        \n        return smoothed_path\n    \n    def _a_star_search(self, start: Tuple[int, int, int], \n                       goal: Tuple[int, int, int], \n                       occupancy_grid: np.ndarray) -> Optional[List[Tuple[int, int, int]]]:\n        """A* pathfinding algorithm with humanoid constraints"""\n        \n        open_set = []\n        heapq.heappush(open_set, Node(start[0], start[1], start[2], 0, 0))\n        \n        closed_set = set()\n        g_costs = {start: 0}\n        \n        while open_set:\n            current = heapq.heappop(open_set)\n            \n            # Convert back to tuple for hashing\n            current_pos = (current.x, current.y, current.z)\n            \n            if current_pos == goal:\n                # Reconstruct path\n                path = []\n                node = current\n                while node:\n                    path.append((int(node.x), int(node.y), int(node.z)))\n                    node = node.parent\n                return path[::-1]\n            \n            if current_pos in closed_set:\n                continue\n            \n            closed_set.add(current_pos)\n            \n            # Generate neighbors (considering humanoid step constraints)\n            neighbors = self._get_valid_neighbors(current_pos, occupancy_grid)\n            \n            for neighbor_pos in neighbors:\n                neighbor_x, neighbor_y, neighbor_z = neighbor_pos\n                \n                # Calculate movement cost\n                movement_cost = self._calculate_movement_cost(\n                    current_pos, neighbor_pos, occupancy_grid\n                )\n                \n                tentative_g_cost = g_costs[current_pos] + movement_cost\n                \n                if neighbor_pos in g_costs and tentative_g_cost >= g_costs[neighbor_pos]:\n                    continue\n                \n                g_costs[neighbor_pos] = tentative_g_cost\n                heuristic = self._calculate_heuristic(neighbor_pos, goal)\n                \n                neighbor_node = Node(\n                    neighbor_x, neighbor_y, neighbor_z,\n                    tentative_g_cost, heuristic,\n                    current\n                )\n                \n                heapq.heappush(open_set, neighbor_node)\n        \n        return None  # No path found\n    \n    def _get_valid_neighbors(self, position: Tuple[int, int, int], \n                            occupancy_grid: np.ndarray) -> List[Tuple[int, int, int]]:\n        """Get valid neighboring positions for humanoid"""\n        x, y, z = position\n        neighbors = []\n        \n        # Define possible movement directions (considering humanoid constraints)\n        # We\'ll use 3D movements for full navigation\n        possible_moves = [\n            # Horizontal moves (4-connected)\n            (1, 0, 0), (-1, 0, 0), (0, 1, 0), (0, -1, 0),\n            # Diagonal moves (4-connected)\n            (1, 1, 0), (1, -1, 0), (-1, 1, 0), (-1, -1, 0),\n            # Vertical moves (for stairs/steps)\n            (0, 0, 1), (0, 0, -1),\n            # Combinations for complex terrain\n            (1, 0, 1), (-1, 0, 1), (0, 1, 1), (0, -1, 1),\n            (1, 0, -1), (-1, 0, -1), (0, 1, -1), (0, -1, -1),\n        ]\n        \n        for dx, dy, dz in possible_moves:\n            nx, ny, nz = x + dx, y + dy, z + dz\n            \n            # Check bounds\n            if (0 <= nx < occupancy_grid.shape[0] and \n                0 <= ny < occupancy_grid.shape[1] and \n                0 <= nz < occupancy_grid.shape[2] and \n                self._is_valid_position((nx, ny, nz), occupancy_grid)):\n                \n                # Check humanoid-specific constraints\n                if self._is_valid_humanoid_move((x, y, z), (nx, ny, nz)):\n                    neighbors.append((nx, ny, nz))\n        \n        return neighbors\n    \n    def _is_valid_position(self, pos: Tuple[int, int, int], \n                          occupancy_grid: np.ndarray) -> bool:\n        """Check if a position is valid (not occupied)"""\n        x, y, z = pos\n        \n        if (0 <= x < occupancy_grid.shape[0] and \n            0 <= y < occupancy_grid.shape[1] and \n            0 <= z < occupancy_grid.shape[2]):\n            \n            return occupancy_grid[x, y, z] == 0  # 0 = free space, 1 = obstacle\n        return False\n    \n    def _is_valid_humanoid_move(self, from_pos: Tuple[float, float, float], \n                               to_pos: Tuple[float, float, float]) -> bool:\n        """Check if move is valid for humanoid based on physical constraints"""\n        from_x, from_y, from_z = from_pos\n        to_x, to_y, to_z = to_pos\n        \n        # Calculate step dimensions\n        step_width = np.sqrt((to_x - from_x)**2 + (to_y - from_y)**2)\n        step_height = abs(to_z - from_z)\n        \n        # Check against humanoid constraints\n        if step_width > self.step_width_limit:\n            return False\n        if step_height > self.step_height_limit:\n            return False\n        \n        return True\n    \n    def _calculate_movement_cost(self, from_pos: Tuple[int, int, int], \n                                to_pos: Tuple[int, int, int], \n                                occupancy_grid: np.ndarray) -> float:\n        """Calculate movement cost based on terrain and obstacles"""\n        # Base cost is distance\n        dx = to_pos[0] - from_pos[0]\n        dy = to_pos[1] - from_pos[1]\n        dz = to_pos[2] - from_pos[2]\n        \n        distance_cost = np.sqrt(dx*dx + dy*dy + dz*dz)\n        \n        # Add penalty for proximity to obstacles\n        obstacle_penalty = self._calculate_obstacle_penalty(to_pos, occupancy_grid)\n        \n        return distance_cost + obstacle_penalty\n    \n    def _calculate_obstacle_penalty(self, pos: Tuple[int, int, int], \n                                   occupancy_grid: np.ndarray) -> float:\n        """Calculate penalty based on proximity to obstacles"""\n        penalty = 0.0\n        \n        # Check surrounding cells for obstacles\n        for dx in [-1, 0, 1]:\n            for dy in [-1, 0, 1]:\n                for dz in [-1, 0, 1]:\n                    if dx == 0 and dy == 0 and dz == 0:\n                        continue\n                    \n                    nx, ny, nz = pos[0] + dx, pos[1] + dy, pos[2] + dz\n                    \n                    if (0 <= nx < occupancy_grid.shape[0] and \n                        0 <= ny < occupancy_grid.shape[1] and \n                        0 <= nz < occupancy_grid.shape[2]):\n                        \n                        if occupancy_grid[nx, ny, nz] > 0:  # Obstacle\n                            distance = np.sqrt(dx*dx + dy*dy + dz*dz)\n                            penalty += 1.0 / (distance + 0.1)  # Closer obstacles = higher penalty\n        \n        return penalty\n    \n    def _calculate_heuristic(self, pos: Tuple[int, int, int], \n                            goal: Tuple[int, int, int]) -> float:\n        """Calculate heuristic (estimated cost to goal)"""\n        dx = goal[0] - pos[0]\n        dy = goal[1] - pos[1]\n        dz = goal[2] - pos[2]\n        \n        return np.sqrt(dx*dx + dy*dy + dz*dz)\n    \n    def _smooth_path(self, path: List[Tuple[float, float, float]], \n                     occupancy_grid: np.ndarray) -> List[Tuple[float, float, float]]:\n        """Smooth path considering humanoid dynamics"""\n        if len(path) < 3:\n            return path\n        \n        smoothed_path = [path[0]]\n        \n        i = 0\n        while i < len(path) - 1:\n            j = i + 1\n            # Try to connect further points if possible\n            while j < len(path) - 1:\n                if self._is_direct_path_valid(path[i], path[j], occupancy_grid):\n                    j += 1\n                else:\n                    break\n            \n            # Add the last valid point\n            smoothed_path.append(path[j-1])\n            i = j - 1\n        \n        return smoothed_path\n    \n    def _is_direct_path_valid(self, start: Tuple[float, float, float], \n                             end: Tuple[float, float, float], \n                             occupancy_grid: np.ndarray) -> bool:\n        """Check if direct path between two points is collision-free"""\n        # Sample points along the line and check for collisions\n        num_samples = max(int(np.linalg.norm(np.array(end) - np.array(start)) / self.map_resolution), 10)\n        \n        for i in range(num_samples + 1):\n            t = i / num_samples\n            sample_pos = tuple(\n                start[j] + t * (end[j] - start[j]) for j in range(3)\n            )\n            \n            grid_pos = self._world_to_grid(sample_pos)\n            if (not self._is_valid_position(grid_pos, occupancy_grid) or \n                not self._is_valid_humanoid_move(start, sample_pos)):\n                return False\n        \n        return True\n    \n    def _world_to_grid(self, world_pos: Tuple[float, float, float]) -> Tuple[int, int, int]:\n        """Convert world coordinates to grid coordinates"""\n        x, y, z = world_pos\n        return (int(x / self.map_resolution), \n                int(y / self.map_resolution), \n                int(z / self.map_resolution))\n    \n    def _grid_to_world(self, grid_pos: Tuple[int, int, int]) -> Tuple[float, float, float]:\n        """Convert grid coordinates to world coordinates"""\n        x, y, z = grid_pos\n        return (x * self.map_resolution, \n                y * self.map_resolution, \n                z * self.map_resolution)\n\nclass NavigationController:\n    """Controls the humanoid\'s movement along planned paths"""\n    def __init__(self):\n        self.path = []\n        self.current_waypoint_idx = 0\n        self.arrival_threshold = 0.3  # meters\n        self.balance_controller = BalanceController()\n        \n    def set_path(self, path: List[Tuple[float, float, float]]):\n        """Set new path for navigation"""\n        self.path = path\n        self.current_waypoint_idx = 0\n        print(f"Set path with {len(path)} waypoints")\n    \n    def update_navigation(self, current_pose: Tuple[float, float, float, float, float, float, float]):\n        """Update navigation based on current pose (x, y, z, qx, qy, qz, qw)"""\n        if not self.path or self.current_waypoint_idx >= len(self.path):\n            return None  # No path to follow or completed\n        \n        # Get current position\n        current_pos = current_pose[:3]  # x, y, z\n        target_pos = self.path[self.current_waypoint_idx]\n        \n        # Check if we\'ve reached current waypoint\n        distance = np.linalg.norm(np.array(current_pos) - np.array(target_pos))\n        \n        if distance < self.arrival_threshold:\n            # Move to next waypoint\n            self.current_waypoint_idx += 1\n            if self.current_waypoint_idx >= len(self.path):\n                print("Navigation completed!")\n                return "NAVIGATION_COMPLETED"\n        \n        if self.current_waypoint_idx < len(self.path):\n            next_target = self.path[self.current_waypoint_idx]\n            return self._calculate_navigation_command(current_pos, next_target)\n        \n        return None\n    \n    def _calculate_navigation_command(self, current_pos: Tuple[float, float, float], \n                                    target_pos: Tuple[float, float, float]):\n        """Calculate navigation command to reach target"""\n        # Calculate desired direction\n        direction = np.array(target_pos) - np.array(current_pos)\n        distance = np.linalg.norm(direction)\n        \n        # Normalize direction\n        if distance > 0:\n            direction = direction / distance\n        \n        # Create navigation command\n        cmd_vel = {\n            \'linear_x\': min(direction[0] * 0.3, 0.3),  # Limit speed\n            \'linear_y\': min(direction[1] * 0.3, 0.3),\n            \'linear_z\': min(direction[2] * 0.1, 0.1),  # Vertical motion limited\n            \'angular_z\': 0.0  # For now, assume perfect orientation\n        }\n        \n        # Apply balance corrections\n        cmd_vel = self.balance_controller.apply_balance_corrections(cmd_vel)\n        \n        return cmd_vel\n\nclass BalanceController:\n    """Maintains balance during humanoid navigation"""\n    def __init__(self):\n        self.com_height = 0.8  # Center of mass height\n        self.balance_margin = 0.1  # Safety margin\n        \n    def apply_balance_corrections(self, cmd_vel: Dict):\n        """Apply balance corrections to navigation commands"""\n        # This would integrate with actual balance control system\n        # For now, return commands with potential modifications\n        return cmd_vel\n'})}),"\n",(0,o.jsx)(n.h2,{id:"manipulation-system-for-humanoids",children:"Manipulation System for Humanoids"}),"\n",(0,o.jsx)(n.h3,{id:"grasp-planning-and-execution",children:"Grasp Planning and Execution"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# manipulation_system.py\nimport numpy as np\nfrom typing import List, Tuple, Dict, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass GraspConfiguration:\n    """Represents a grasp configuration"""\n    position: Tuple[float, float, float]\n    orientation: Tuple[float, float, float, float]  # quaternion (x, y, z, w)\n    joint_angles: List[float]\n    grasp_type: str  # \'power\', \'precision\', etc.\n    approach_direction: Tuple[float, float, float]\n\nclass GraspPlanner:\n    """Plans grasps for humanoid robot hands"""\n    def __init__(self):\n        self.hand_dof = 15  # Example: 5 fingers \xd7 3 joints each\n        self.grasp_database = self._load_grasp_database()\n        \n    def _load_grasp_database(self) -> Dict[str, List[GraspConfiguration]]:\n        """Load pre-computed grasp configurations for common objects"""\n        # In reality, this would be loaded from a database or computed\n        return {\n            \'bottle\': [\n                GraspConfiguration(\n                    position=(0, 0, 0.15),  # 15cm up from base\n                    orientation=(0, 0, 0, 1),  # upright\n                    joint_angles=[0.0] * self.hand_dof,\n                    grasp_type=\'cylindrical\',\n                    approach_direction=(0, 0, 1)  # from above\n                )\n            ],\n            \'cup\': [\n                GraspConfiguration(\n                    position=(0, 0, 0.08),  # 8cm up from base\n                    orientation=(0, 0, 0, 1),\n                    joint_angles=[0.0] * self.hand_dof,\n                    grasp_type=\'cylindrical\',\n                    approach_direction=(0, 0, 1)\n                )\n            ],\n            \'book\': [\n                GraspConfiguration(\n                    position=(0, 0, 0.01),  # near the center\n                    orientation=(0, 1, 0, 0),  # rotated 180\xb0 around Y\n                    joint_angles=[0.0] * self.hand_dof,\n                    grasp_type=\'edge\',\n                    approach_direction=(1, 0, 0)  # from the side\n                )\n            ]\n        }\n    \n    def plan_grasp(self, object_info: Dict) -> Optional[GraspConfiguration]:\n        """Plan grasp for a detected object"""\n        obj_class = object_info.get(\'class\', \'unknown\')\n        obj_position = object_info.get(\'position\', (0, 0, 0))\n        obj_dimensions = object_info.get(\'dimensions\', (0.1, 0.1, 0.1))\n        \n        if obj_class in self.grasp_database:\n            # Get the first available grasp for this object type\n            base_grasp = self.grasp_database[obj_class][0]\n            \n            # Adjust grasp position based on actual object position\n            adjusted_position = (\n                obj_position[0] + base_grasp.position[0],\n                obj_position[1] + base_grasp.position[1],\n                obj_position[2] + base_grasp.position[2]\n            )\n            \n            # Adjust approach direction based on object orientation\n            adjusted_approach = self._adjust_approach_direction(\n                base_grasp.approach_direction,\n                object_info.get(\'orientation\', (0, 0, 0, 1))\n            )\n            \n            return GraspConfiguration(\n                position=adjusted_position,\n                orientation=base_grasp.orientation,\n                joint_angles=base_grasp.joint_angles,\n                grasp_type=base_grasp.grasp_type,\n                approach_direction=adjusted_approach\n            )\n        \n        # If no predefined grasp, try generic approach\n        return self._generic_grasp_plan(object_info)\n    \n    def _adjust_approach_direction(self, base_direction: Tuple[float, float, float], \n                                  object_orientation: Tuple[float, float, float, float]) -> Tuple[float, float, float]:\n        """Adjust approach direction based on object orientation"""\n        # This would implement coordinate transformation\n        # For now, return base direction\n        return base_direction\n    \n    def _generic_grasp_plan(self, object_info: Dict) -> Optional[GraspConfiguration]:\n        """Plan generic grasp when object-specific grasp is not available"""\n        obj_class = object_info.get(\'class\', \'unknown\')\n        obj_position = object_info.get(\'position\', (0, 0, 0))\n        obj_dimensions = object_info.get(\'dimensions\', (0.1, 0.1, 0.1))\n        \n        # Determine grasp type based on object size and shape\n        largest_dim = max(obj_dimensions)\n        \n        if largest_dim > 0.2:  # Large object\n            grasp_type = \'power\'\n            approach_dir = (0, 0, 1)  # From above\n        else:  # Small object\n            grasp_type = \'precision\'\n            approach_dir = (1, 0, 0)  # From the side\n        \n        # Estimate good grasp position (center of object)\n        grasp_pos = obj_position\n        \n        return GraspConfiguration(\n            position=grasp_pos,\n            orientation=(0, 0, 0, 1),  # Default orientation\n            joint_angles=[0.0] * self.hand_dof,\n            grasp_type=grasp_type,\n            approach_direction=approach_dir\n        )\n\nclass ManipulationController:\n    """Controls the humanoid\'s manipulation execution"""\n    def __init__(self):\n        self.arm_controller = ArmController()\n        self.gripper_controller = GripperController()\n        self.grasp_planner = GraspPlanner()\n        \n    def execute_grasp(self, object_info: Dict) -> bool:\n        """Execute grasp action for the specified object"""\n        print(f"Planning grasp for {object_info.get(\'class\', \'unknown\')} at {object_info.get(\'position\')}")\n        \n        # Plan the grasp\n        grasp_config = self.grasp_planner.plan_grasp(object_info)\n        \n        if not grasp_config:\n            print("Could not plan grasp for object")\n            return False\n        \n        # Execute approach motion\n        approach_success = self._execute_approach_motion(grasp_config)\n        \n        if not approach_success:\n            print("Approach motion failed")\n            return False\n        \n        # Execute actual grasp\n        grasp_success = self._execute_grasp_motion(grasp_config)\n        \n        if not grasp_success:\n            print("Grasp execution failed")\n            # Try to recover\n            self._try_grasp_recovery(object_info, grasp_config)\n            return False\n        \n        print("Successfully grasped object!")\n        return True\n    \n    def _execute_approach_motion(self, grasp_config: GraspConfiguration) -> bool:\n        """Execute approach motion to grasp position"""\n        # Calculate approach position (slightly above/before grasp point)\n        approach_pos = (\n            grasp_config.position[0] + grasp_config.approach_direction[0] * 0.1,\n            grasp_config.position[1] + grasp_config.approach_direction[1] * 0.1,\n            grasp_config.position[2] + grasp_config.approach_direction[2] * 0.1\n        )\n        \n        # Move arm to approach position\n        approach_success = self.arm_controller.move_to_position(\n            approach_pos, grasp_config.orientation\n        )\n        \n        if not approach_success:\n            return False\n        \n        # Execute fine approach to grasp position\n        final_approach_success = self.arm_controller.move_to_position(\n            grasp_config.position, grasp_config.orientation\n        )\n        \n        return final_approach_success\n    \n    def _execute_grasp_motion(self, grasp_config: GraspConfiguration) -> bool:\n        """Execute the actual grasp motion"""\n        # Close gripper with appropriate force\n        grasp_success = self.gripper_controller.grasp_with_force(50.0)  # 50N force\n        \n        if not grasp_success:\n            return False\n        \n        # Verify grasp success (could use force/tactile sensors)\n        verification_success = self._verify_grasp_success()\n        \n        if not verification_success:\n            # Open gripper to try again\n            self.gripper_controller.open_gripper()\n            return False\n        \n        return True\n    \n    def _verify_grasp_success(self) -> bool:\n        """Verify that object was successfully grasped"""\n        # This would check force sensors, tactile sensors, or visual feedback\n        # For simulation, assume success\n        return True\n    \n    def _try_grasp_recovery(self, object_info: Dict, grasp_config: GraspConfiguration):\n        """Try to recover from failed grasp"""\n        print("Attempting grasp recovery...")\n        # Could try different grasp angles, positions, or forces\n        pass\n    \n    def place_object(self, target_location: Tuple[float, float, float]) -> bool:\n        """Place currently grasped object at target location"""\n        # Move to target location\n        move_success = self.arm_controller.move_to_position(target_location, (0, 0, 0, 1))\n        \n        if not move_success:\n            return False\n        \n        # Open gripper to release object\n        self.gripper_controller.open_gripper()\n        \n        # Move away slightly to avoid collision\n        move_away_pos = (\n            target_location[0],\n            target_location[1],\n            target_location[2] + 0.05  # Move up slightly\n        )\n        \n        return self.arm_controller.move_to_position(move_away_pos, (0, 0, 0, 1))\n\nclass ArmController:\n    """Controls humanoid arm movements"""\n    def __init__(self):\n        # In a real implementation, this would interface with robot hardware\n        self.current_joints = [0.0] * 7  # Example 7-DOF arm\n    \n    def move_to_position(self, position: Tuple[float, float, float], \n                        orientation: Tuple[float, float, float, float]) -> bool:\n        """Move arm to specified position and orientation"""\n        print(f"Moving arm to position {position} with orientation {orientation}")\n        # In reality, this would solve inverse kinematics\n        # and send commands to joint controllers\n        return True  # Simulate success\n\nclass GripperController:\n    """Controls humanoid hand/gripper"""\n    def __init__(self):\n        self.is_closed = False\n        self.current_force = 0.0\n    \n    def grasp_with_force(self, force: float) -> bool:\n        """Grasp with specified force"""\n        print(f"Closing gripper with force {force}N")\n        self.current_force = force\n        self.is_closed = True\n        return True\n    \n    def open_gripper(self):\n        """Open the gripper"""\n        print("Opening gripper")\n        self.is_closed = False\n        self.current_force = 0.0\n'})}),"\n",(0,o.jsx)(n.h2,{id:"complete-capstone-system-integration",children:"Complete Capstone System Integration"}),"\n",(0,o.jsx)(n.h3,{id:"main-capstone-node",children:"Main Capstone Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# capstone_system.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nimport threading\nimport time\n\nclass AutonomousHumanoidCapstone(Node):\n    def __init__(self):\n        super().__init__('autonomous_humanoid_capstone')\n        \n        # Initialize all subsystems\n        self.voice_system = CapstoneVoiceSystem(api_key=\"your-openai-api-key\")\n        self.intent_processor = IntentProcessor()\n        self.perception_system = MultiSensorPerceptionSystem()\n        self.path_planner = HumanoidPathPlanner()\n        self.navigation_controller = NavigationController()\n        self.manipulation_controller = ManipulationController()\n        \n        # State management\n        self.current_state = \"IDLE\"  # IDLE, LISTENING, PROCESSING, NAVIGATING, MANIPULATING, COMPLETED\n        self.current_tasks = []\n        self.system_active = False\n        \n        # ROS publishers and subscribers\n        self.status_pub = self.create_publisher(String, '/capstone/status', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.voice_cmd_sub = self.create_subscription(\n            String, '/vla/command', self.voice_command_callback, 10\n        )\n        \n        # Main control timer\n        self.main_timer = self.create_timer(0.1, self.main_control_loop)\n        \n        # Initialize voice system\n        self.voice_system.start_system()\n        \n        self.get_logger().info('Autonomous Humanoid Capstone System initialized')\n\n    def voice_command_callback(self, msg):\n        \"\"\"Handle voice commands from other nodes\"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Voice command received: {command_text}')\n        \n        # Create a simulated voice command\n        from dataclasses import dataclass\n        @dataclass\n        class SimulatedVoiceCommand:\n            text: str\n            confidence: float = 0.9\n            timestamp: float = time.time()\n            intent: str = \"unknown\"\n            objects: list = None\n            location: str = None\n            \n        sim_command = SimulatedVoiceCommand(text=command_text)\n        self.process_voice_command(sim_command)\n\n    def process_voice_command(self, command):\n        \"\"\"Process a voice command through the full pipeline\"\"\"\n        self.get_logger().info(f'Processing command: {command.text}')\n        \n        # Update state\n        self.current_state = \"PROCESSING\"\n        self._publish_status(f\"Processing: {command.text}\")\n        \n        # Process with intent processor\n        tasks = self.intent_processor.process_command(command)\n        \n        if tasks:\n            self.current_tasks.extend(tasks)\n            self.execute_tasks()\n        else:\n            self.get_logger().warn(f'Could not generate tasks for command: {command.text}')\n            self.current_state = \"IDLE\"\n            self._publish_status(\"Could not process command\")\n\n    def execute_tasks(self):\n        \"\"\"Execute the queue of tasks\"\"\"\n        for task in self.current_tasks:\n            self.get_logger().info(f'Executing task: {task.description}')\n            \n            if task.type == 'navigation':\n                self.execute_navigation_task(task)\n            elif task.type == 'manipulation':\n                self.execute_manipulation_task(task)\n            elif task.type == 'grasp_object':\n                self.execute_grasp_task(task)\n            elif task.type == 'approach_object':\n                self.execute_approach_task(task)\n            # Add more task types as needed\n            \n        # Clear executed tasks\n        self.current_tasks.clear()\n        self.current_state = \"COMPLETED\"\n        self._publish_status(\"Task completed\")\n\n    def execute_navigation_task(self, task):\n        \"\"\"Execute navigation task\"\"\"\n        self.get_logger().info('Executing navigation task')\n        self.current_state = \"NAVIGATING\"\n        \n        # For simplicity, simulate navigation to a predefined location\n        # In reality, this would involve complex path planning\n        target_location = task.parameters.get('target_location', 'unknown')\n        \n        self._publish_status(f\"Navigating to {target_location}\")\n        \n        # Simulate navigation (in reality, this would involve real navigation)\n        time.sleep(3.0)  # Simulate navigation time\n        \n        self.get_logger().info(f'Navigation to {target_location} completed')\n\n    def execute_manipulation_task(self, task):\n        \"\"\"Execute manipulation task\"\"\"\n        self.get_logger().info('Executing manipulation task')\n        self.current_state = \"MANIPULATING\"\n        \n        # For this example, we'll look for objects in the environment\n        # and attempt to manipulate them\n        target_object = task.parameters.get('target_object', 'unknown')\n        \n        self._publish_status(f\"Looking for {target_object}\")\n        \n        # Simulate object detection and manipulation\n        # In a real system, this would integrate with perception and manipulation systems\n        time.sleep(2.0)\n        \n        # Try to find the object in the environment\n        detected_objects = []  # This would come from perception system\n        target_obj_info = None\n        \n        # Simulate finding an object\n        target_obj_info = {\n            'class': target_object,\n            'position': [1.0, 1.0, 0.8],  # x, y, z coordinates\n            'dimensions': [0.1, 0.1, 0.2]  # width, depth, height\n        }\n        \n        if target_obj_info:\n            self.get_logger().info(f'Found {target_object}, attempting manipulation')\n            \n            # Execute grasp\n            grasp_success = self.manipulation_controller.execute_grasp(target_obj_info)\n            \n            if grasp_success:\n                self.get_logger().info(f'Successfully grasped {target_object}')\n                \n                # If there's a placement location, execute placement\n                if 'target_location' in task.parameters:\n                    placement_location = task.parameters['target_location']\n                    self.manipulation_controller.place_object(\n                        tuple(target_obj_info['position'])  # Use original position as example\n                    )\n                    self.get_logger().info(f'Placed {target_object} at {placement_location}')\n            else:\n                self.get_logger().error(f'Failed to grasp {target_object}')\n        else:\n            self.get_logger().error(f'Could not find {target_object} in environment')\n\n    def execute_grasp_task(self, task):\n        \"\"\"Execute grasp task\"\"\"\n        self.get_logger().info('Executing grasp task')\n        \n        # This would integrate with perception to find the specific object\n        target_object = task.parameters.get('target_object', 'unknown')\n        \n        # Simulate finding and grasping the object\n        obj_info = {\n            'class': target_object,\n            'position': [0.5, 0.5, 0.8],  # Example position\n            'dimensions': [0.1, 0.1, 0.1]\n        }\n        \n        success = self.manipulation_controller.execute_grasp(obj_info)\n        if success:\n            self.get_logger().info(f'Successfully executed grasp for {target_object}')\n        else:\n            self.get_logger().error(f'Failed to execute grasp for {target_object}')\n\n    def execute_approach_task(self, task):\n        \"\"\"Execute approach task\"\"\"\n        self.get_logger().info('Executing approach task')\n        \n        # Simulate approach to object\n        target_object = task.parameters.get('target_object', 'unknown')\n        self.get_logger().info(f'Approaching {target_object}')\n        \n        # In reality, this would involve navigation to approach the object\n        time.sleep(2.0)  # Simulate approach time\n\n    def main_control_loop(self):\n        \"\"\"Main control loop for the capstone system\"\"\"\n        # This runs continuously at 10 Hz\n        \n        # Check for new voice commands from the voice system\n        if hasattr(self, 'voice_system'):\n            try:\n                cmd = self.voice_system.get_next_command()\n                if cmd:\n                    self.process_voice_command(cmd)\n            except Exception as e:\n                self.get_logger().error(f'Error checking voice commands: {e}')\n        \n        # Publish current status\n        status_msg = String()\n        status_msg.data = f\"State: {self.current_state}, Tasks: {len(self.current_tasks)}\"\n        self.status_pub.publish(status_msg)\n\n    def _publish_status(self, status_text):\n        \"\"\"Publish status message\"\"\"\n        status_msg = String()\n        status_msg.data = status_text\n        self.status_pub.publish(status_msg)\n\n    def start_system(self):\n        \"\"\"Start the capstone system\"\"\"\n        self.system_active = True\n        self.current_state = \"IDLE\"\n        self.get_logger().info('Capstone system started')\n\n    def stop_system(self):\n        \"\"\"Stop the capstone system\"\"\"\n        self.system_active = False\n        self.current_state = \"STOPPED\"\n        self.voice_system.stop_system()\n        self.get_logger().info('Capstone system stopped')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    capstone_node = AutonomousHumanoidCapstone()\n    capstone_node.start_system()\n    \n    try:\n        rclpy.spin(capstone_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        capstone_node.stop_system()\n        capstone_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"mission-planning-and-execution",children:"Mission Planning and Execution"}),"\n",(0,o.jsx)(n.h3,{id:"hierarchical-task-planner",children:"Hierarchical Task Planner"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# mission_planner.py\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport heapq\n\nclass MissionState(Enum):\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    PAUSED = "paused"\n    COMPLETED = "completed"\n    FAILED = "failed"\n\n@dataclass\nclass MissionTask:\n    """Represents a task in the mission"""\n    id: str\n    name: str\n    description: str\n    priority: int\n    dependencies: List[str]\n    type: str  # navigation, manipulation, perception, communication\n    estimated_duration: float  # in seconds\n    required_resources: List[str]  # e.g., left_arm, right_arm, camera, etc.\n    success_criteria: List[str]  # conditions that must be true for success\n\nclass MissionPlanner:\n    def __init__(self):\n        self.tasks = {}\n        self.mission_state = MissionState.PLANNING\n        self.execution_order = []\n        self.robot_resources = {\n            \'left_arm\': True,\n            \'right_arm\': True,\n            \'navigation_system\': True,\n            \'camera\': True,\n            \'microphone\': True\n        }\n        \n    def plan_mission(self, objectives: List[str]) -> bool:\n        """Plan mission based on high-level objectives"""\n        # Convert objectives to specific tasks\n        self.tasks = self._objectives_to_tasks(objectives)\n        \n        # Check resource availability\n        if not self._check_resource_availability():\n            return False\n        \n        # Create execution order considering dependencies\n        self.execution_order = self._create_execution_order()\n        \n        self.mission_state = MissionState.PLANNING\n        return True\n    \n    def _objectives_to_tasks(self, objectives: List[str]) -> Dict[str, MissionTask]:\n        """Convert high-level objectives to specific tasks"""\n        tasks = {}\n        task_counter = 0\n        \n        for objective in objectives:\n            if "bring" in objective.lower() or "pick up" in objective.lower():\n                # Create navigation, manipulation, and return tasks\n                obj_to_grab = self._extract_object(objective)\n                \n                # Task 1: Navigate to object\n                task_counter += 1\n                nav_task = MissionTask(\n                    id=f"nav_to_{obj_to_grab}_{task_counter}",\n                    name=f"Navigate to {obj_to_grab}",\n                    description=f"Navigate to the location of {obj_to_grab}",\n                    priority=1,\n                    dependencies=[],\n                    type="navigation",\n                    estimated_duration=30.0,\n                    required_resources=["navigation_system"],\n                    success_criteria=[f"at_{obj_to_grab}_location"]\n                )\n                tasks[nav_task.id] = nav_task\n                \n                # Task 2: Grasp object\n                task_counter += 1\n                grasp_task = MissionTask(\n                    id=f"grasp_{obj_to_grab}_{task_counter}",\n                    name=f"Grasp {obj_to_grab}",\n                    description=f"Grasp the {obj_to_grab}",\n                    priority=2,\n                    dependencies=[nav_task.id],\n                    type="manipulation",\n                    estimated_duration=10.0,\n                    required_resources=["left_arm"],  # or right_arm\n                    success_criteria=[f"holding_{obj_to_grab}"]\n                )\n                tasks[grasp_task.id] = grasp_task\n                \n                # Task 3: Navigate to destination\n                destination = self._extract_destination(objective)\n                if destination:\n                    task_counter += 1\n                    return_task = MissionTask(\n                        id=f"return_to_{destination}_{task_counter}",\n                        name=f"Return to {destination}",\n                        description=f"Navigate back to {destination} with {obj_to_grab}",\n                        priority=1,\n                        dependencies=[grasp_task.id],\n                        type="navigation",\n                        estimated_duration=30.0,\n                        required_resources=["navigation_system"],\n                        success_criteria=[f"at_{destination}_location"]\n                    )\n                    tasks[return_task.id] = return_task\n        \n        return tasks\n    \n    def _extract_object(self, objective: str) -> str:\n        """Extract object name from objective"""\n        # Simple extraction - in reality this would use NLP\n        common_objects = [\'cup\', \'bottle\', \'book\', \'phone\', \'keys\', \'wallet\', \'laptop\']\n        for obj in common_objects:\n            if obj in objective.lower():\n                return obj\n        return "object"\n    \n    def _extract_destination(self, objective: str) -> str:\n        """Extract destination from objective"""\n        common_destinations = [\'kitchen\', \'bedroom\', \'living room\', \'office\', \'table\', \'desk\']\n        for dest in common_destinations:\n            if dest in objective.lower():\n                return dest\n        return "current location"\n    \n    def _check_resource_availability(self) -> bool:\n        """Check if required resources are available for all tasks"""\n        # Check if resources are available at the same time needed\n        # This is a simplified check\n        return True\n    \n    def _create_execution_order(self) -> List[str]:\n        """Create execution order considering dependencies"""\n        # Use topological sort for task dependencies\n        visited = set()\n        order = []\n        \n        def dfs(task_id):\n            if task_id in visited:\n                return\n            visited.add(task_id)\n            \n            task = self.tasks[task_id]\n            for dep_id in task.dependencies:\n                if dep_id in self.tasks:\n                    dfs(dep_id)\n            \n            order.append(task_id)\n        \n        for task_id in self.tasks:\n            if task_id not in visited:\n                dfs(task_id)\n        \n        # Sort by priority\n        priority_order = []\n        for task_id in order:\n            task = self.tasks[task_id]\n            priority_order.append((task.priority, task_id))\n        \n        priority_order.sort(reverse=True)  # Higher priority first\n        return [task_id for _, task_id in priority_order]\n    \n    def execute_mission(self) -> MissionState:\n        """Execute the planned mission"""\n        self.mission_state = MissionState.EXECUTING\n        \n        for task_id in self.execution_order:\n            task = self.tasks[task_id]\n            \n            self._publish_task_status(task, "STARTED")\n            \n            # Check if task can be executed (resources available)\n            if not self._check_resources_available(task.required_resources):\n                self.mission_state = MissionState.FAILED\n                self._publish_task_status(task, "FAILED - Resources unavailable")\n                return self.mission_state\n            \n            # Execute task\n            success = self._execute_task(task)\n            \n            if success:\n                self._publish_task_status(task, "COMPLETED")\n                \n                # Update resource availability\n                self._update_resource_availability(task, True)\n            else:\n                # Update resource availability\n                self._update_resource_availability(task, False)\n                \n                self.mission_state = MissionState.FAILED\n                self._publish_task_status(task, "FAILED")\n                return self.mission_state\n        \n        self.mission_state = MissionState.COMPLETED\n        return self.mission_state\n    \n    def _check_resources_available(self, required_resources: List[str]) -> bool:\n        """Check if required resources are available"""\n        for resource in required_resources:\n            if resource in self.robot_resources and not self.robot_resources[resource]:\n                return False\n        return True\n    \n    def _execute_task(self, task: MissionTask) -> bool:\n        """Execute a single task"""\n        print(f"Executing task: {task.name}")\n        \n        # Simulate task execution time\n        import time\n        time.sleep(task.estimated_duration / 10)  # Simulate with faster execution\n        \n        # For simulation, assume all tasks succeed\n        # In reality, this would involve actual robot execution and feedback\n        return True\n    \n    def _update_resource_availability(self, task: MissionTask, success: bool):\n        """Update resource availability after task execution"""\n        for resource in task.required_resources:\n            if resource in self.robot_resources:\n                if success:\n                    # Resource is now free\n                    self.robot_resources[resource] = True\n                else:\n                    # Resource might be in error state\n                    self.robot_resources[resource] = True  # Reset to available\n    \n    def _publish_task_status(self, task: MissionTask, status: str):\n        """Publish task execution status"""\n        print(f"Task {task.id}: {status}")\n\n# Example mission execution\ndef run_example_mission():\n    """Run an example mission with the capstone system"""\n    planner = MissionPlanner()\n    \n    # Define mission objectives\n    objectives = [\n        "Bring the red cup from the kitchen to the living room",\n        "Pick up the keys from the office and bring them to the bedroom"\n    ]\n    \n    # Plan the mission\n    success = planner.plan_mission(objectives)\n    \n    if success:\n        print("Mission planning successful!")\n        print(f"Tasks planned: {len(planner.tasks)}")\n        print(f"Execution order: {planner.execution_order}")\n        \n        # Execute the mission\n        final_state = planner.execute_mission()\n        print(f"Mission completed with state: {final_state}")\n    else:\n        print("Mission planning failed!")\n\nif __name__ == "__main__":\n    run_example_mission()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"system-validation-and-testing",children:"System Validation and Testing"}),"\n",(0,o.jsx)(n.h3,{id:"comprehensive-testing-framework",children:"Comprehensive Testing Framework"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# system_validation.py\nimport unittest\nfrom typing import Dict, List\nimport numpy as np\nimport time\n\nclass CapstoneSystemValidator:\n    def __init__(self):\n        self.test_results = {\n            'voice_recognition': [],\n            'navigation': [],\n            'manipulation': [],\n            'integration': [],\n            'safety': []\n        }\n        \n    def run_comprehensive_tests(self) -> Dict:\n        \"\"\"Run all system tests and return results\"\"\"\n        print(\"Running comprehensive validation tests...\")\n        \n        # Run individual test suites\n        self._test_voice_recognition()\n        self._test_navigation_system()\n        self._test_manipulation_system()\n        self._test_integration()\n        self._test_safety_features()\n        \n        # Generate final report\n        report = self._generate_validation_report()\n        return report\n    \n    def _test_voice_recognition(self):\n        \"\"\"Test voice recognition and command processing\"\"\"\n        print(\"Testing voice recognition...\")\n        \n        # Simulate voice commands and verify processing\n        test_commands = [\n            \"Bring me the cup from the kitchen\",\n            \"Go to the bedroom\",\n            \"Clean the table\",\n            \"Where are my keys?\"\n        ]\n        \n        for command in test_commands:\n            try:\n                # This would test the actual voice processing pipeline\n                result = self._simulate_command_processing(command)\n                self.test_results['voice_recognition'].append({\n                    'command': command,\n                    'success': result['success'],\n                    'accuracy': result['accuracy'],\n                    'processing_time': result['time']\n                })\n            except Exception as e:\n                self.test_results['voice_recognition'].append({\n                    'command': command,\n                    'success': False,\n                    'error': str(e)\n                })\n    \n    def _test_navigation_system(self):\n        \"\"\"Test navigation system capabilities\"\"\"\n        print(\"Testing navigation system...\")\n        \n        # Test navigation to various locations\n        test_destinations = [\n            (1.0, 1.0, 0.0),\n            (5.0, 3.0, 0.0),\n            (0.0, 0.0, 0.0),  # Return to start\n            (2.5, 4.0, 0.5)   # With height variation\n        ]\n        \n        for dest in test_destinations:\n            try:\n                result = self._simulate_navigation_test(dest)\n                self.test_results['navigation'].append({\n                    'destination': dest,\n                    'success': result['success'],\n                    'path_efficiency': result['efficiency'],\n                    'collision_free': result['collision_free'],\n                    'time_taken': result['time']\n                })\n            except Exception as e:\n                self.test_results['navigation'].append({\n                    'destination': dest,\n                    'success': False,\n                    'error': str(e)\n                })\n    \n    def _test_manipulation_system(self):\n        \"\"\"Test manipulation capabilities\"\"\"\n        print(\"Testing manipulation system...\")\n        \n        # Test manipulation of various objects\n        test_objects = [\n            {'class': 'cup', 'position': (1.0, 1.0, 0.8)},\n            {'class': 'book', 'position': (1.5, 1.5, 0.8)},\n            {'class': 'box', 'position': (2.0, 2.0, 0.8)}\n        ]\n        \n        for obj in test_objects:\n            try:\n                result = self._simulate_manipulation_test(obj)\n                self.test_results['manipulation'].append({\n                    'object': obj,\n                    'success': result['success'],\n                    'grasp_success': result['grasp_success'],\n                    'placement_accuracy': result['placement_accuracy'],\n                    'time_taken': result['time']\n                })\n            except Exception as e:\n                self.test_results['manipulation'].append({\n                    'object': obj,\n                    'success': False,\n                    'error': str(e)\n                })\n    \n    def _test_integration(self):\n        \"\"\"Test system integration and coordination\"\"\"\n        print(\"Testing system integration...\")\n        \n        # Test complete scenarios that involve multiple subsystems\n        scenarios = [\n            \"Pick up cup and bring to user\",\n            \"Navigate to kitchen, find object, bring back\",\n            \"Clean table by picking up items\"\n        ]\n        \n        for scenario in scenarios:\n            try:\n                result = self._simulate_integration_test(scenario)\n                self.test_results['integration'].append({\n                    'scenario': scenario,\n                    'success': result['success'],\n                    'subsystem_coordination': result['coordination'],\n                    'task_completion_rate': result['completion_rate'],\n                    'time_taken': result['time']\n                })\n            except Exception as e:\n                self.test_results['integration'].append({\n                    'scenario': scenario,\n                    'success': False,\n                    'error': str(e)\n                })\n    \n    def _test_safety_features(self):\n        \"\"\"Test safety systems and emergency procedures\"\"\"\n        print(\"Testing safety features...\")\n        \n        safety_tests = [\n            ('stop_on_collision', True),\n            ('emergency_stop', True),\n            ('obstacle_avoidance', True),\n            ('balance_recovery', True)\n        ]\n        \n        for test_name, should_pass in safety_tests:\n            try:\n                result = self._simulate_safety_test(test_name, should_pass)\n                self.test_results['safety'].append({\n                    'test': test_name,\n                    'success': result['success'],\n                    'passed': result['passed'],\n                    'response_time': result['time']\n                })\n            except Exception as e:\n                self.test_results['safety'].append({\n                    'test': test_name,\n                    'success': False,\n                    'error': str(e)\n                })\n    \n    def _simulate_command_processing(self, command: str) -> Dict:\n        \"\"\"Simulate command processing\"\"\"\n        # Simulate processing time\n        time.sleep(0.1)\n        \n        return {\n            'success': True,\n            'accuracy': 0.95,\n            'time': 0.1\n        }\n    \n    def _simulate_navigation_test(self, destination: tuple) -> Dict:\n        \"\"\"Simulate navigation test\"\"\"\n        # Simulate navigation time\n        time.sleep(0.5)\n        \n        return {\n            'success': True,\n            'efficiency': 0.85,\n            'collision_free': True,\n            'time': 0.5\n        }\n    \n    def _simulate_manipulation_test(self, obj: Dict) -> Dict:\n        \"\"\"Simulate manipulation test\"\"\"\n        # Simulate manipulation time\n        time.sleep(0.3)\n        \n        return {\n            'success': True,\n            'grasp_success': True,\n            'placement_accuracy': 0.92,\n            'time': 0.3\n        }\n    \n    def _simulate_integration_test(self, scenario: str) -> Dict:\n        \"\"\"Simulate integration test\"\"\"\n        # Simulate complex scenario time\n        time.sleep(1.0)\n        \n        return {\n            'success': True,\n            'coordination': 0.90,\n            'completion_rate': 0.95,\n            'time': 1.0\n        }\n    \n    def _simulate_safety_test(self, test_name: str, should_pass: bool) -> Dict:\n        \"\"\"Simulate safety test\"\"\"\n        # Simulate safety test time\n        time.sleep(0.2)\n        \n        return {\n            'success': True,\n            'passed': should_pass,\n            'time': 0.2\n        }\n    \n    def _generate_validation_report(self) -> Dict:\n        \"\"\"Generate comprehensive validation report\"\"\"\n        report = {}\n        \n        for category, results in self.test_results.items():\n            if results:\n                successful_tests = [r for r in results if r.get('success', False)]\n                success_rate = len(successful_tests) / len(results) if results else 0\n                \n                report[category] = {\n                    'total_tests': len(results),\n                    'successful_tests': len(successful_tests),\n                    'success_rate': success_rate,\n                    'average_performance': self._calculate_average_performance(results),\n                    'detailed_results': results\n                }\n        \n        # Calculate overall system validation score\n        overall_success_rates = [\n            category_results['success_rate'] \n            for category_results in report.values()\n        ]\n        report['overall_score'] = np.mean(overall_success_rates) if overall_success_rates else 0\n        \n        return report\n\n# Unit tests for individual components\nclass TestVoiceSystem(unittest.TestCase):\n    def test_voice_command_processing(self):\n        \"\"\"Test voice command processing\"\"\"\n        # This would test the actual voice system\n        self.assertTrue(True)  # Placeholder\n    \n    def test_intent_classification(self):\n        \"\"\"Test intent classification accuracy\"\"\"\n        self.assertTrue(True)  # Placeholder\n\nclass TestNavigationSystem(unittest.TestCase):\n    def test_path_planning(self):\n        \"\"\"Test path planning algorithm\"\"\"\n        planner = HumanoidPathPlanner()\n        \n        # Test with a simple grid\n        grid = np.zeros((10, 10, 3))  # 3D grid (x, y, z)\n        start = (1.0, 1.0, 0.0)\n        goal = (8.0, 8.0, 0.0)\n        \n        path = planner.plan_path(start, goal, grid)\n        self.assertIsNotNone(path, \"Path should be found in free space\")\n        self.assertGreater(len(path), 0, \"Path should have waypoints\")\n\nclass TestManipulationSystem(unittest.TestCase):\n    def test_grasp_planning(self):\n        \"\"\"Test grasp planning for common objects\"\"\"\n        planner = GraspPlanner()\n        \n        # Test with a known object\n        object_info = {\n            'class': 'bottle',\n            'position': (0.5, 0.5, 0.8),\n            'dimensions': (0.08, 0.08, 0.25)\n        }\n        \n        grasp_config = planner.plan_grasp(object_info)\n        self.assertIsNotNone(grasp_config, \"Grasp should be planned for known object\")\n\ndef run_all_tests():\n    \"\"\"Run all validation tests\"\"\"\n    validator = CapstoneSystemValidator()\n    report = validator.run_comprehensive_tests()\n    \n    print(\"\\n=== VALIDATION REPORT ===\")\n    for category, results in report.items():\n        if category != 'overall_score':\n            print(f\"\\n{category.upper()}:\")\n            print(f\"  Success Rate: {results['success_rate']:.2%}\")\n            print(f\"  Average Performance: {results.get('average_performance', 'N/A')}\")\n            print(f\"  Tests Run: {results['total_tests']}\")\n    \n    print(f\"\\nOVERALL SYSTEM SCORE: {report['overall_score']:.2%}\")\n    \n    # Run unit tests as well\n    unittest.main(argv=[''], exit=False, verbosity=2)\n\ndef _calculate_average_performance(self, results: List[Dict]) -> float:\n    \"\"\"Calculate average performance metric\"\"\"\n    if not results:\n        return 0.0\n    \n    # Look for 'accuracy', 'efficiency', or similar metrics\n    metrics = []\n    for result in results:\n        if 'accuracy' in result:\n            metrics.append(result['accuracy'])\n        elif 'efficiency' in result:\n            metrics.append(result['efficiency'])\n        elif 'success_rate' in result:\n            metrics.append(result['success_rate'])\n    \n    return np.mean(metrics) if metrics else 0.0\n"})}),"\n",(0,o.jsx)(n.h2,{id:"deployment-and-optimization-strategies",children:"Deployment and Optimization Strategies"}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimization-and-deployment",children:"Performance Optimization and Deployment"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# deployment_optimization.py\nimport threading\nimport time\nimport psutil\nimport GPUtil\nfrom typing import Dict, List\nimport queue\n\nclass SystemOptimizer:\n    def __init__(self):\n        self.performance_metrics = {\n            \'cpu_usage\': [],\n            \'memory_usage\': [],\n            \'gpu_usage\': [],\n            \'processing_times\': [],\n            \'throughput\': []\n        }\n        self.resource_manager = ResourceManager()\n        self.is_monitoring = False\n        self.monitoring_thread = None\n    \n    def start_monitoring(self):\n        """Start system performance monitoring"""\n        self.is_monitoring = True\n        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)\n        self.monitoring_thread.daemon = True\n        self.monitoring_thread.start()\n    \n    def stop_monitoring(self):\n        """Stop system performance monitoring"""\n        self.is_monitoring = False\n        if self.monitoring_thread:\n            self.monitoring_thread.join()\n    \n    def _monitoring_loop(self):\n        """Continuous monitoring loop"""\n        while self.is_monitoring:\n            # Collect performance metrics\n            cpu_percent = psutil.cpu_percent()\n            memory_percent = psutil.virtual_memory().percent\n            \n            # GPU usage (if available)\n            gpu_percent = 0\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                gpu_percent = gpus[0].load * 100\n            \n            self.performance_metrics[\'cpu_usage\'].append(cpu_percent)\n            self.performance_metrics[\'memory_usage\'].append(memory_percent)\n            self.performance_metrics[\'gpu_usage\'].append(gpu_percent)\n            \n            time.sleep(1.0)  # Monitor every second\n    \n    def optimize_for_platform(self, platform_specs: Dict):\n        """Optimize system settings for specific hardware"""\n        # Adjust processing frequencies based on platform\n        cpu_count = platform_specs.get(\'cpu_cores\', 4)\n        gpu_compute = platform_specs.get(\'gpu_compute\', 1.0)  # Relative compute power\n        memory_gb = platform_specs.get(\'memory_gb\', 8)\n        \n        # Adjust processing frequencies\n        if cpu_count < 4:\n            # Reduce processing frequencies for low-power systems\n            self._set_low_power_mode()\n        elif gpu_compute < 10:  # Low-end GPU\n            # Reduce visual processing intensity\n            self._set_efficient_vision_mode()\n        elif memory_gb < 8:\n            # Reduce data buffering\n            self._set_low_memory_mode()\n    \n    def _set_low_power_mode(self):\n        """Set system to low power mode"""\n        print("Setting low power mode...")\n        # Reduce processing frequencies\n        # Use simpler algorithms\n        # Disable non-critical features temporarily\n    \n    def _set_efficient_vision_mode(self):\n        """Optimize vision processing for efficiency"""\n        print("Setting efficient vision mode...")\n        # Use smaller CNN models\n        # Reduce image processing resolution\n        # Use faster but less accurate algorithms\n    \n    def _set_low_memory_mode(self):\n        """Optimize for low memory systems"""\n        print("Setting low memory mode...")\n        # Reduce buffer sizes\n        # Stream processing instead of batch\n        # Use memory mapping for large data\n\nclass ResourceManager:\n    """Manages system resources and allocation"""\n    def __init__(self):\n        self.resource_pools = {\n            \'cpu\': queue.Queue(maxsize=4),\n            \'gpu\': queue.Queue(maxsize=1),  # Usually just one primary GPU\n            \'memory\': 0,\n            \'bandwidth\': 100  # MB/s\n        }\n        self.active_processes = {}\n    \n    def allocate_resources(self, process_id: str, requirements: Dict) -> bool:\n        """Allocate resources for process"""\n        # Check if sufficient resources are available\n        for resource, req_amount in requirements.items():\n            if resource == \'cpu\':\n                if self.resource_pools[\'cpu\'].full():\n                    return False\n            elif resource == \'gpu\':\n                if self.resource_pools[\'gpu\'].full():\n                    return False\n        \n        # Allocate resources\n        for resource, req_amount in requirements.items():\n            if resource == \'cpu\':\n                self.resource_pools[\'cpu\'].put(process_id)\n            elif resource == \'gpu\':\n                self.resource_pools[\'gpu\'].put(process_id)\n        \n        self.active_processes[process_id] = requirements\n        return True\n    \n    def release_resources(self, process_id: str):\n        """Release resources from process"""\n        if process_id in self.active_processes:\n            requirements = self.active_processes[process_id]\n            \n            # Remove from resource queues\n            for resource, _ in requirements.items():\n                if resource in self.resource_pools:\n                    try:\n                        # Remove process from queue\n                        temp_queue = queue.Queue()\n                        while not self.resource_pools[resource].empty():\n                            item = self.resource_pools[resource].get()\n                            if item != process_id:\n                                temp_queue.put(item)\n                        \n                        # Put back non-matching items\n                        while not temp_queue.empty():\n                            self.resource_pools[resource].put(temp_queue.get())\n                    except queue.Empty:\n                        pass\n            \n            del self.active_processes[process_id]\n\nclass DeploymentManager:\n    """Manages deployment to different platforms"""\n    def __init__(self):\n        self.supported_platforms = {\n            \'jetson_orin_nano\': {\n                \'cpu_cores\': 8,\n                \'gpu_compute\': 40,  # TOPS\n                \'memory_gb\': 4,\n                \'power_watts\': 15\n            },\n            \'jetson_agx_orin\': {\n                \'cpu_cores\': 12,\n                \'gpu_compute\': 275,  # TOPS\n                \'memory_gb\': 32,\n                \'power_watts\': 60\n            },\n            \'desktop_gpu\': {\n                \'cpu_cores\': 16,\n                \'gpu_compute\': 500,  # Estimated\n                \'memory_gb\': 64,\n                \'power_watts\': 200\n            }\n        }\n        self.optimizer = SystemOptimizer()\n    \n    def deploy_to_platform(self, platform_name: str, system_config: Dict):\n        """Deploy system to specified platform"""\n        if platform_name not in self.supported_platforms:\n            raise ValueError(f"Unsupported platform: {platform_name}")\n        \n        platform_specs = self.supported_platforms[platform_name]\n        \n        print(f"Deploying to {platform_name} with specs: {platform_specs}")\n        \n        # Optimize system for target platform\n        self.optimizer.optimize_for_platform(platform_specs)\n        \n        # Configure system with platform-specific settings\n        self._configure_platform_specifics(platform_name, system_config)\n        \n        # Validate deployment\n        success = self._validate_deployment(platform_name)\n        \n        if success:\n            print(f"Successfully deployed to {platform_name}")\n        else:\n            print(f"Deployment to {platform_name} failed validation")\n        \n        return success\n    \n    def _configure_platform_specifics(self, platform_name: str, config: Dict):\n        """Configure system with platform-specific optimizations"""\n        # Platform-specific configuration\n        if \'jetson\' in platform_name.lower():\n            # Configure for NVIDIA Jetson platform\n            config[\'vision_model\'] = \'tensorrt_optimized\'\n            config[\'processing_threads\'] = 4\n        elif platform_name == \'desktop_gpu\':\n            # Configure for high-performance desktop\n            config[\'vision_model\'] = \'full_precision\'\n            config[\'processing_threads\'] = 8\n    \n    def _validate_deployment(self, platform_name: str) -> bool:\n        """Validate that system runs properly on platform"""\n        # This would run platform-specific tests\n        # For simulation, assume validation passes\n        return True\n\n# Example deployment\ndef deploy_capstone_system():\n    """Deploy the capstone system to target hardware"""\n    deploy_manager = DeploymentManager()\n    \n    # Example: Deploy to Jetson Orin Nano\n    system_config = {\n        \'voice_model\': \'whisper-tiny\',\n        \'vision_model\': \'efficientdet-lite\',\n        \'navigation_planner\': \'grid_based\',\n        \'manipulation_planner\': \'simple_grasps\'\n    }\n    \n    success = deploy_manager.deploy_to_platform(\'jetson_orin_nano\', system_config)\n    \n    if success:\n        print("Capstone system successfully deployed!")\n        print("Starting performance optimization...")\n        deploy_manager.optimizer.start_monitoring()\n    else:\n        print("Deployment failed - please check hardware compatibility")\n\nif __name__ == "__main__":\n    # Run validation tests\n    run_all_tests()\n    \n    # Deploy system\n    deploy_capstone_system()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,o.jsx)(n.p,{children:"The Autonomous Humanoid Capstone Project represents the integration of all Physical AI concepts into a functioning autonomous system. This chapter demonstrated how to combine ROS 2 middleware, NVIDIA Isaac perception, Vision-Language-Action capabilities, and humanoid-specific navigation and manipulation into a complete autonomous robot. Key achievements include:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-to-End Pipeline"}),": Created a complete pipeline from voice command to physical action execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-Sensor Integration"}),": Combined visual, LiDAR, and IMU data for robust environmental understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humanoid-Specific Navigation"}),": Implemented path planning considering humanoid physical constraints"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Advanced Manipulation"}),": Developed grasp planning and execution for humanoid hands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mission Planning"}),": Created hierarchical task planning for complex multi-step objectives"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"System Validation"}),": Implemented comprehensive testing and validation frameworks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Deployment Strategies"}),": Developed optimization strategies for different hardware platforms"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The success of this capstone project demonstrates that autonomous humanoid robots capable of understanding natural language commands, navigating complex environments, and manipulating objects are achievable with current technology when properly integrated."}),"\n",(0,o.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Autonomous Humanoid"}),": Robot capable of independent operation with human-like form and capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-to-End Pipeline"}),": Complete system from input to output without human intervention"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-Sensor Fusion"}),": Integration of data from multiple sensor types for enhanced perception"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humanoid Kinematics"}),": Study of humanoid robot movement and joint mechanics"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hierarchical Task Planning"}),": Organization of tasks at different levels of abstraction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"System Integration"}),": Combination of multiple subsystems into a cohesive whole"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance Optimization"}),": Techniques to maximize system efficiency"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Deployment Validation"}),": Verification of system functionality on target hardware"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Assurance"}),": Systems to ensure safe robot operation in human environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-Time Processing"}),": Systems capable of responding within strict time constraints"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"practice-questions",children:"Practice Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"System Integration"}),": Design a complete integration plan for the capstone system on a humanoid robot. Include all necessary ROS nodes, communication protocols, and safety systems."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance Optimization"}),": Given a Jetson Orin Nano platform (40 TOPS, 4GB RAM), optimize the capstone system for real-time operation. What components would you modify and why?"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Safety Systems"}),": Design safety mechanisms for the autonomous humanoid system. Include collision avoidance, emergency stops, and human safety protocols."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Task Planning"}),': For the command "Bring me my keys from the office and then clean the kitchen table," design the complete task execution plan including navigation, manipulation, and error handling.']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"System Validation"}),": Create a comprehensive test suite for the capstone system. What metrics would you measure and how would you validate successful operation?"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"How might the integration challenges observed in this capstone project influence the design of future autonomous robotic systems?"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"What are the key technical and ethical considerations for deploying truly autonomous humanoid robots in human environments?"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"How could the system architecture developed in this capstone project be extended to support additional capabilities like social interaction or learning from experience?"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Congratulations! You have completed the Physical AI & Humanoid Robotics course. This capstone project demonstrates your ability to integrate all the technologies and concepts covered throughout the course into a functioning autonomous humanoid system."})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},3113:(e,n,t)=>{t.d(n,{A:()=>a});var s=t(6540),o=t(4746),i=t(4848);const a=({chapterId:e,userId:n="demo-user"})=>{const[t,a]=(0,s.useState)(!1),[r,l]=(0,s.useState)(!1),[c,p]=(0,s.useState)("ur"),[d,u]=(0,s.useState)(""),[m,_]=(0,s.useState)(0),[f,g]=(0,s.useState)(""),[h,y]=(0,s.useState)(null),b=(0,s.useRef)(null),v=(0,s.useRef)(null);(0,s.useEffect)(()=>{if("undefined"==typeof window)return;const e=()=>{const e=[".theme-doc-markdown",".markdown","article","main .container",'[class*="docItemContainer"]'];for(const n of e){const e=document.querySelector(n);if(e&&e.textContent&&e.textContent.length>50)return void(b.current=e)}};e();const n=new MutationObserver(()=>{b.current||e()}),t=document.querySelector("body");return t&&n.observe(t,{childList:!0,subtree:!0}),()=>{n.disconnect(),v.current&&v.current.abort()}},[e]);const x=()=>{const e=b.current;e&&d&&(e.style.transition="opacity 0.3s ease-in-out",e.style.opacity="0",setTimeout(()=>{e.innerHTML=d,e.style.direction="ltr",e.style.textAlign="left",e.style.opacity="1",l(!1),y(null)},300))};return(0,i.jsx)("div",{style:{margin:"2rem auto",padding:"1.5rem",border:"2px solid #e5e7eb",borderRadius:"12px",backgroundColor:"linear-gradient(-45deg,  #ff0055,  #7000ff,  #009c98, #000000)",boxShadow:"0 4px 6px -1px rgba(0, 0, 0, 0.1)",textAlign:"center",maxWidth:"700px",fontFamily:"system-ui, sans-serif"},children:(0,i.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"center",gap:"1rem"},children:[(0,i.jsxs)("div",{style:{display:"flex",gap:"10px",alignItems:"center",flexWrap:"wrap",justifyContent:"center"},children:[(0,i.jsxs)("select",{value:c,onChange:e=>{p(e.target.value),r&&x()},disabled:t,style:{padding:"8px",borderRadius:"5px",border:"1px solid #ccc"},children:[(0,i.jsx)("option",{value:"ur",children:"\ud83c\uddf5\ud83c\uddf0 \u0627\u0631\u062f\u0648 (Urdu)"}),(0,i.jsx)("option",{value:"ar",children:"\ud83c\uddf8\ud83c\udde6 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 (Arabic)"}),(0,i.jsx)("option",{value:"es",children:"\ud83c\uddea\ud83c\uddf8 Espa\xf1ol (Spanish)"}),(0,i.jsx)("option",{value:"fr",children:"\ud83c\uddeb\ud83c\uddf7 Fran\xe7ais (French)"})]}),(0,i.jsx)("button",{onClick:async()=>{if(r)return void x();const e=b.current;if(e){a(!0),y(null),_(0),g("Preparing content..."),d||u(e.innerHTML);try{const t=(e=>{const n=3500,t=[];if(e.children.length>0){let s="";Array.from(e.children).forEach(e=>{const o=e.outerHTML;s.length+o.length>n&&s.length>0&&(t.push(s),s=""),s+=o}),s&&t.push(s)}else{const s=e.innerHTML;for(let e=0;e<s.length;e+=n)t.push(s.substring(e,e+n))}return t})(e);if(console.log(`Total chunks to translate: ${t.length}`),0===t.length)throw new Error("No content to translate");let s="";v.current&&v.current.abort(),v.current=new AbortController;for(let e=0;e<t.length;e++){const i=t[e],a=Math.round(e/t.length*100);_(a),g(`Translating part ${e+1} of ${t.length}...`);try{const e=await o.u.post("/api/v1/translate",{text:i,target_language:c,source_language:"en",preserve_formatting:!0});s+=e.translated_text,await new Promise(e=>setTimeout(e,200))}catch(n){console.error(`Error in chunk ${e}:`,n),s+=i}}_(100),g("Finalizing..."),setTimeout(()=>{e&&(e.style.opacity="0",setTimeout(()=>{const n="ur"===c||"ar"===c;e.innerHTML='\n              <div class="alert alert--success margin-bottom--md" style="direction: ltr; text-align: left; padding: 10px; border: 1px solid green; border-radius: 8px; margin-bottom: 20px;">\n                <strong>\u2705 Translated successfully</strong>\n              </div>\n            '+s,e.style.direction=n?"rtl":"ltr",e.style.textAlign=n?"right":"left",e.style.opacity="1"},300)),l(!0),a(!1),g("")},500)}catch(t){console.error("Translation Error:",t),y("Translation failed. Please try again later."),a(!1),_(0)}}else y("Content not found.")},disabled:t,style:{padding:"10px 20px",backgroundColor:r?"#10b981":"#4f46e5",color:"white",border:"none",borderRadius:"6px",cursor:t?"not-allowed":"pointer",fontWeight:"bold",opacity:t?.7:1},children:t?"Translating...":r?"Restore Original":"Translate Page"})]}),t&&(0,i.jsxs)("div",{style:{width:"100%",marginTop:"10px"},children:[(0,i.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",fontSize:"0.8rem",marginBottom:"5px"},children:[(0,i.jsx)("span",{children:f}),(0,i.jsxs)("span",{children:[m,"%"]})]}),(0,i.jsx)("div",{style:{width:"100%",height:"8px",backgroundColor:"#eee",borderRadius:"4px",overflow:"hidden"},children:(0,i.jsx)("div",{style:{width:`${m}%`,height:"100%",backgroundColor:"#4f46e5",transition:"width 0.3s ease"}})})]}),h&&(0,i.jsxs)("p",{style:{color:"red",marginTop:"10px",fontSize:"0.9rem"},children:["\u26a0\ufe0f ",h]})]})})}},7561:(e,n,t)=>{t.d(n,{A:()=>a});var s=t(6540),o=t(4746),i=t(4848);const a=({chapterId:e,userId:n="1"})=>{const[t,a]=(0,s.useState)(!1),[r,l]=(0,s.useState)(!1),[c,p]=(0,s.useState)(""),[d,u]=(0,s.useState)(0),[m,_]=(0,s.useState)(""),[f,g]=(0,s.useState)(null),[h,y]=(0,s.useState)("intermediate"),b=(0,s.useRef)(null),v=(0,s.useRef)(null);(0,s.useEffect)(()=>{if("undefined"==typeof window)return;const e=()=>{const e=[".theme-doc-markdown","article",".markdown","main .container",'[class*="docItemContainer"]'];for(const n of e){const e=document.querySelector(n);if(e&&e.textContent&&e.textContent.length>50)return void(b.current=e)}};e();const n=new MutationObserver(()=>{b.current||e()}),t=document.querySelector("body");return t&&n.observe(t,{childList:!0,subtree:!0}),()=>{n.disconnect(),v.current&&v.current.abort()}},[e]);const x=()=>{const e=b.current;e&&c&&(e.style.transition="opacity 0.3s ease-in-out",e.style.opacity="0",setTimeout(()=>{e.innerHTML=c,e.style.opacity="1",l(!1),g(null)},300))},j=()=>{switch(h){case"beginner":return"#10b981";case"advanced":return"#8b5cf6";default:return"#3b82f6"}},k=()=>{switch(h){case"beginner":return"\ud83c\udf31";case"advanced":return"\ud83d\ude80";default:return"\ud83d\udcd6"}};return(0,i.jsxs)("div",{className:"custom-button-container",style:{margin:"1.5rem auto",padding:"1.5rem",border:"2px solid #e5e7eb",borderRadius:"16px",backgroundColor:"linear-gradient(-45deg,  #ff0055,  #7000ff,  #009c98, #000000)",boxShadow:"0 4px 6px -1px rgba(0, 0, 0, 0.1)",textAlign:"center",maxWidth:"700px",fontFamily:"system-ui, sans-serif"},children:[(0,i.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"center",gap:"1rem"},children:[(0,i.jsxs)("div",{style:{display:"flex",gap:"1rem",alignItems:"center",flexWrap:"wrap",justifyContent:"center"},children:[(0,i.jsx)("label",{style:{fontSize:"0.875rem",fontWeight:"600",color:"#374151"},children:"Target Level:"}),(0,i.jsxs)("select",{value:h,onChange:e=>{y(e.target.value),r&&x()},disabled:t,style:{padding:"0.6rem 1rem",borderRadius:"8px",border:"2px solid "+(t?"#e5e7eb":"#d1d5db"),outline:"none",cursor:t?"not-allowed":"pointer"},children:[(0,i.jsx)("option",{value:"beginner",children:"\ud83c\udf31 Beginner (Simple)"}),(0,i.jsx)("option",{value:"intermediate",children:"\ud83d\udcd6 Intermediate (Standard)"}),(0,i.jsx)("option",{value:"advanced",children:"\ud83d\ude80 Advanced (Technical)"})]})]}),(0,i.jsx)("button",{onClick:async()=>{if(r)return void x();const t=b.current;if(t){a(!0),g(null),u(0),_(`Preparing to personalize for ${h}...`),c||p(t.innerHTML);try{const i=(e=>{const n=3e3,t=[];if(e.children.length>0){let s="";Array.from(e.children).forEach(e=>{const o=e.outerHTML;s.length+o.length>n&&s.length>0&&(t.push(s),s=""),s+=o}),s&&t.push(s)}else{const s=e.innerHTML;for(let e=0;e<s.length;e+=n)t.push(s.substring(e,e+n))}return t})(t);if(console.log(`Personalizing in ${i.length} chunks`),0===i.length)throw new Error("Page content is empty");let r="";v.current&&v.current.abort(),v.current=new AbortController;for(let t=0;t<i.length;t++){const a=i[t],l=Math.round(t/i.length*100);u(l),_(`Adapting section ${t+1} of ${i.length}...`);try{const t=await o.u.post("/api/v1/personalize",{content:a,user_id:n?parseInt(n):1,learning_level:h,chapter_context:e,content_type:"partial_chapter"});r+=t.personalized_content,await new Promise(e=>setTimeout(e,150))}catch(s){console.error(`Error in chunk ${t}:`,s),r+=a}}u(100),_("Finalizing changes..."),setTimeout(()=>{t&&(t.style.transition="opacity 0.3s ease-in-out",t.style.opacity="0",setTimeout(()=>{const e=`\n              <div class="alert alert--success margin-bottom--md" style="padding: 1rem; border-radius: 8px; background-color: ${j()}20; border: 1px solid ${j()}; margin-bottom: 20px;">\n                <strong>${k()} Content Adapted: ${h.charAt(0).toUpperCase()+h.slice(1)} Level</strong>\n                <div style="font-size: 0.85em; margin-top: 5px;">Content has been simplified and examples adjusted for your level.</div>\n              </div>\n            `;t.innerHTML=e+r,t.style.opacity="1"},300)),l(!0),a(!1),_("")},500)}catch(f){console.error("Personalization Error:",f);const n=f.response?.data?.detail||f.message||"Process failed";g(`Failed: ${n}`),a(!1),u(0)}}else g("Content to personalize not found.")},disabled:t,style:{padding:"1rem 2rem",backgroundColor:r?"#059669":j(),color:"white",border:"none",borderRadius:"12px",cursor:t?"wait":"pointer",fontWeight:"700",fontSize:"1.05rem",display:"flex",alignItems:"center",gap:"0.75rem",transition:"all 0.2s",opacity:t?.8:1,boxShadow:"0 2px 4px rgba(0,0,0,0.1)"},children:t?(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("span",{style:{width:"18px",height:"18px",border:"3px solid rgba(255,255,255,0.3)",borderTop:"3px solid white",borderRadius:"50%",animation:"spin 1s linear infinite",display:"inline-block"}}),(0,i.jsx)("span",{children:"Processing..."})]}):r?(0,i.jsx)(i.Fragment,{children:"\u21a9\ufe0f Restore Original"}):(0,i.jsx)(i.Fragment,{children:"\u2728 Personalize Content"})}),t&&(0,i.jsxs)("div",{style:{width:"100%",marginTop:"0.5rem"},children:[(0,i.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",fontSize:"0.9rem",color:"#666",marginBottom:"4px"},children:[(0,i.jsx)("span",{children:m}),(0,i.jsxs)("span",{children:[d,"%"]})]}),(0,i.jsx)("div",{style:{width:"100%",height:"6px",backgroundColor:"#e5e7eb",borderRadius:"3px",overflow:"hidden"},children:(0,i.jsx)("div",{style:{width:`${d}%`,height:"100%",backgroundColor:j(),transition:"width 0.3s ease"}})})]}),!t&&(0,i.jsx)("p",{style:{margin:0,fontSize:"0.9rem",color:f?"#dc2626":"#6b7280",marginTop:"0.5rem"},children:f?`\u26a0\ufe0f ${f}`:r?"\u2705 Content updated based on your preferences.":"AI will rewrite the content to match your selected difficulty level."})]}),(0,i.jsx)("style",{children:"\n        @keyframes spin { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }\n      "})]})}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const o={},i=s.createContext(o);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(i.Provider,{value:n},e.children)}},9815:(e,n,t)=>{t(6540),t(9345),t(4848)}}]);