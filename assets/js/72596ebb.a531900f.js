"use strict";(globalThis.webpackChunkbook_ui=globalThis.webpackChunkbook_ui||[]).push([[365],{3113:(e,n,t)=>{t.d(n,{A:()=>s});var a=t(6540),o=t(4746),i=t(4848);const s=({chapterId:e,userId:n="demo-user"})=>{const[t,s]=(0,a.useState)(!1),[r,c]=(0,a.useState)(!1),[l,d]=(0,a.useState)("ur"),[p,m]=(0,a.useState)(""),[u,_]=(0,a.useState)(0),[g,f]=(0,a.useState)(""),[h,b]=(0,a.useState)(null),v=(0,a.useRef)(null),x=(0,a.useRef)(null);(0,a.useEffect)(()=>{if("undefined"==typeof window)return;const e=()=>{const e=[".theme-doc-markdown",".markdown","article","main .container",'[class*="docItemContainer"]'];for(const n of e){const e=document.querySelector(n);if(e&&e.textContent&&e.textContent.length>50)return void(v.current=e)}};e();const n=new MutationObserver(()=>{v.current||e()}),t=document.querySelector("body");return t&&n.observe(t,{childList:!0,subtree:!0}),()=>{n.disconnect(),x.current&&x.current.abort()}},[e]);const y=()=>{const e=v.current;e&&p&&(e.style.transition="opacity 0.3s ease-in-out",e.style.opacity="0",setTimeout(()=>{e.innerHTML=p,e.style.direction="ltr",e.style.textAlign="left",e.style.opacity="1",c(!1),b(null)},300))};return(0,i.jsx)("div",{style:{margin:"2rem auto",padding:"1.5rem",border:"2px solid #e5e7eb",borderRadius:"12px",backgroundColor:"linear-gradient(-45deg,  #ff0055,  #7000ff,  #009c98, #000000)",boxShadow:"0 4px 6px -1px rgba(0, 0, 0, 0.1)",textAlign:"center",maxWidth:"700px",fontFamily:"system-ui, sans-serif"},children:(0,i.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"center",gap:"1rem"},children:[(0,i.jsxs)("div",{style:{display:"flex",gap:"10px",alignItems:"center",flexWrap:"wrap",justifyContent:"center"},children:[(0,i.jsxs)("select",{value:l,onChange:e=>{d(e.target.value),r&&y()},disabled:t,style:{padding:"8px",borderRadius:"5px",border:"1px solid #ccc"},children:[(0,i.jsx)("option",{value:"ur",children:"\ud83c\uddf5\ud83c\uddf0 \u0627\u0631\u062f\u0648 (Urdu)"}),(0,i.jsx)("option",{value:"ar",children:"\ud83c\uddf8\ud83c\udde6 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 (Arabic)"}),(0,i.jsx)("option",{value:"es",children:"\ud83c\uddea\ud83c\uddf8 Espa\xf1ol (Spanish)"}),(0,i.jsx)("option",{value:"fr",children:"\ud83c\uddeb\ud83c\uddf7 Fran\xe7ais (French)"})]}),(0,i.jsx)("button",{onClick:async()=>{if(r)return void y();const e=v.current;if(e){s(!0),b(null),_(0),f("Preparing content..."),p||m(e.innerHTML);try{const t=(e=>{const n=3500,t=[];if(e.children.length>0){let a="";Array.from(e.children).forEach(e=>{const o=e.outerHTML;a.length+o.length>n&&a.length>0&&(t.push(a),a=""),a+=o}),a&&t.push(a)}else{const a=e.innerHTML;for(let e=0;e<a.length;e+=n)t.push(a.substring(e,e+n))}return t})(e);if(console.log(`Total chunks to translate: ${t.length}`),0===t.length)throw new Error("No content to translate");let a="";x.current&&x.current.abort(),x.current=new AbortController;for(let e=0;e<t.length;e++){const i=t[e],s=Math.round(e/t.length*100);_(s),f(`Translating part ${e+1} of ${t.length}...`);try{const e=await o.u.post("/api/v1/translate",{text:i,target_language:l,source_language:"en",preserve_formatting:!0});a+=e.translated_text,await new Promise(e=>setTimeout(e,200))}catch(n){console.error(`Error in chunk ${e}:`,n),a+=i}}_(100),f("Finalizing..."),setTimeout(()=>{e&&(e.style.opacity="0",setTimeout(()=>{const n="ur"===l||"ar"===l;e.innerHTML='\n              <div class="alert alert--success margin-bottom--md" style="direction: ltr; text-align: left; padding: 10px; border: 1px solid green; border-radius: 8px; margin-bottom: 20px;">\n                <strong>\u2705 Translated successfully</strong>\n              </div>\n            '+a,e.style.direction=n?"rtl":"ltr",e.style.textAlign=n?"right":"left",e.style.opacity="1"},300)),c(!0),s(!1),f("")},500)}catch(t){console.error("Translation Error:",t),b("Translation failed. Please try again later."),s(!1),_(0)}}else b("Content not found.")},disabled:t,style:{padding:"10px 20px",backgroundColor:r?"#10b981":"#4f46e5",color:"white",border:"none",borderRadius:"6px",cursor:t?"not-allowed":"pointer",fontWeight:"bold",opacity:t?.7:1},children:t?"Translating...":r?"Restore Original":"Translate Page"})]}),t&&(0,i.jsxs)("div",{style:{width:"100%",marginTop:"10px"},children:[(0,i.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",fontSize:"0.8rem",marginBottom:"5px"},children:[(0,i.jsx)("span",{children:g}),(0,i.jsxs)("span",{children:[u,"%"]})]}),(0,i.jsx)("div",{style:{width:"100%",height:"8px",backgroundColor:"#eee",borderRadius:"4px",overflow:"hidden"},children:(0,i.jsx)("div",{style:{width:`${u}%`,height:"100%",backgroundColor:"#4f46e5",transition:"width 0.3s ease"}})})]}),h&&(0,i.jsxs)("p",{style:{color:"red",marginTop:"10px",fontSize:"0.9rem"},children:["\u26a0\ufe0f ",h]})]})})}},5125:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>c,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"chapter-5/vision-language-action","title":"Chapter 5: Vision-Language-Action (VLA)","description":"Overview","source":"@site/docs/chapter-5/vision-language-action.mdx","sourceDirName":"chapter-5","slug":"/chapter-5/vision-language-action","permalink":"/ai-spec-hackathone/docs/chapter-5/vision-language-action","draft":false,"unlisted":false,"editUrl":"https://github.com/rabiasohail098/ai-spec-hackathone/tree/main/docs/chapter-5/vision-language-action.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Chapter 5: Vision-Language-Action (VLA)","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/ai-spec-hackathone/docs/chapter-4/ai-robot-brain-isaac"},"next":{"title":"Chapter 6: The Autonomous Humanoid Capstone Project","permalink":"/ai-spec-hackathone/docs/chapter-6/autonomous-humanoid-capstone"}}');var o=t(4848),i=t(8453),s=(t(9815),t(7561)),r=t(3113);const c={title:"Chapter 5: Vision-Language-Action (VLA)",sidebar_position:5},l="Chapter 5: Vision-Language-Action (VLA)",d={},p=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Vision-Language-Action (VLA)",id:"introduction-to-vision-language-action-vla",level:2},{value:"The VLA Paradigm",id:"the-vla-paradigm",level:3},{value:"Why VLA Matters for Physical AI",id:"why-vla-matters-for-physical-ai",level:3},{value:"Voice Command Processing with OpenAI Whisper",id:"voice-command-processing-with-openai-whisper",level:2},{value:"Introduction to Voice-to-Action Pipeline",id:"introduction-to-voice-to-action-pipeline",level:3},{value:"Advanced Voice Processing with Context Integration",id:"advanced-voice-processing-with-context-integration",level:3},{value:"Cognitive Planning: Natural Language to Robot Actions",id:"cognitive-planning-natural-language-to-robot-actions",level:2},{value:"The Cognitive Planning Architecture",id:"the-cognitive-planning-architecture",level:3},{value:"Natural Language Understanding for Robotics",id:"natural-language-understanding-for-robotics",level:3},{value:"Vision-Language Integration",id:"vision-language-integration",level:2},{value:"Multimodal Perception System",id:"multimodal-perception-system",level:3},{value:"Vision-Language-Action Coordination System",id:"vision-language-action-coordination-system",level:3},{value:"Implementation: Complete VLA System",id:"implementation-complete-vla-system",level:2},{value:"Integrated VLA System",id:"integrated-vla-system",level:3},{value:"VLA Model Architectures",id:"vla-model-architectures",level:2},{value:"Modern VLA Model Overview",id:"modern-vla-model-overview",level:3},{value:"Performance and Evaluation Metrics",id:"performance-and-evaluation-metrics",level:2},{value:"VLA System Evaluation",id:"vla-system-evaluation",level:3},{value:"Real-World Applications and Challenges",id:"real-world-applications-and-challenges",level:2},{value:"VLA in Human-Robot Interaction",id:"vla-in-human-robot-interaction",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Practice Questions",id:"practice-questions",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2}];function m(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-5-vision-language-action-vla",children:"Chapter 5: Vision-Language-Action (VLA)"})}),"\n",(0,o.jsx)(s.A,{chapterId:"chapter-5"}),"\n",(0,o.jsx)(r.A,{chapterId:"chapter-5"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"The Vision-Language-Action (VLA) paradigm represents the convergence of three critical AI domains that enable robots to understand, interpret, and act upon human commands in natural language while perceiving and manipulating the physical world. This chapter explores how modern AI systems integrate visual perception, natural language understanding, and physical action to create robots that can follow complex human instructions, plan tasks, and execute physical manipulations. Understanding VLA is fundamental for developing robots that can seamlessly collaborate with humans in natural environments."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand the Vision-Language-Action (VLA) paradigm and its importance in Physical AI"}),"\n",(0,o.jsx)(n.li,{children:"Implement voice command processing using OpenAI Whisper"}),"\n",(0,o.jsx)(n.li,{children:"Design cognitive planning systems that translate natural language commands into robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Integrate multimodal perception with language understanding"}),"\n",(0,o.jsx)(n.li,{children:"Create task planning systems for complex robot behaviors"}),"\n",(0,o.jsx)(n.li,{children:"Understand the architecture of VLA models and their training requirements"}),"\n",(0,o.jsx)(n.li,{children:"Implement vision-language-action coordination for humanoid robots"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-vision-language-action-vla",children:"Introduction to Vision-Language-Action (VLA)"}),"\n",(0,o.jsx)(n.h3,{id:"the-vla-paradigm",children:"The VLA Paradigm"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) represents a unified approach to embodied AI where:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision"})," enables robots to perceive and understand their environment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language"})," allows for natural communication with humans"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action"})," provides the capability to physically manipulate the world"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Vision-Language-Action (VLA) Cycle\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    1. PERCEIVE    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Environment   \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2192 \u2502    Robot\'s      \u2502\n\u2502    (Physical    \u2502                   \u2502  Perception     \u2502\n\u2502    World)       \u2502 \u2190 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502   (Cameras,    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     6. ACT        \u2502  LiDAR, etc.)  \u2502\n        \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502 2. INTERPRET                       \u2502\n        \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Language      \u2502 \u2190 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502   Cognitive     \u2502\n\u2502   Command       \u2502     3. PLAN       \u2502   Planning      \u2502\n\u2502   ("Clean the   \u2502 \u2192 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502   System        \u2502\n\u2502   room")        \u2502     4. SEQUENCE   \u2502   (Transform    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502   NL to Actions) \u2502\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                              \u2502\n                                        5. EXECUTE\n                                              \u25bc\n                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                      \u2502    Physical     \u2502\n                                      \u2502    Actions      \u2502\n                                      \u2502   (Movement,    \u2502\n                                      \u2502   Manipulation) \u2502\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,o.jsx)(n.h3,{id:"why-vla-matters-for-physical-ai",children:"Why VLA Matters for Physical AI"}),"\n",(0,o.jsx)(n.p,{children:"Traditional robotics approaches separated perception, planning, and action systems, leading to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fragmented Understanding"}),": Robots could see but not understand commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Limited Interaction"}),": Humans needed technical knowledge to operate robots"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Rigid Behavior"}),": Robots could not adapt to natural language instructions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Poor Generalization"}),": Systems couldn't handle novel situations described in language"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"VLA addresses these challenges by creating unified systems that:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand human commands in natural language"}),"\n",(0,o.jsx)(n.li,{children:"Perceive the environment in context of the command"}),"\n",(0,o.jsx)(n.li,{children:"Plan and execute actions to fulfill the command"}),"\n",(0,o.jsx)(n.li,{children:"Adapt to new situations described in language"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-processing-with-openai-whisper",children:"Voice Command Processing with OpenAI Whisper"}),"\n",(0,o.jsx)(n.h3,{id:"introduction-to-voice-to-action-pipeline",children:"Introduction to Voice-to-Action Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The first component of the VLA paradigm involves processing human voice commands into actionable instructions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# voice_command_processor.py\nimport openai\nimport speech_recognition as sr\nimport numpy as np\nfrom typing import Dict, List, Optional\nimport threading\nimport queue\n\nclass VoiceCommandProcessor:\n    def __init__(self, api_key: str, model: str = \"whisper-1\"):\n        \"\"\"\n        Initialize voice command processor using OpenAI Whisper\n        \"\"\"\n        openai.api_key = api_key\n        self.model = model\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.command_queue = queue.Queue()\n        \n        # Initialize speech recognition parameters\n        self.recognizer.energy_threshold = 300  # Adjust based on environment\n        self.recognizer.dynamic_energy_threshold = True\n        \n        self.is_listening = False\n        self.listening_thread = None\n\n    def start_listening(self):\n        \"\"\"Start continuous voice command listening\"\"\"\n        self.is_listening = True\n        self.listening_thread = threading.Thread(target=self._listen_loop)\n        self.listening_thread.start()\n\n    def stop_listening(self):\n        \"\"\"Stop voice command listening\"\"\"\n        self.is_listening = False\n        if self.listening_thread:\n            self.listening_thread.join()\n\n    def _listen_loop(self):\n        \"\"\"Continuous listening loop\"\"\"\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n            print(\"Listening for voice commands...\")\n\n            while self.is_listening:\n                try:\n                    # Listen for audio with timeout\n                    audio = self.recognizer.listen(source, timeout=1.0)\n                    \n                    # Process audio and get transcript\n                    transcript = self._transcribe_audio(audio)\n                    \n                    if transcript:\n                        print(f\"Recognized: {transcript}\")\n                        # Process the command\n                        processed_command = self.process_command(transcript)\n                        \n                        # Add to queue for higher-level processing\n                        self.command_queue.put(processed_command)\n\n                except sr.WaitTimeoutError:\n                    # No speech detected, continue listening\n                    continue\n                except sr.UnknownValueError:\n                    # Could not understand audio\n                    print(\"Could not understand audio\")\n                except Exception as e:\n                    print(f\"Error in voice recognition: {e}\")\n\n    def _transcribe_audio(self, audio) -> Optional[str]:\n        \"\"\"Transcribe audio to text using OpenAI Whisper\"\"\"\n        try:\n            # Save audio to temporary file for Whisper API\n            audio_data = audio.get_raw_data()\n            with open(\"temp_audio.wav\", \"wb\") as f:\n                f.write(audio_data)\n\n            # Transcribe using Whisper API\n            with open(\"temp_audio.wav\", \"rb\") as audio_file:\n                transcript = openai.Audio.transcribe(\n                    model=self.model,\n                    file=audio_file,\n                    response_format=\"text\"\n                )\n            \n            return transcript.strip()\n        except Exception as e:\n            print(f\"Error transcribing audio: {e}\")\n            return None\n\n    def process_command(self, transcript: str) -> Dict:\n        \"\"\"Process natural language command and convert to robot actions\"\"\"\n        # This would involve NLP processing to understand the command\n        # and convert it to actionable robot instructions\n        \n        command_analysis = {\n            'raw_text': transcript,\n            'intent': self.analyze_intent(transcript),\n            'objects': self.extract_objects(transcript),\n            'actions': self.extract_actions(transcript),\n            'spatial_ref': self.extract_spatial_references(transcript)\n        }\n        \n        return command_analysis\n\n    def analyze_intent(self, text: str) -> str:\n        \"\"\"Analyze the intent of the voice command\"\"\"\n        text_lower = text.lower()\n        \n        if any(word in text_lower for word in ['clean', 'tidy', 'organize', 'pick up']):\n            return 'cleaning'\n        elif any(word in text_lower for word in ['move', 'go', 'navigate', 'walk']):\n            return 'navigation'\n        elif any(word in text_lower for word in ['grasp', 'pick', 'take', 'bring']):\n            return 'manipulation'\n        elif any(word in text_lower for word in ['find', 'locate', 'search', 'look']):\n            return 'search'\n        else:\n            return 'unknown'\n\n    def extract_objects(self, text: str) -> List[str]:\n        \"\"\"Extract objects mentioned in the command\"\"\"\n        # In a real implementation, this would use more sophisticated NLP\n        # For now, using simple keyword matching\n        common_objects = [\n            'box', 'bottle', 'cup', 'book', 'chair', 'table', 'trash', \n            'dust', 'floor', 'room', 'object', 'item'\n        ]\n        \n        found_objects = []\n        text_lower = text.lower()\n        for obj in common_objects:\n            if obj in text_lower:\n                found_objects.append(obj)\n        \n        return found_objects\n\n    def extract_actions(self, text: str) -> List[str]:\n        \"\"\"Extract action verbs from the command\"\"\"\n        action_words = [\n            'pick', 'up', 'take', 'go', 'move', 'clean', 'tidy', \n            'organize', 'put', 'drop', 'carry', 'transport'\n        ]\n        \n        found_actions = []\n        text_lower = text.lower()\n        words = text_lower.split()\n        \n        for word in words:\n            if word in action_words:\n                found_actions.append(word)\n        \n        return found_actions\n\n    def extract_spatial_references(self, text: str) -> List[str]:\n        \"\"\"Extract spatial references (locations, directions)\"\"\"\n        spatial_refs = [\n            'here', 'there', 'left', 'right', 'front', 'back', \n            'up', 'down', 'near', 'far', 'table', 'floor', 'desk'\n        ]\n        \n        found_refs = []\n        text_lower = text.lower()\n        for ref in spatial_refs:\n            if ref in text_lower:\n                found_refs.append(ref)\n        \n        return found_refs\n\n# Usage example\ndef main():\n    processor = VoiceCommandProcessor(api_key=\"your-openai-api-key\")\n    processor.start_listening()\n    \n    try:\n        while True:\n            if not processor.command_queue.empty():\n                command = processor.command_queue.get()\n                print(f\"Processed command: {command}\")\n                # Here you would forward the command to the robot's action system\n    except KeyboardInterrupt:\n        processor.stop_listening()\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"advanced-voice-processing-with-context-integration",children:"Advanced Voice Processing with Context Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# contextual_voice_processor.py\nimport openai\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport asyncio\n\n@dataclass\nclass RobotState:\n    """Represents the current state of the robot"""\n    position: List[float]\n    orientation: List[float]\n    battery_level: float\n    current_task: Optional[str]\n    detected_objects: List[Dict]\n    environment_map: Optional[object]\n\nclass ContextualVoiceProcessor:\n    def __init__(self, api_key: str):\n        openai.api_key = api_key\n        self.robot_state = RobotState(\n            position=[0.0, 0.0, 0.0],\n            orientation=[0.0, 0.0, 0.0, 1.0],\n            battery_level=100.0,\n            current_task=None,\n            detected_objects=[],\n            environment_map=None\n        )\n        self.conversation_history = []\n\n    def process_command_with_context(self, user_command: str) -> Dict:\n        """Process command considering robot\'s current context"""\n        # Create contextual prompt for AI\n        context_prompt = self.create_contextual_prompt(user_command)\n        \n        response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": self.get_system_prompt()},\n                {"role": "user", "content": context_prompt}\n            ],\n            functions=[\n                {\n                    "name": "parse_command",\n                    "description": "Parse user command into executable robot actions",\n                    "parameters": {\n                        "type": "object",\n                        "properties": {\n                            "intent": {"type": "string", "enum": ["navigate", "manipulate", "search", "clean"]},\n                            "target_object": {"type": "string"},\n                            "target_location": {"type": "string"},\n                            "action_sequence": {"type": "array", "items": {"type": "string"}},\n                            "confidence": {"type": "number", "minimum": 0, "maximum": 1}\n                        }\n                    }\n                }\n            ],\n            function_call={"name": "parse_command"}\n        )\n        \n        # Extract function arguments from response\n        message = response.choices[0].message\n        if message.function_call:\n            import json\n            args = json.loads(message.function_call.arguments)\n            return args\n        \n        return {"error": "Could not parse command"}\n\n    def create_contextual_prompt(self, user_command: str) -> str:\n        """Create a prompt that includes robot\'s context"""\n        prompt = f"""\n        User Command: "{user_command}"\n        \n        Robot Context:\n        - Position: {self.robot_state.position}\n        - Battery: {self.robot_state.battery_level}%\n        - Current Task: {self.robot_state.current_task}\n        - Detected Objects: {[obj[\'name\'] for obj in self.robot_state.detected_objects]}\n        \n        Please parse this command into specific robot actions considering the current context.\n        """\n        return prompt\n\n    def get_system_prompt(self) -> str:\n        """System prompt for command understanding"""\n        return """\n        You are an assistant that helps parse natural language commands into robot actions.\n        Consider the robot\'s current state and environment when interpreting commands.\n        Be specific about objects, locations, and actions needed.\n        """\n'})}),"\n",(0,o.jsx)(n.h2,{id:"cognitive-planning-natural-language-to-robot-actions",children:"Cognitive Planning: Natural Language to Robot Actions"}),"\n",(0,o.jsx)(n.h3,{id:"the-cognitive-planning-architecture",children:"The Cognitive Planning Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Cognitive planning bridges the gap between high-level language commands and low-level robot actions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# cognitive_planning.py\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Optional\nimport networkx as nx\n\nclass TaskType(Enum):\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    PERCEPTION = \"perception\"\n    COMPOSITE = \"composite\"\n\n@dataclass\nclass Task:\n    \"\"\"Represents a single robot task\"\"\"\n    id: str\n    type: TaskType\n    description: str\n    dependencies: List[str]  # IDs of tasks that must be completed first\n    parameters: Dict[str, Any]\n    priority: int = 1\n\n@dataclass\nclass Action:\n    \"\"\"Low-level robot action\"\"\"\n    command: str\n    parameters: Dict[str, Any]\n    execution_time: float\n\nclass CognitivePlanner:\n    def __init__(self):\n        self.task_graph = nx.DiGraph()\n        self.action_library = self._initialize_action_library()\n        \n    def _initialize_action_library(self) -> Dict[str, List[Action]]:\n        \"\"\"Initialize library of basic robot actions\"\"\"\n        return {\n            'navigation': [\n                Action('move_to', {'target_pose': [0,0,0,0,0,0,1]}, 2.0),\n                Action('rotate_to', {'target_orientation': [0,0,0,1]}, 1.0),\n                Action('navigate_path', {'waypoints': []}, 5.0)\n            ],\n            'manipulation': [\n                Action('open_gripper', {}, 0.5),\n                Action('close_gripper', {'force': 50}, 0.5),\n                Action('move_arm', {'target_pose': [0,0,0,0,0,0]}, 2.0)\n            ],\n            'perception': [\n                Action('look_at', {'target_point': [0,0,0]}, 1.0),\n                Action('scan_environment', {}, 3.0),\n                Action('detect_objects', {'target_class': 'any'}, 2.0)\n            ]\n        }\n    \n    def plan_from_command(self, command_analysis: Dict) -> List[Action]:\n        \"\"\"Plan robot actions from command analysis\"\"\"\n        # Create high-level task decomposition based on command\n        tasks = self._decompose_command(command_analysis)\n        \n        # Build task dependency graph\n        for task in tasks:\n            self.task_graph.add_node(task.id, task_obj=task)\n            for dep_id in task.dependencies:\n                # Ensure dependency exists, add edge\n                if self.task_graph.has_node(dep_id):\n                    self.task_graph.add_edge(dep_id, task.id)\n        \n        # Topologically sort tasks based on dependencies\n        ordered_task_ids = list(nx.topological_sort(self.task_graph))\n        \n        # Convert tasks to executable actions\n        actions = []\n        for task_id in ordered_task_ids:\n            task = self.task_graph.nodes[task_id]['task_obj']\n            task_actions = self._task_to_actions(task)\n            actions.extend(task_actions)\n        \n        return actions\n    \n    def _decompose_command(self, command_analysis: Dict) -> List[Task]:\n        \"\"\"Decompose natural language command into robot tasks\"\"\"\n        intent = command_analysis.get('intent', 'unknown')\n        objects = command_analysis.get('objects', [])\n        actions = command_analysis.get('actions', [])\n        spatial_refs = command_analysis.get('spatial_ref', [])\n        \n        tasks = []\n        \n        if intent == 'navigation':\n            # Add navigation tasks\n            for obj in objects:\n                tasks.append(Task(\n                    id=f\"find_{obj}\",\n                    type=TaskType.PERCEPTION,\n                    description=f\"Locate {obj} in environment\",\n                    dependencies=[],\n                    parameters={'object_class': obj}\n                ))\n            \n            # Add navigation task\n            tasks.append(Task(\n                id=\"navigate_to_target\",\n                type=TaskType.NAVIGATION,\n                description=\"Navigate to target location\",\n                dependencies=[f\"find_{obj}\" for obj in objects],\n                parameters={'target_objects': objects}\n            ))\n        \n        elif intent == 'manipulation':\n            # Add object detection task\n            for obj in objects:\n                tasks.append(Task(\n                    id=f\"detect_{obj}\",\n                    type=TaskType.PERCEPTION,\n                    description=f\"Detect and localize {obj}\",\n                    dependencies=[],\n                    parameters={'object_class': obj}\n                ))\n            \n            # Add approach task\n            tasks.append(Task(\n                id=\"approach_object\",\n                type=TaskType.NAVIGATION,\n                description=\"Approach detected object\",\n                dependencies=[f\"detect_{obj}\" for obj in objects],\n                parameters={'target_objects': objects}\n            ))\n            \n            # Add manipulation task\n            tasks.append(Task(\n                id=\"manipulate_object\",\n                type=TaskType.MANIPULATION,\n                description=\"Manipulate target object\",\n                dependencies=[\"approach_object\"],\n                parameters={'action': 'grasp', 'objects': objects}\n            ))\n        \n        elif intent == 'cleaning':\n            # Complex cleaning task involving multiple subtasks\n            tasks.append(Task(\n                id=\"scan_area\",\n                type=TaskType.PERCEPTION,\n                description=\"Scan area to identify cleaning targets\",\n                dependencies=[],\n                parameters={'scan_area': 'room'}\n            ))\n            \n            tasks.append(Task(\n                id=\"plan_cleaning_path\",\n                type=TaskType.COMPOSITE,\n                description=\"Plan path for systematic cleaning\",\n                dependencies=[\"scan_area\"],\n                parameters={'coverage_strategy': 'grid'}\n            ))\n        \n        return tasks\n    \n    def _task_to_actions(self, task: Task) -> List[Action]:\n        \"\"\"Convert high-level task to sequence of low-level actions\"\"\"\n        if task.type == TaskType.PERCEPTION:\n            return self._perception_task_to_actions(task)\n        elif task.type == TaskType.NAVIGATION:\n            return self._navigation_task_to_actions(task)\n        elif task.type == TaskType.MANIPULATION:\n            return self._manipulation_task_to_actions(task)\n        else:\n            # For composite tasks, decompose further\n            return self._composite_task_to_actions(task)\n    \n    def _perception_task_to_actions(self, task: Task) -> List[Action]:\n        \"\"\"Convert perception task to actions\"\"\"\n        actions = []\n        \n        if task.parameters.get('object_class'):\n            actions.append(Action(\n                'detect_objects',\n                {'target_class': task.parameters['object_class']},\n                2.0\n            ))\n        \n        return actions\n    \n    def _navigation_task_to_actions(self, task: Task) -> List[Action]:\n        \"\"\"Convert navigation task to actions\"\"\"\n        actions = []\n        \n        if task.parameters.get('target_objects'):\n            # Navigate to detected objects\n            actions.append(Action(\n                'move_to_detected_object',\n                {'object_class': task.parameters['target_objects'][0]},\n                5.0\n            ))\n        \n        return actions\n    \n    def _manipulation_task_to_actions(self, task: Task) -> List[Action]:\n        \"\"\"Convert manipulation task to actions\"\"\"\n        actions = []\n        \n        if task.parameters.get('action') == 'grasp':\n            actions.extend([\n                Action('approach_object', {}, 3.0),\n                Action('grasp_object', {'object_class': task.parameters.get('objects', [None])[0]}, 2.0)\n            ])\n        \n        return actions\n    \n    def _composite_task_to_actions(self, task: Task) -> List[Action]:\n        \"\"\"Convert composite task to actions\"\"\"\n        # Handle complex tasks that require multiple sub-operations\n        if task.parameters.get('coverage_strategy') == 'grid':\n            return [\n                Action('execute_cleaning_pattern', {'pattern': 'grid'}, 20.0)\n            ]\n        \n        return []\n"})}),"\n",(0,o.jsx)(n.h3,{id:"natural-language-understanding-for-robotics",children:"Natural Language Understanding for Robotics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# nlu_robotics.py\nimport spacy\nimport re\nfrom typing import Dict, List, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass ParsedCommand:\n    """Represents a parsed natural language command"""\n    action: str\n    target_objects: List[str]\n    target_location: Optional[str]\n    adverbial_phrases: List[str]  # e.g., "carefully", "slowly"\n    spatial_relations: List[str]  # e.g., "left of", "on top of"\n\nclass NaturalLanguageUnderstanding:\n    def __init__(self):\n        # Load spaCy model for NLP processing\n        try:\n            self.nlp = spacy.load("en_core_web_sm")\n        except OSError:\n            print("Please install spaCy English model: python -m spacy download en_core_web_sm")\n            self.nlp = None\n    \n    def parse_command(self, command: str) -> ParsedCommand:\n        """Parse natural language command into structured representation"""\n        if not self.nlp:\n            return self._fallback_parse(command)\n        \n        doc = self.nlp(command)\n        \n        # Extract action (verb)\n        action = self._extract_action(doc)\n        \n        # Extract target objects (nouns)\n        target_objects = self._extract_objects(doc)\n        \n        # Extract target location (prepositional phrases)\n        target_location = self._extract_location(doc)\n        \n        # Extract adverbial phrases\n        adverbial_phrases = self._extract_adverbial_phrases(doc)\n        \n        # Extract spatial relations\n        spatial_relations = self._extract_spatial_relations(doc)\n        \n        return ParsedCommand(\n            action=action,\n            target_objects=target_objects,\n            target_location=target_location,\n            adverbial_phrases=adverbial_phrases,\n            spatial_relations=spatial_relations\n        )\n    \n    def _extract_action(self, doc) -> str:\n        """Extract the main action from the command"""\n        # Find the root verb or main action\n        for token in doc:\n            if token.pos_ == "VERB" and token.dep_ == "ROOT":\n                return token.lemma_\n        \n        # Fallback: find first verb\n        for token in doc:\n            if token.pos_ == "VERB":\n                return token.lemma_\n        \n        return "unknown"\n    \n    def _extract_objects(self, doc) -> List[str]:\n        """Extract target objects from the command"""\n        objects = []\n        \n        for token in doc:\n            if token.pos_ in ["NOUN", "PROPN"] and token.dep_ in ["dobj", "pobj", "attr"]:\n                objects.append(token.text.lower())\n        \n        return objects\n    \n    def _extract_location(self, doc) -> Optional[str]:\n        """Extract target location from prepositional phrases"""\n        for token in doc:\n            if token.pos_ == "ADP":  # preposition\n                # Look for the object of the preposition\n                for child in token.children:\n                    if child.pos_ in ["NOUN", "PROPN"]:\n                        return f"{token.text} {child.text}"\n        \n        return None\n    \n    def _extract_adverbial_phrases(self, doc) -> List[str]:\n        """Extract adverbial phrases that modify the action"""\n        phrases = []\n        \n        for token in doc:\n            if token.pos_ == "ADV" or (token.pos_ == "ADV" and token.dep_ == "advmod"):\n                phrases.append(token.text)\n        \n        return phrases\n    \n    def _extract_spatial_relations(self, doc) -> List[str]:\n        """Extract spatial relationship expressions"""\n        relations = []\n        \n        # Pattern: "to the left of", "on top of", "next to", etc.\n        for i, token in enumerate(doc):\n            if token.text.lower() in ["to", "on", "in", "next", "near", "beside", "above", "below"]:\n                # Look for next tokens that form spatial phrases\n                phrase = token.text\n                for j in range(i+1, min(i+4, len(doc))):\n                    next_token = doc[j]\n                    if next_token.pos_ in ["DET", "ADP", "NOUN", "ADJ"]:\n                        phrase += f" {next_token.text}"\n                    else:\n                        break\n                relations.append(phrase)\n        \n        return relations\n    \n    def _fallback_parse(self, command: str) -> ParsedCommand:\n        """Fallback parsing using simple regex if spaCy is not available"""\n        # Simple keyword-based parsing\n        command_lower = command.lower()\n        \n        # Extract action keywords\n        action_keywords = [\n            "move", "go", "navigate", "pick", "grasp", "take", "bring", \n            "clean", "tidy", "organize", "find", "locate", "search"\n        ]\n        action = "unknown"\n        for keyword in action_keywords:\n            if keyword in command_lower:\n                action = keyword\n                break\n        \n        # Extract common objects\n        object_keywords = [\n            "box", "bottle", "cup", "book", "chair", "table", \n            "trash", "object", "item", "room", "area"\n        ]\n        objects = []\n        for keyword in object_keywords:\n            if keyword in command_lower:\n                objects.append(keyword)\n        \n        # Extract location keywords\n        location_keywords = [\n            "here", "there", "kitchen", "living room", "bedroom", \n            "table", "floor", "desk", "shelf", "cabinet"\n        ]\n        location = None\n        for keyword in location_keywords:\n            if keyword in command_lower:\n                location = keyword\n                break\n        \n        return ParsedCommand(\n            action=action,\n            target_objects=objects,\n            target_location=location,\n            adverbial_phrases=[],\n            spatial_relations=[]\n        )\n\n# Example usage\ndef demonstrate_nlu():\n    nlu = NaturalLanguageUnderstanding()\n    \n    commands = [\n        "Please carefully pick up the red cup from the table",\n        "Navigate to the kitchen and find the blue bottle",\n        "Clean the room by organizing the books on the shelf",\n        "Slowly move the box to the left of the chair"\n    ]\n    \n    for cmd in commands:\n        parsed = nlu.parse_command(cmd)\n        print(f"Command: {cmd}")\n        print(f"Parsed: Action={parsed.action}, Objects={parsed.target_objects}, Location={parsed.target_location}")\n        print("---")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,o.jsx)(n.h3,{id:"multimodal-perception-system",children:"Multimodal Perception System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# vision_language_integration.py\nimport cv2\nimport numpy as np\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport openai\n\nclass MultimodalPerceptionSystem:\n    def __init__(self):\n        # Initialize CLIP model for vision-language understanding\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Initialize object detection model\n        self.object_detector = self._initialize_detector()\n        \n        # Store environment context\n        self.current_scene = None\n        self.detected_objects = []\n        self.scene_description = \"\"\n    \n    def process_visual_input(self, image: np.ndarray) -> Dict:\n        \"\"\"Process visual input and create multimodal representation\"\"\"\n        # Convert numpy image to PIL for CLIP\n        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        \n        # Detect objects in the image\n        detections = self.object_detector.detect(image)\n        \n        # Use CLIP to understand the scene contextually\n        scene_context = self._get_scene_context(pil_image)\n        \n        # Create multimodal representation\n        multimodal_data = {\n            'image_features': self.clip_model.get_image_features(\n                **self.clip_processor(images=pil_image, return_tensors=\"pt\")\n            ),\n            'detected_objects': detections,\n            'scene_context': scene_context,\n            'image': image\n        }\n        \n        return multimodal_data\n    \n    def _get_scene_context(self, image: Image) -> str:\n        \"\"\"Get contextual description of the scene using CLIP\"\"\"\n        # This would typically use a text-to-image model or scene classifier\n        # For now, we'll simulate this with a placeholder\n        possible_scenes = [\n            \"kitchen environment with appliances\",\n            \"living room with furniture\",\n            \"office space with desk and chair\",\n            \"bedroom with bed and closet\",\n            \"workshop with tools and materials\"\n        ]\n        # In a real implementation, this would use a trained classifier\n        return \"indoor environment\"\n    \n    def match_command_to_visual_context(self, command: str, multimodal_data: Dict) -> Dict:\n        \"\"\"Match natural language command to visual context\"\"\"\n        # Extract objects mentioned in command\n        command_objects = self._extract_command_objects(command)\n        \n        # Find matching objects in visual scene\n        matching_objects = self._find_matching_objects(\n            command_objects, \n            multimodal_data['detected_objects']\n        )\n        \n        # Create action plan based on matched objects\n        action_plan = self._create_action_plan(command, matching_objects)\n        \n        return {\n            'matched_objects': matching_objects,\n            'action_plan': action_plan,\n            'confidence': len(matching_objects) / len(command_objects) if command_objects else 1.0\n        }\n    \n    def _extract_command_objects(self, command: str) -> List[str]:\n        \"\"\"Extract object references from command using NLP\"\"\"\n        # This would use the NLU system developed earlier\n        # For now, using simple keyword matching\n        common_objects = [\n            'cup', 'bottle', 'book', 'box', 'chair', 'table', \n            'laptop', 'phone', 'trash', 'food', 'drink'\n        ]\n        \n        found_objects = []\n        command_lower = command.lower()\n        for obj in common_objects:\n            if obj in command_lower:\n                found_objects.append(obj)\n        \n        return found_objects\n    \n    def _find_matching_objects(self, command_objects: List[str], \n                             detected_objects: List[Dict]) -> List[Dict]:\n        \"\"\"Find detected objects that match command references\"\"\"\n        matching_objects = []\n        \n        for cmd_obj in command_objects:\n            for det_obj in detected_objects:\n                if cmd_obj.lower() in det_obj['class'].lower():\n                    matching_objects.append(det_obj)\n        \n        return matching_objects\n    \n    def _create_action_plan(self, command: str, matching_objects: List[Dict]) -> List[Dict]:\n        \"\"\"Create detailed action plan for command execution\"\"\"\n        # Analyze command intent\n        intent = self._analyze_command_intent(command)\n        \n        action_plan = []\n        \n        if intent == 'manipulation':\n            for obj in matching_objects:\n                action_plan.extend([\n                    {\n                        'action': 'approach_object',\n                        'target': obj['bbox'],\n                        'description': f'Approach the {obj[\"class\"]}'\n                    },\n                    {\n                        'action': 'grasp_object',\n                        'target': obj['bbox'],\n                        'description': f'Grasp the {obj[\"class\"]}'\n                    }\n                ])\n        \n        elif intent == 'navigation':\n            if matching_objects:\n                action_plan.append({\n                    'action': 'navigate_to_object',\n                    'target': matching_objects[0]['bbox'],\n                    'description': f'Navigate to the {matching_objects[0][\"class\"]}'\n                })\n        \n        return action_plan\n    \n    def _analyze_command_intent(self, command: str) -> str:\n        \"\"\"Analyze the intent of the command\"\"\"\n        command_lower = command.lower()\n        \n        if any(word in command_lower for word in ['pick', 'grasp', 'take', 'grab']):\n            return 'manipulation'\n        elif any(word in command_lower for word in ['go', 'move', 'navigate', 'walk', 'approach']):\n            return 'navigation'\n        elif any(word in command_lower for word in ['find', 'locate', 'search']):\n            return 'search'\n        else:\n            return 'unknown'\n\n# Object detector placeholder\nclass PlaceholderDetector:\n    def detect(self, image):\n        \"\"\"Placeholder object detector that returns mock detections\"\"\"\n        # In reality, this would be a YOLO, Mask R-CNN, or similar detector\n        h, w, _ = image.shape\n        return [\n            {\n                'class': 'bottle',\n                'bbox': [w//2 - 25, h//2 - 50, w//2 + 25, h//2 + 50],\n                'confidence': 0.9\n            }\n        ]\n    \n    def _initialize_detector(self):\n        return PlaceholderDetector()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"vision-language-action-coordination-system",children:"Vision-Language-Action Coordination System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# vla_coordination.py\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport time\n\nclass ExecutionStatus(Enum):\n    PENDING = "pending"\n    EXECUTING = "executing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    CANCELLED = "cancelled"\n\n@dataclass\nclass ActionStep:\n    """Represents a single step in VLA execution"""\n    action_type: str\n    parameters: Dict\n    expected_duration: float\n    preconditions: List[str]  # What must be true before executing\n    postconditions: List[str]  # What should be true after executing\n    priority: int = 1\n\nclass VLAExecutionEngine:\n    def __init__(self):\n        self.current_action_plan = []\n        self.execution_status = ExecutionStatus.PENDING\n        self.start_time = None\n        self.current_step = 0\n        self.perception_system = MultimodalPerceptionSystem()\n        self.robot_interface = self._initialize_robot_interface()\n        \n    def _initialize_robot_interface(self):\n        """Initialize interface to robot hardware"""\n        # This would connect to actual robot ROS nodes\n        return {\n            \'navigation\': None,\n            \'manipulation\': None,\n            \'sensors\': None\n        }\n    \n    def execute_action_plan(self, action_plan: List[Dict]) -> bool:\n        """Execute a complete action plan from VLA processing"""\n        self.current_action_plan = [self._dict_to_action_step(step) for step in action_plan]\n        self.execution_status = ExecutionStatus.EXECUTING\n        self.start_time = time.time()\n        self.current_step = 0\n        \n        success = True\n        \n        while self.current_step < len(self.current_action_plan) and success:\n            step = self.current_action_plan[self.current_step]\n            \n            # Check preconditions\n            if not self._check_preconditions(step):\n                self.execution_status = ExecutionStatus.FAILED\n                success = False\n                break\n            \n            # Execute the action\n            step_success = self._execute_action_step(step)\n            \n            if not step_success:\n                self.execution_status = ExecutionStatus.FAILED\n                success = False\n                break\n            \n            # Verify postconditions\n            if not self._verify_postconditions(step):\n                self.execution_status = ExecutionStatus.FAILED\n                success = False\n                break\n            \n            # Move to next step\n            self.current_step += 1\n        \n        if success:\n            self.execution_status = ExecutionStatus.COMPLETED\n        else:\n            self.execution_status = ExecutionStatus.FAILED\n        \n        return success\n    \n    def _dict_to_action_step(self, step_dict: Dict) -> ActionStep:\n        """Convert dictionary representation to ActionStep"""\n        return ActionStep(\n            action_type=step_dict[\'action\'],\n            parameters=step_dict.get(\'parameters\', {}),\n            expected_duration=step_dict.get(\'expected_duration\', 2.0),\n            preconditions=step_dict.get(\'preconditions\', []),\n            postconditions=step_dict.get(\'postconditions\', []),\n            priority=step_dict.get(\'priority\', 1)\n        )\n    \n    def _check_preconditions(self, step: ActionStep) -> bool:\n        """Check if preconditions for action step are met"""\n        # This would check robot state, environment conditions, etc.\n        for condition in step.preconditions:\n            if not self._evaluate_condition(condition):\n                return False\n        return True\n    \n    def _execute_action_step(self, step: ActionStep) -> bool:\n        """Execute a single action step"""\n        print(f"Executing: {step.action_type} with parameters {step.parameters}")\n        \n        if step.action_type == \'navigate_to_object\':\n            return self._execute_navigation(step)\n        elif step.action_type == \'grasp_object\':\n            return self._execute_grasp(step)\n        elif step.action_type == \'approach_object\':\n            return self._execute_approach(step)\n        else:\n            print(f"Unknown action type: {step.action_type}")\n            return False\n    \n    def _verify_postconditions(self, step: ActionStep) -> bool:\n        """Verify that postconditions were achieved"""\n        # This would check robot state after action execution\n        for condition in step.postconditions:\n            if not self._evaluate_condition(condition):\n                return False\n        return True\n    \n    def _evaluate_condition(self, condition: str) -> bool:\n        """Evaluate a logical condition about robot state"""\n        # Placeholder implementation\n        # In reality, this would check actual robot sensors/state\n        return True\n    \n    def _execute_navigation(self, step: ActionStep) -> bool:\n        """Execute navigation action"""\n        # This would send navigation commands to robot\n        target = step.parameters.get(\'target\', [0, 0, 0])\n        print(f"Navigating to target: {target}")\n        \n        # Simulate navigation\n        time.sleep(2.0)  # Simulate navigation time\n        return True\n    \n    def _execute_grasp(self, step: ActionStep) -> bool:\n        """Execute grasping action"""\n        target = step.parameters.get(\'target\', [0, 0, 0])\n        print(f"Attempting to grasp object at: {target}")\n        \n        # Simulate grasping\n        time.sleep(2.0)  # Simulate grasping time\n        return True\n    \n    def _execute_approach(self, step: ActionStep) -> bool:\n        """Execute approach action"""\n        target = step.parameters.get(\'target\', [0, 0, 0])\n        print(f"Approaching object at: {target}")\n        \n        # Simulate approach\n        time.sleep(1.5)  # Simulate approach time\n        return True\n'})}),"\n",(0,o.jsx)(n.h2,{id:"implementation-complete-vla-system",children:"Implementation: Complete VLA System"}),"\n",(0,o.jsx)(n.h3,{id:"integrated-vla-system",children:"Integrated VLA System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# complete_vla_system.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, Pose\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport threading\nimport queue\n\nclass VLASystem(Node):\n    def __init__(self):\n        super().__init__('vla_system')\n        \n        # Initialize components\n        self.vision_processor = MultimodalPerceptionSystem()\n        self.nlu_system = NaturalLanguageUnderstanding()\n        self.cognitive_planner = CognitivePlanner()\n        self.vla_engine = VLAExecutionEngine()\n        self.cv_bridge = CvBridge()\n        \n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        self.command_sub = self.create_subscription(\n            String,\n            '/vla/command',\n            self.command_callback,\n            10\n        )\n        \n        # Create publishers\n        self.status_pub = self.create_publisher(\n            String,\n            '/vla/status',\n            10\n        )\n        \n        self.cmd_vel_pub = self.create_publisher(\n            Twist,\n            '/cmd_vel',\n            10\n        )\n        \n        # Internal state\n        self.current_image = None\n        self.command_queue = queue.Queue()\n        self.is_processing = False\n        \n        # Processing thread\n        self.processing_thread = threading.Thread(target=self.processing_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n        \n        self.get_logger().info('VLA System initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            self.current_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr2rgb\")\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\n    def command_callback(self, msg):\n        \"\"\"Process incoming voice commands\"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Received command: {command_text}')\n        \n        # Add to processing queue\n        self.command_queue.put(command_text)\n\n    def processing_loop(self):\n        \"\"\"Main processing loop for VLA system\"\"\"\n        while rclpy.ok():\n            try:\n                if not self.command_queue.empty():\n                    command = self.command_queue.get_nowait()\n                    \n                    if self.current_image is not None and not self.is_processing:\n                        self.is_processing = True\n                        self.process_command_with_image(command, self.current_image)\n                        self.is_processing = False\n                        \n            except queue.Empty:\n                pass\n            \n            # Small sleep to prevent busy waiting\n            time.sleep(0.1)\n\n    def process_command_with_image(self, command: str, image: np.ndarray):\n        \"\"\"Process a command with the corresponding image\"\"\"\n        try:\n            self.get_logger().info(f'Processing command: {command}')\n            \n            # Publish status\n            status_msg = String()\n            status_msg.data = f'Processing: {command}'\n            self.status_pub.publish(status_msg)\n            \n            # Step 1: Natural Language Understanding\n            self.get_logger().info('Step 1: Natural Language Understanding')\n            parsed_command = self.nlu_system.parse_command(command)\n            self.get_logger().info(f'Parsed: {parsed_command}')\n            \n            # Step 2: Vision Processing\n            self.get_logger().info('Step 2: Vision Processing')\n            multimodal_data = self.vision_processor.process_visual_input(image)\n            self.get_logger().info(f'Detected {len(multimodal_data[\"detected_objects\"])} objects')\n            \n            # Step 3: Vision-Language Matching\n            self.get_logger().info('Step 3: Vision-Language Matching')\n            command_vision_match = self.vision_processor.match_command_to_visual_context(\n                command, \n                multimodal_data\n            )\n            self.get_logger().info(f'Match confidence: {command_vision_match[\"confidence\"]}')\n            \n            # Step 4: Cognitive Planning\n            self.get_logger().info('Step 4: Cognitive Planning')\n            action_plan = self.cognitive_planner.plan_from_command({\n                'intent': self.cognitive_planner._decompose_command({'intent': parsed_command.action})[0].type.value if self.cognitive_planner._decompose_command({'intent': parsed_command.action}) else 'unknown',\n                'objects': parsed_command.target_objects,\n                'spatial_ref': parsed_command.spatial_relations\n            })\n            self.get_logger().info(f'Generated {len(action_plan)} action steps')\n            \n            # Step 5: Execution\n            self.get_logger().info('Step 5: Execution')\n            execution_success = self.vla_engine.execute_action_plan([\n                {'action': 'navigate_to_object', 'parameters': {'target': [1, 1, 0]}}  # Example\n            ])\n            \n            # Publish final status\n            final_status = String()\n            final_status.data = f'Completed: {command}' if execution_success else f'Failed: {command}'\n            self.status_pub.publish(final_status)\n            \n            self.get_logger().info(f'Command processing completed: {execution_success}')\n            \n        except Exception as e:\n            self.get_logger().error(f'Error in VLA processing: {str(e)}')\n            error_status = String()\n            error_status.data = f'Error processing: {command}'\n            self.status_pub.publish(error_status)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_system = VLASystem()\n    \n    try:\n        rclpy.spin(vla_system)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"vla-model-architectures",children:"VLA Model Architectures"}),"\n",(0,o.jsx)(n.h3,{id:"modern-vla-model-overview",children:"Modern VLA Model Overview"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# vla_models.py - Conceptual overview of VLA model architectures\n\nclass VLAConceptualModel:\n    """\n    This represents the conceptual architecture of modern VLA models.\n    In practice, these would be implemented using deep learning frameworks.\n    """\n    \n    def __init__(self):\n        # Vision encoder (e.g., ViT, ConvNeXt)\n        self.vision_encoder = "VisionTransformer"\n        \n        # Language encoder (e.g., GPT, OPT)\n        self.language_encoder = "GPT-3.5-turbo"\n        \n        # Action decoder (e.g., transformer-based policy)\n        self.action_decoder = "Transformers with action heads"\n        \n        # Fusion mechanism (e.g., cross-attention)\n        self.fusion_mechanism = "Cross-modal attention"\n        \n    def forward_pass(self, image, text_command):\n        """\n        Conceptual forward pass of VLA model\n        In practice, each component would be implemented with deep learning\n        """\n        # 1. Encode visual input\n        visual_features = self.encode_vision(image)\n        \n        # 2. Encode language command\n        language_features = self.encode_language(text_command)\n        \n        # 3. Fuse modalities\n        fused_features = self.fuse_modalities(visual_features, language_features)\n        \n        # 4. Generate actions\n        actions = self.generate_actions(fused_features)\n        \n        return actions\n    \n    def encode_vision(self, image):\n        """Encode visual information"""\n        # This would use a CNN or Vision Transformer\n        return "visual_features_placeholder"\n    \n    def encode_language(self, text):\n        """Encode language command"""\n        # This would use a transformer language model\n        return "language_features_placeholder"\n    \n    def fuse_modalities(self, visual_features, language_features):\n        """Fuse vision and language features"""\n        # This would use cross-attention mechanisms\n        return "fused_features_placeholder"\n    \n    def generate_actions(self, fused_features):\n        """Generate robot actions from fused features"""\n        # This would generate a sequence of actions\n        return ["action1", "action2", "action3"]\n\n# Example: OpenVLA model structure (conceptual)\nclass OpenVLAModel(VLAConceptualModel):\n    """\n    OpenVLA - An open-source VLA model architecture\n    Based on the real OpenVLA project which combines vision, language, and action\n    """\n    def __init__(self):\n        super().__init__()\n        self.name = "OpenVLA"\n        self.architecture = {\n            "vision_backbone": "ViT-L/14",\n            "language_model": "LLaMA-2/7B",\n            "action_head": "Multi-layer perceptron",\n            "training_method": "Behavior cloning + Reinforcement learning"\n        }\n    \n    def train(self, demonstrations):\n        """\n        Train on robot demonstration data\n        This is a conceptual representation\n        """\n        print("Training OpenVLA model on demonstration data...")\n        # In reality: training with behavioral cloning loss\n        # and potentially reinforcement learning components\n        pass\n\n# Example: RT-2 (Robotics Transformer 2) model structure (conceptual)\nclass RT2Model(VLAConceptualModel):\n    """\n    RT-2 - Robotics Transformer 2 architecture\n    Combines vision-language models with robotic action generation\n    """\n    def __init__(self):\n        super().__init__()\n        self.name = "RT-2"\n        self.architecture = {\n            "vision_backbone": "CLIP-ViT-L/14",\n            "language_backbone": "T5-XXL",\n            "fusion": "Sequential fusion with language prompting",\n            "action_generation": "Token-based action prediction"\n        }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-and-evaluation-metrics",children:"Performance and Evaluation Metrics"}),"\n",(0,o.jsx)(n.h3,{id:"vla-system-evaluation",children:"VLA System Evaluation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# vla_eval_metrics.py\nimport numpy as np\nfrom typing import Dict, List, Tuple\n\nclass VLAEvaluator:\n    def __init__(self):\n        self.metrics = {\n            \'command_accuracy\': [],\n            \'action_success_rate\': [],\n            \'execution_time\': [],\n            \'perception_accuracy\': [],\n            \'language_understanding\': []\n        }\n    \n    def evaluate_command_understanding(self, predicted_command: str, ground_truth: str) -> float:\n        """Evaluate how well the system understood the command"""\n        # Calculate semantic similarity\n        # In practice, this might use sentence transformers or similar\n        similarity_score = self._calculate_semantic_similarity(\n            predicted_command, \n            ground_truth\n        )\n        return similarity_score\n    \n    def evaluate_action_execution(self, predicted_actions: List[str], \n                                ground_truth_actions: List[str]) -> Dict:\n        """Evaluate how well the system executed the command"""\n        # Calculate action sequence similarity\n        sequence_similarity = self._calculate_sequence_similarity(\n            predicted_actions, \n            ground_truth_actions\n        )\n        \n        # Calculate success rate (did the robot do what was asked?)\n        success_rate = self._calculate_success_rate(\n            predicted_actions,\n            ground_truth_actions\n        )\n        \n        return {\n            \'sequence_similarity\': sequence_similarity,\n            \'success_rate\': success_rate,\n            \'action_accuracy\': self._calculate_action_accuracy(\n                predicted_actions,\n                ground_truth_actions\n            )\n        }\n    \n    def evaluate_perception(self, detected_objects: List[Dict], \n                          ground_truth_objects: List[Dict]) -> Dict:\n        """Evaluate perception accuracy"""\n        # Calculate object detection accuracy\n        detection_accuracy = self._calculate_detection_accuracy(\n            detected_objects, \n            ground_truth_objects\n        )\n        \n        # Calculate spatial accuracy\n        spatial_accuracy = self._calculate_spatial_accuracy(\n            detected_objects,\n            ground_truth_objects\n        )\n        \n        return {\n            \'detection_accuracy\': detection_accuracy,\n            \'spatial_accuracy\': spatial_accuracy\n        }\n    \n    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:\n        """Calculate semantic similarity between two texts"""\n        # In practice, this would use sentence transformers\n        # or other semantic similarity models\n        return 0.8  # Placeholder\n    \n    def _calculate_sequence_similarity(self, seq1: List[str], seq2: List[str]) -> float:\n        """Calculate similarity between two action sequences"""\n        if not seq1 and not seq2:\n            return 1.0\n        if not seq1 or not seq2:\n            return 0.0\n            \n        # Calculate longest common subsequence\n        lcs_length = self._longest_common_subsequence(seq1, seq2)\n        max_length = max(len(seq1), len(seq2))\n        return lcs_length / max_length\n    \n    def _longest_common_subsequence(self, seq1: List[str], seq2: List[str]) -> int:\n        """Calculate longest common subsequence length"""\n        m, n = len(seq1), len(seq2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        \n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if seq1[i-1] == seq2[j-1]:\n                    dp[i][j] = dp[i-1][j-1] + 1\n                else:\n                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n        \n        return dp[m][n]\n    \n    def _calculate_success_rate(self, pred_actions: List[str], \n                               gt_actions: List[str]) -> float:\n        """Calculate whether the overall task was successful"""\n        # This depends on the specific task and desired outcomes\n        # For now, using a simple heuristic\n        return 0.7  # Placeholder\n    \n    def _calculate_action_accuracy(self, pred_actions: List[str], \n                                  gt_actions: List[str]) -> float:\n        """Calculate accuracy of individual actions"""\n        if not gt_actions:\n            return 1.0 if not pred_actions else 0.0\n            \n        correct = sum(1 for pred, gt in zip(pred_actions, gt_actions) if pred == gt)\n        return correct / len(gt_actions)\n    \n    def _calculate_detection_accuracy(self, detected: List[Dict], \n                                    ground_truth: List[Dict]) -> float:\n        """Calculate object detection accuracy"""\n        # Calculate IoU, precision, recall, etc.\n        return 0.85  # Placeholder\n    \n    def _calculate_spatial_accuracy(self, detected: List[Dict], \n                                   ground_truth: List[Dict]) -> float:\n        """Calculate spatial position accuracy"""\n        # Calculate distance between detected and ground truth positions\n        return 0.9  # Placeholder\n    \n    def generate_evaluation_report(self) -> Dict:\n        """Generate comprehensive evaluation report"""\n        report = {}\n        \n        for metric_name, values in self.metrics.items():\n            if values:\n                report[metric_name] = {\n                    \'mean\': np.mean(values),\n                    \'std\': np.std(values),\n                    \'min\': np.min(values),\n                    \'max\': np.max(values),\n                    \'count\': len(values)\n                }\n        \n        return report\n\n# Example evaluation loop\ndef evaluate_vla_system():\n    evaluator = VLAEvaluator()\n    \n    # Example test cases\n    test_cases = [\n        {\n            \'command\': \'Pick up the red cup from the table\',\n            \'ground_truth_actions\': [\'navigate_to_table\', \'detect_red_cup\', \'grasp_cup\'],\n            \'expected_objects\': [{\'name\': \'red_cup\', \'position\': [1, 1, 0]}]\n        }\n    ]\n    \n    for i, test_case in enumerate(test_cases):\n        print(f"Evaluating test case {i+1}: {test_case[\'command\']}")\n        \n        # Simulate system response (in practice, this would run the actual system)\n        predicted_actions = [\'navigate_to_table\', \'detect_cup\', \'grasp_object\']  # Simulated\n        detected_objects = [{\'name\': \'cup\', \'position\': [1.1, 1.05, 0]}]  # Simulated\n        \n        # Evaluate different aspects\n        command_acc = evaluator.evaluate_command_understanding(\n            test_case[\'command\'], \n            test_case[\'command\']  # Same for testing\n        )\n        \n        action_eval = evaluator.evaluate_action_execution(\n            predicted_actions,\n            test_case[\'ground_truth_actions\']\n        )\n        \n        perception_eval = evaluator.evaluate_perception(\n            detected_objects,\n            test_case[\'expected_objects\']\n        )\n        \n        print(f"Command accuracy: {command_acc:.3f}")\n        print(f"Action evaluation: {action_eval}")\n        print(f"Perception evaluation: {perception_eval}")\n        print("---")\n    \n    # Generate final report\n    report = evaluator.generate_evaluation_report()\n    print("Final Evaluation Report:")\n    print(report)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-world-applications-and-challenges",children:"Real-World Applications and Challenges"}),"\n",(0,o.jsx)(n.h3,{id:"vla-in-human-robot-interaction",children:"VLA in Human-Robot Interaction"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# vla_human_interaction.py\nclass HumanRobotInteractionManager:\n    def __init__(self):\n        self.conversation_context = []\n        self.user_preferences = {}\n        self.system_confidence_threshold = 0.7\n        self.feedback_buffer = []\n        \n    def handle_user_command(self, user_command: str, image_context: np.ndarray):\n        """Handle a user command in the context of ongoing interaction"""\n        # Add to conversation context\n        self.conversation_context.append({\n            \'user_input\': user_command,\n            \'timestamp\': time.time(),\n            \'context_image\': image_context\n        })\n        \n        # Process with full context\n        result = self.process_command_with_context(user_command, image_context)\n        \n        # Consider previous context for disambiguation\n        disambiguated_command = self.resolve_ambiguities(\n            user_command, \n            result\n        )\n        \n        return disambiguated_command\n    \n    def resolve_ambiguities(self, user_command: str, current_result: Dict) -> Dict:\n        """Resolve ambiguities based on conversation history"""\n        # Example: "Pick that up" - what is "that"?\n        if "that" in user_command.lower():\n            # Look at recent conversation for context\n            for context in reversed(self.conversation_context[-3:]):  # Look at last 3 exchanges\n                if \'pointing_action\' in context or \'recent_object\' in context:\n                    # Resolve "that" to the previously mentioned object\n                    resolved_command = user_command.replace("that", context.get(\'recent_object\', \'object\'))\n                    return {\n                        \'original\': user_command,\n                        \'resolved\': resolved_command,\n                        \'object_reference\': context.get(\'recent_object\')\n                    }\n        \n        # Example: "Over there" - where is "over there"?\n        if "there" in user_command.lower() or "here" in user_command.lower():\n            # Use spatial context from recent actions\n            pass\n        \n        return {\'original\': user_command, \'resolved\': user_command}\n    \n    def provide_feedback_to_user(self, action_result: Dict):\n        """Provide feedback to user about action execution"""\n        feedback_msg = String()\n        \n        if action_result.get(\'success\', False):\n            feedback_msg.data = f"Successfully completed: {action_result.get(\'description\', \'task\')}"\n        else:\n            feedback_msg.data = f"Could not complete: {action_result.get(\'description\', \'task\')}. {action_result.get(\'error\', \'\')}"\n        \n        # Publish feedback\n        # self.feedback_pub.publish(feedback_msg)\n        \n        # Store for learning\n        self.feedback_buffer.append(action_result)\n    \n    def adapt_to_user_preferences(self, user_feedback: Dict):\n        """Adapt system behavior based on user feedback"""\n        # Learn from positive/negative feedback\n        if user_feedback.get(\'positive\', False):\n            # Reinforce current approach\n            self._reinforce_behavior(user_feedback)\n        else:\n            # Adjust approach based on feedback\n            self._adjust_behavior(user_feedback)\n    \n    def _reinforce_behavior(self, feedback: Dict):\n        """Reinforce successful behaviors"""\n        # Update internal models based on positive feedback\n        pass\n    \n    def _adjust_behavior(self, feedback: Dict):\n        """Adjust behavior based on negative feedback"""\n        # Update internal models based on negative feedback\n        # Possibly request clarification from user\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,o.jsx)(n.p,{children:"The Vision-Language-Action (VLA) paradigm represents a fundamental advancement in Physical AI, enabling robots to understand natural language commands, perceive their environment in context, and execute complex physical actions. Modern VLA systems integrate advanced computer vision, natural language processing, and robotic control in unified architectures that can interpret human instructions and translate them into appropriate physical behaviors. The success of VLA systems depends on effective multimodal fusion, robust cognitive planning, and adaptive execution frameworks that can handle the uncertainties and variabilities of real-world environments. Understanding VLA principles is essential for developing robots that can truly collaborate with humans in natural and intuitive ways."}),"\n",(0,o.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"}),": Integrated AI paradigm combining visual perception, language understanding, and physical action"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multimodal Fusion"}),": Techniques for combining information from different sensory modalities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning"}),": High-level planning that translates natural language commands into executable actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Systems that interpret human language commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OpenVLA"}),": Open-source Vision-Language-Action model architecture"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"RT-2"}),": Robotics Transformer 2, a VLA model architecture"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Behavioral Cloning"}),": Learning robot behaviors from human demonstrations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Semantic Similarity"}),": Measuring similarity in meaning between different expressions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cross-Modal Attention"}),": Attention mechanisms that connect different input modalities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human-Robot Interaction (HRI)"}),": Study of how humans and robots communicate and collaborate"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"practice-questions",children:"Practice Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"VLA System Design"}),": Design a complete VLA system for a household robot. Include all components: voice processing, language understanding, vision processing, cognitive planning, and action execution. Describe how they would interact."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command Interpretation"}),': For the command "Please put the red cup on the table near the window," identify the vision, language, and action components needed. What specific technologies would you use for each?']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Ambiguity Resolution"}),': How would your VLA system handle ambiguous commands like "Pick that up" or "Go over there"? Provide an implementation approach.']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Evaluation Metrics"}),": Design an evaluation framework for a VLA system. What metrics would you use, and how would you collect ground truth data?"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Real-time Performance"}),": How would you optimize your VLA system to operate in real-time with a humanoid robot? What trade-offs would you consider?"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"How does the VLA paradigm change the way we think about human-robot collaboration compared to traditional programming approaches?"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"What are the key challenges in scaling VLA systems to handle diverse, real-world environments?"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"How might advances in large language models impact the development of future VLA systems?"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsxs)(n.em,{children:["Continue to ",(0,o.jsx)(n.a,{href:"/docs/chapter-6/autonomous-humanoid-capstone",children:"Chapter 6: The Autonomous Humanoid Capstone Project"})]})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},7561:(e,n,t)=>{t.d(n,{A:()=>s});var a=t(6540),o=t(4746),i=t(4848);const s=({chapterId:e,userId:n="1"})=>{const[t,s]=(0,a.useState)(!1),[r,c]=(0,a.useState)(!1),[l,d]=(0,a.useState)(""),[p,m]=(0,a.useState)(0),[u,_]=(0,a.useState)(""),[g,f]=(0,a.useState)(null),[h,b]=(0,a.useState)("intermediate"),v=(0,a.useRef)(null),x=(0,a.useRef)(null);(0,a.useEffect)(()=>{if("undefined"==typeof window)return;const e=()=>{const e=[".theme-doc-markdown","article",".markdown","main .container",'[class*="docItemContainer"]'];for(const n of e){const e=document.querySelector(n);if(e&&e.textContent&&e.textContent.length>50)return void(v.current=e)}};e();const n=new MutationObserver(()=>{v.current||e()}),t=document.querySelector("body");return t&&n.observe(t,{childList:!0,subtree:!0}),()=>{n.disconnect(),x.current&&x.current.abort()}},[e]);const y=()=>{const e=v.current;e&&l&&(e.style.transition="opacity 0.3s ease-in-out",e.style.opacity="0",setTimeout(()=>{e.innerHTML=l,e.style.opacity="1",c(!1),f(null)},300))},j=()=>{switch(h){case"beginner":return"#10b981";case"advanced":return"#8b5cf6";default:return"#3b82f6"}},k=()=>{switch(h){case"beginner":return"\ud83c\udf31";case"advanced":return"\ud83d\ude80";default:return"\ud83d\udcd6"}};return(0,i.jsxs)("div",{className:"custom-button-container",style:{margin:"1.5rem auto",padding:"1.5rem",border:"2px solid #e5e7eb",borderRadius:"16px",backgroundColor:"linear-gradient(-45deg,  #ff0055,  #7000ff,  #009c98, #000000)",boxShadow:"0 4px 6px -1px rgba(0, 0, 0, 0.1)",textAlign:"center",maxWidth:"700px",fontFamily:"system-ui, sans-serif"},children:[(0,i.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"center",gap:"1rem"},children:[(0,i.jsxs)("div",{style:{display:"flex",gap:"1rem",alignItems:"center",flexWrap:"wrap",justifyContent:"center"},children:[(0,i.jsx)("label",{style:{fontSize:"0.875rem",fontWeight:"600",color:"#374151"},children:"Target Level:"}),(0,i.jsxs)("select",{value:h,onChange:e=>{b(e.target.value),r&&y()},disabled:t,style:{padding:"0.6rem 1rem",borderRadius:"8px",border:"2px solid "+(t?"#e5e7eb":"#d1d5db"),outline:"none",cursor:t?"not-allowed":"pointer"},children:[(0,i.jsx)("option",{value:"beginner",children:"\ud83c\udf31 Beginner (Simple)"}),(0,i.jsx)("option",{value:"intermediate",children:"\ud83d\udcd6 Intermediate (Standard)"}),(0,i.jsx)("option",{value:"advanced",children:"\ud83d\ude80 Advanced (Technical)"})]})]}),(0,i.jsx)("button",{onClick:async()=>{if(r)return void y();const t=v.current;if(t){s(!0),f(null),m(0),_(`Preparing to personalize for ${h}...`),l||d(t.innerHTML);try{const i=(e=>{const n=3e3,t=[];if(e.children.length>0){let a="";Array.from(e.children).forEach(e=>{const o=e.outerHTML;a.length+o.length>n&&a.length>0&&(t.push(a),a=""),a+=o}),a&&t.push(a)}else{const a=e.innerHTML;for(let e=0;e<a.length;e+=n)t.push(a.substring(e,e+n))}return t})(t);if(console.log(`Personalizing in ${i.length} chunks`),0===i.length)throw new Error("Page content is empty");let r="";x.current&&x.current.abort(),x.current=new AbortController;for(let t=0;t<i.length;t++){const s=i[t],c=Math.round(t/i.length*100);m(c),_(`Adapting section ${t+1} of ${i.length}...`);try{const t=await o.u.post("/api/v1/personalize",{content:s,user_id:n?parseInt(n):1,learning_level:h,chapter_context:e,content_type:"partial_chapter"});r+=t.personalized_content,await new Promise(e=>setTimeout(e,150))}catch(a){console.error(`Error in chunk ${t}:`,a),r+=s}}m(100),_("Finalizing changes..."),setTimeout(()=>{t&&(t.style.transition="opacity 0.3s ease-in-out",t.style.opacity="0",setTimeout(()=>{const e=`\n              <div class="alert alert--success margin-bottom--md" style="padding: 1rem; border-radius: 8px; background-color: ${j()}20; border: 1px solid ${j()}; margin-bottom: 20px;">\n                <strong>${k()} Content Adapted: ${h.charAt(0).toUpperCase()+h.slice(1)} Level</strong>\n                <div style="font-size: 0.85em; margin-top: 5px;">Content has been simplified and examples adjusted for your level.</div>\n              </div>\n            `;t.innerHTML=e+r,t.style.opacity="1"},300)),c(!0),s(!1),_("")},500)}catch(g){console.error("Personalization Error:",g);const n=g.response?.data?.detail||g.message||"Process failed";f(`Failed: ${n}`),s(!1),m(0)}}else f("Content to personalize not found.")},disabled:t,style:{padding:"1rem 2rem",backgroundColor:r?"#059669":j(),color:"white",border:"none",borderRadius:"12px",cursor:t?"wait":"pointer",fontWeight:"700",fontSize:"1.05rem",display:"flex",alignItems:"center",gap:"0.75rem",transition:"all 0.2s",opacity:t?.8:1,boxShadow:"0 2px 4px rgba(0,0,0,0.1)"},children:t?(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("span",{style:{width:"18px",height:"18px",border:"3px solid rgba(255,255,255,0.3)",borderTop:"3px solid white",borderRadius:"50%",animation:"spin 1s linear infinite",display:"inline-block"}}),(0,i.jsx)("span",{children:"Processing..."})]}):r?(0,i.jsx)(i.Fragment,{children:"\u21a9\ufe0f Restore Original"}):(0,i.jsx)(i.Fragment,{children:"\u2728 Personalize Content"})}),t&&(0,i.jsxs)("div",{style:{width:"100%",marginTop:"0.5rem"},children:[(0,i.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",fontSize:"0.9rem",color:"#666",marginBottom:"4px"},children:[(0,i.jsx)("span",{children:u}),(0,i.jsxs)("span",{children:[p,"%"]})]}),(0,i.jsx)("div",{style:{width:"100%",height:"6px",backgroundColor:"#e5e7eb",borderRadius:"3px",overflow:"hidden"},children:(0,i.jsx)("div",{style:{width:`${p}%`,height:"100%",backgroundColor:j(),transition:"width 0.3s ease"}})})]}),!t&&(0,i.jsx)("p",{style:{margin:0,fontSize:"0.9rem",color:g?"#dc2626":"#6b7280",marginTop:"0.5rem"},children:g?`\u26a0\ufe0f ${g}`:r?"\u2705 Content updated based on your preferences.":"AI will rewrite the content to match your selected difficulty level."})]}),(0,i.jsx)("style",{children:"\n        @keyframes spin { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }\n      "})]})}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const o={},i=a.createContext(o);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:n},e.children)}},9815:(e,n,t)=>{t(6540),t(9345),t(4848)}}]);