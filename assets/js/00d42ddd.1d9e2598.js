"use strict";(globalThis.webpackChunkbook_ui=globalThis.webpackChunkbook_ui||[]).push([[89],{3113:(e,n,i)=>{i.d(n,{A:()=>s});var t=i(6540),r=i(4746),a=i(4848);const s=({chapterId:e,userId:n="demo-user"})=>{const[i,s]=(0,t.useState)(!1),[o,l]=(0,t.useState)(!1),[c,d]=(0,t.useState)("ur"),[m,h]=(0,t.useState)(""),[u,p]=(0,t.useState)(0),[g,f]=(0,t.useState)(""),[x,y]=(0,t.useState)(null),b=(0,t.useRef)(null),v=(0,t.useRef)(null);(0,t.useEffect)(()=>{if("undefined"==typeof window)return;const e=()=>{const e=[".theme-doc-markdown",".markdown","article","main .container",'[class*="docItemContainer"]'];for(const n of e){const e=document.querySelector(n);if(e&&e.textContent&&e.textContent.length>50)return void(b.current=e)}};e();const n=new MutationObserver(()=>{b.current||e()}),i=document.querySelector("body");return i&&n.observe(i,{childList:!0,subtree:!0}),()=>{n.disconnect(),v.current&&v.current.abort()}},[e]);const j=()=>{const e=b.current;e&&m&&(e.style.transition="opacity 0.3s ease-in-out",e.style.opacity="0",setTimeout(()=>{e.innerHTML=m,e.style.direction="ltr",e.style.textAlign="left",e.style.opacity="1",l(!1),y(null)},300))};return(0,a.jsx)("div",{style:{margin:"2rem auto",padding:"1.5rem",border:"2px solid #e5e7eb",borderRadius:"12px",backgroundColor:"linear-gradient(-45deg,  #ff0055,  #7000ff,  #009c98, #000000)",boxShadow:"0 4px 6px -1px rgba(0, 0, 0, 0.1)",textAlign:"center",maxWidth:"700px",fontFamily:"system-ui, sans-serif"},children:(0,a.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"center",gap:"1rem"},children:[(0,a.jsxs)("div",{style:{display:"flex",gap:"10px",alignItems:"center",flexWrap:"wrap",justifyContent:"center"},children:[(0,a.jsxs)("select",{value:c,onChange:e=>{d(e.target.value),o&&j()},disabled:i,style:{padding:"8px",borderRadius:"5px",border:"1px solid #ccc"},children:[(0,a.jsx)("option",{value:"ur",children:"\ud83c\uddf5\ud83c\uddf0 \u0627\u0631\u062f\u0648 (Urdu)"}),(0,a.jsx)("option",{value:"ar",children:"\ud83c\uddf8\ud83c\udde6 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 (Arabic)"}),(0,a.jsx)("option",{value:"es",children:"\ud83c\uddea\ud83c\uddf8 Espa\xf1ol (Spanish)"}),(0,a.jsx)("option",{value:"fr",children:"\ud83c\uddeb\ud83c\uddf7 Fran\xe7ais (French)"})]}),(0,a.jsx)("button",{onClick:async()=>{if(o)return void j();const e=b.current;if(e){s(!0),y(null),p(0),f("Preparing content..."),m||h(e.innerHTML);try{const i=(e=>{const n=3500,i=[];if(e.children.length>0){let t="";Array.from(e.children).forEach(e=>{const r=e.outerHTML;t.length+r.length>n&&t.length>0&&(i.push(t),t=""),t+=r}),t&&i.push(t)}else{const t=e.innerHTML;for(let e=0;e<t.length;e+=n)i.push(t.substring(e,e+n))}return i})(e);if(console.log(`Total chunks to translate: ${i.length}`),0===i.length)throw new Error("No content to translate");let t="";v.current&&v.current.abort(),v.current=new AbortController;for(let e=0;e<i.length;e++){const a=i[e],s=Math.round(e/i.length*100);p(s),f(`Translating part ${e+1} of ${i.length}...`);try{const e=await r.u.post("/api/v1/translate",{text:a,target_language:c,source_language:"en",preserve_formatting:!0});t+=e.translated_text,await new Promise(e=>setTimeout(e,200))}catch(n){console.error(`Error in chunk ${e}:`,n),t+=a}}p(100),f("Finalizing..."),setTimeout(()=>{e&&(e.style.opacity="0",setTimeout(()=>{const n="ur"===c||"ar"===c;e.innerHTML='\n              <div class="alert alert--success margin-bottom--md" style="direction: ltr; text-align: left; padding: 10px; border: 1px solid green; border-radius: 8px; margin-bottom: 20px;">\n                <strong>\u2705 Translated successfully</strong>\n              </div>\n            '+t,e.style.direction=n?"rtl":"ltr",e.style.textAlign=n?"right":"left",e.style.opacity="1"},300)),l(!0),s(!1),f("")},500)}catch(i){console.error("Translation Error:",i),y("Translation failed. Please try again later."),s(!1),p(0)}}else y("Content not found.")},disabled:i,style:{padding:"10px 20px",backgroundColor:o?"#10b981":"#4f46e5",color:"white",border:"none",borderRadius:"6px",cursor:i?"not-allowed":"pointer",fontWeight:"bold",opacity:i?.7:1},children:i?"Translating...":o?"Restore Original":"Translate Page"})]}),i&&(0,a.jsxs)("div",{style:{width:"100%",marginTop:"10px"},children:[(0,a.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",fontSize:"0.8rem",marginBottom:"5px"},children:[(0,a.jsx)("span",{children:g}),(0,a.jsxs)("span",{children:[u,"%"]})]}),(0,a.jsx)("div",{style:{width:"100%",height:"8px",backgroundColor:"#eee",borderRadius:"4px",overflow:"hidden"},children:(0,a.jsx)("div",{style:{width:`${u}%`,height:"100%",backgroundColor:"#4f46e5",transition:"width 0.3s ease"}})})]}),x&&(0,a.jsxs)("p",{style:{color:"red",marginTop:"10px",fontSize:"0.9rem"},children:["\u26a0\ufe0f ",x]})]})})}},6001:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>l,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"chapter-3/digital-twin-simulation","title":"Chapter 3: The Digital Twin (Gazebo & Unity)","description":"Overview","source":"@site/docs/chapter-3/digital-twin-simulation.mdx","sourceDirName":"chapter-3","slug":"/chapter-3/digital-twin-simulation","permalink":"/ai-spec-hackathone/docs/chapter-3/digital-twin-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/rabiasohail098/ai-spec-kit-book/tree/main/docs/chapter-3/digital-twin-simulation.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Chapter 3: The Digital Twin (Gazebo & Unity)","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: The Robotic Nervous System (ROS 2)","permalink":"/ai-spec-hackathone/docs/chapter-2/ros2-fundamentals"},"next":{"title":"Chapter 4: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/ai-spec-hackathone/docs/chapter-4/ai-robot-brain-isaac"}}');var r=i(4848),a=i(8453),s=(i(9815),i(7561)),o=i(3113);const l={title:"Chapter 3: The Digital Twin (Gazebo & Unity)",sidebar_position:3},c="Chapter 3: The Digital Twin (Gazebo & Unity)",d={},m=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"The Digital Twin Paradigm in Robotics",id:"the-digital-twin-paradigm-in-robotics",level:2},{value:"Understanding the Digital Twin Concept",id:"understanding-the-digital-twin-concept",level:3},{value:"The Physical AI Context",id:"the-physical-ai-context",level:3},{value:"Gazebo: Physics-Based Simulation",id:"gazebo-physics-based-simulation",level:2},{value:"Introduction to Gazebo",id:"introduction-to-gazebo",level:3},{value:"Key Features of Gazebo",id:"key-features-of-gazebo",level:3},{value:"1. Physics Simulation",id:"1-physics-simulation",level:4},{value:"2. Sensor Simulation",id:"2-sensor-simulation",level:4},{value:"3. Environment Modeling",id:"3-environment-modeling",level:4},{value:"Setting Up Gazebo Simulation",id:"setting-up-gazebo-simulation",level:3},{value:"Basic Gazebo World",id:"basic-gazebo-world",level:4},{value:"ROS 2 Integration with Gazebo",id:"ros-2-integration-with-gazebo",level:4},{value:"Advanced Gazebo Features for Physical AI",id:"advanced-gazebo-features-for-physical-ai",level:3},{value:"1. Physics Parameter Tuning",id:"1-physics-parameter-tuning",level:4},{value:"2. Sensor Noise Models",id:"2-sensor-noise-models",level:4},{value:"Unity: High-Fidelity Visualization and Human-Robot Interaction",id:"unity-high-fidelity-visualization-and-human-robot-interaction",level:2},{value:"Introduction to Unity for Robotics",id:"introduction-to-unity-for-robotics",level:3},{value:"Unity Robotics Hub",id:"unity-robotics-hub",level:3},{value:"Setting Up Unity for Robotics",id:"setting-up-unity-for-robotics",level:3},{value:"Basic Robot in Unity",id:"basic-robot-in-unity",level:4},{value:"Unity Perception for Synthetic Data",id:"unity-perception-for-synthetic-data",level:3},{value:"Sensor Simulation in Both Platforms",id:"sensor-simulation-in-both-platforms",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:3},{value:"In Gazebo:",id:"in-gazebo",level:4},{value:"In Unity:",id:"in-unity",level:4},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:3},{value:"In Gazebo:",id:"in-gazebo-1",level:4},{value:"In Unity:",id:"in-unity-1",level:4},{value:"Sim-to-Real Transfer Challenges",id:"sim-to-real-transfer-challenges",level:2},{value:"The Reality Gap Problem",id:"the-reality-gap-problem",level:3},{value:"1. Visual Domain Gap",id:"1-visual-domain-gap",level:4},{value:"2. Physical Domain Gap",id:"2-physical-domain-gap",level:4},{value:"3. Sensor Domain Gap",id:"3-sensor-domain-gap",level:4},{value:"Solutions for Sim-to-Real Transfer",id:"solutions-for-sim-to-real-transfer",level:3},{value:"1. Domain Randomization",id:"1-domain-randomization",level:4},{value:"2. System Identification",id:"2-system-identification",level:4},{value:"3. Progressive Domain Transfer",id:"3-progressive-domain-transfer",level:4},{value:"NVIDIA Isaac Sim: The Next Generation",id:"nvidia-isaac-sim-the-next-generation",level:2},{value:"Key Features of Isaac Sim",id:"key-features-of-isaac-sim",level:3},{value:"1. Photorealistic Rendering",id:"1-photorealistic-rendering",level:4},{value:"2. Synthetic Data Generation",id:"2-synthetic-data-generation",level:4},{value:"3. AI Integration",id:"3-ai-integration",level:4},{value:"Isaac Sim Architecture",id:"isaac-sim-architecture",level:3},{value:"Practical Implementation: Creating a Digital Twin",id:"practical-implementation-creating-a-digital-twin",level:2},{value:"Complete Gazebo Simulation Example",id:"complete-gazebo-simulation-example",level:3},{value:"ROS 2 Integration with Gazebo",id:"ros-2-integration-with-gazebo-1",level:3},{value:"Visualization and Debugging Tools",id:"visualization-and-debugging-tools",level:2},{value:"Gazebo Tools",id:"gazebo-tools",level:3},{value:"Unity Tools",id:"unity-tools",level:3},{value:"Performance Optimization Strategies",id:"performance-optimization-strategies",level:2},{value:"1. Simulation Efficiency",id:"1-simulation-efficiency",level:3},{value:"2. Resource Management",id:"2-resource-management",level:3},{value:"Digital Twin Validation",id:"digital-twin-validation",level:2},{value:"Metrics for Simulation Quality",id:"metrics-for-simulation-quality",level:3},{value:"Validation Methodology",id:"validation-methodology",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Practice Questions",id:"practice-questions",level:2},{value:"Reflection Questions",id:"reflection-questions",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-3-the-digital-twin-gazebo--unity",children:"Chapter 3: The Digital Twin (Gazebo & Unity)"})}),"\n",(0,r.jsx)(s.A,{chapterId:"chapter-3"}),"\n",(0,r.jsx)(o.A,{chapterId:"chapter-3"}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"The Digital Twin concept in robotics represents a virtual replica of a physical robot system that allows for testing, training, and validation in a safe, simulated environment before deployment in the real world. This chapter explores the two primary simulation platforms used in Physical AI: Gazebo for physics-based simulation and Unity for high-fidelity visualization and human-robot interaction. We'll examine how these platforms enable the development of robust robot systems that can operate effectively in real-world environments."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the concept and importance of digital twins in robotics"}),"\n",(0,r.jsx)(n.li,{children:"Compare Gazebo and Unity for different simulation needs"}),"\n",(0,r.jsx)(n.li,{children:"Configure physics simulations with accurate gravity, collisions, and material properties"}),"\n",(0,r.jsx)(n.li,{children:"Simulate various sensors including LiDAR, depth cameras, and IMUs"}),"\n",(0,r.jsx)(n.li,{children:"Implement high-fidelity rendering for human-robot interaction"}),"\n",(0,r.jsx)(n.li,{children:"Create synthetic data for AI model training"}),"\n",(0,r.jsx)(n.li,{children:"Understand the sim-to-real transfer challenges and solutions"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"the-digital-twin-paradigm-in-robotics",children:"The Digital Twin Paradigm in Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"understanding-the-digital-twin-concept",children:"Understanding the Digital Twin Concept"}),"\n",(0,r.jsx)(n.p,{children:"A digital twin in robotics is a virtual replica of a physical robot system that mirrors its real-world counterpart in real-time. This concept enables:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safe Testing"}),": Experiment with robot behaviors without physical risk"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rapid Prototyping"}),": Test multiple configurations quickly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Training Ground"}),": Train AI models without needing physical hardware"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validation"}),": Verify algorithms before real-world deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost Reduction"}),": Minimize the need for expensive physical testing"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"[Physical Robot] <------------------\x3e [Digital Twin]\n       \u2502                                     \u2502\n       \u2502 Physical Sensing              Virtual Sensing\n       \u2502 Actuator Commands             Virtual Actuation\n       \u2502 Environmental Interaction     Simulated Environment\n       \u2502 \u2193                           \u2193\n       \u2502 Feedback                   Training Data\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"the-physical-ai-context",children:"The Physical AI Context"}),"\n",(0,r.jsx)(n.p,{children:"In Physical AI, digital twins serve as crucial training grounds where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AI models learn from simulated interactions"}),"\n",(0,r.jsx)(n.li,{children:"Control algorithms are refined before deployment"}),"\n",(0,r.jsx)(n.li,{children:"Human-robot interaction scenarios are tested safely"}),"\n",(0,r.jsx)(n.li,{children:"Edge cases can be explored without physical consequences"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"gazebo-physics-based-simulation",children:"Gazebo: Physics-Based Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"introduction-to-gazebo",children:"Introduction to Gazebo"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo is a 3D simulation environment that provides:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accurate physics simulation with gravity, friction, and collisions"}),"\n",(0,r.jsx)(n.li,{children:"High-quality rendering for visualization"}),"\n",(0,r.jsx)(n.li,{children:"Multiple physics engines (ODE, Bullet, Simbody)"}),"\n",(0,r.jsx)(n.li,{children:"Sensor simulation for cameras, LiDAR, IMUs, etc."}),"\n",(0,r.jsx)(n.li,{children:"Integration with ROS/ROS 2"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-features-of-gazebo",children:"Key Features of Gazebo"}),"\n",(0,r.jsx)(n.h4,{id:"1-physics-simulation",children:"1. Physics Simulation"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo provides realistic physics simulation crucial for Physical AI:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gravity Simulation"}),": Accurate gravitational forces for realistic robot behavior"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Collision Detection"}),": Sophisticated algorithms to handle complex interactions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Material Properties"}),": Realistic friction, bounciness, and surface properties"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Joint Dynamics"}),": Accurate simulation of robotic joints and constraints"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example SDF for physics properties --\x3e\n<model name="robot_with_physics">\n  <link name="base_link">\n    <collision name="collision">\n      <geometry>\n        <box size="1 1 1"/>\n      </geometry>\n    </collision>\n    <visual name="visual">\n      <geometry>\n        <box size="1 1 1"/>\n      </geometry>\n    </visual>\n    <inertial>\n      <mass>1.0</mass>\n      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" izz="1.0"/>\n    </inertial>\n  </link>\n</model>\n'})}),"\n",(0,r.jsx)(n.h4,{id:"2-sensor-simulation",children:"2. Sensor Simulation"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo provides realistic sensor simulation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera Simulation"}),": RGB, depth, and stereo cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR Simulation"}),": 2D and 3D LiDAR with configurable resolution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU Simulation"}),": Inertial measurement units with realistic noise models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force/Torque Sensors"}),": For manipulation and contact sensing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPS Simulation"}),": For outdoor navigation applications"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-environment-modeling",children:"3. Environment Modeling"}),"\n",(0,r.jsx)(n.p,{children:"Gazebo allows for complex environment creation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Terrain Generation"}),": Realistic outdoor environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Building Models"}),": Indoor environments with furniture and obstacles"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic Objects"}),": Moving objects and interactive elements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Weather Simulation"}),": Lighting, fog, and atmospheric effects"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"setting-up-gazebo-simulation",children:"Setting Up Gazebo Simulation"}),"\n",(0,r.jsx)(n.h4,{id:"basic-gazebo-world",children:"Basic Gazebo World"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- simple_world.world --\x3e\n<?xml version="1.0" ?>\n<sdf version="1.7">\n  <world name="simple_world">\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    <include>\n      <uri>model://sun</uri>\n    </include>\n    \n    \x3c!-- Simple robot model --\x3e\n    <model name="simple_robot">\n      <pose>0 0 0.5 0 0 0</pose>\n      <link name="chassis">\n        <pose>0 0 0.1 0 0 0</pose>\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>0.5 0.25 0.1</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>0.5 0.25 0.1</size>\n            </box>\n          </geometry>\n        </visual>\n        <inertial>\n          <mass>1.0</mass>\n          <inertia ixx="1" ixy="0" ixz="0" iyy="1" iyz="0" izz="1"/>\n        </inertial>\n      </link>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,r.jsx)(n.h4,{id:"ros-2-integration-with-gazebo",children:"ROS 2 Integration with Gazebo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# gazebo_robot_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan, Image\nimport numpy as np\n\nclass GazeboRobotBridge(Node):\n    def __init__(self):\n        super().__init__('gazebo_robot_bridge')\n        \n        # Create publisher for velocity commands\n        self.cmd_vel_pub = self.create_publisher(\n            Twist,\n            '/cmd_vel',\n            10\n        )\n        \n        # Create subscribers for simulated sensors\n        self.laser_sub = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.laser_callback,\n            10\n        )\n        \n        self.camera_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.camera_callback,\n            10\n        )\n        \n        # Timer for control loop\n        self.timer = self.create_timer(0.1, self.control_loop)\n        \n        self.get_logger().info('Gazebo Robot Bridge initialized')\n\n    def laser_callback(self, msg):\n        # Process simulated laser data\n        ranges = np.array(msg.ranges)\n        # Implement obstacle detection logic\n        min_range = np.min(ranges[np.isfinite(ranges)])\n        self.get_logger().info(f'Min obstacle distance: {min_range:.2f}m')\n\n    def camera_callback(self, msg):\n        # Process simulated camera data\n        # Convert ROS Image message to OpenCV format for processing\n        pass\n\n    def control_loop(self):\n        # Implement robot control logic based on sensor data\n        cmd = Twist()\n        cmd.linear.x = 0.5  # Move forward\n        cmd.angular.z = 0.0  # No rotation\n        self.cmd_vel_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bridge = GazeboRobotBridge()\n    rclpy.spin(bridge)\n    bridge.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"advanced-gazebo-features-for-physical-ai",children:"Advanced Gazebo Features for Physical AI"}),"\n",(0,r.jsx)(n.h4,{id:"1-physics-parameter-tuning",children:"1. Physics Parameter Tuning"}),"\n",(0,r.jsx)(n.p,{children:"Fine-tuning physics parameters for realistic simulation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Physics parameters in SDF --\x3e\n<physics type="ode">\n  <max_step_size>0.001</max_step_size>\n  <real_time_factor>1.0</real_time_factor>\n  <real_time_update_rate>1000</real_time_update_rate>\n  <gravity>0 0 -9.8</gravity>\n  <ode>\n    <solver>\n      <type>quick</type>\n      <iters>50</iters>\n      <sor>1.3</sor>\n    </solver>\n    <constraints>\n      <cfm>0.0</cfm>\n      <erp>0.2</erp>\n      <contact_max_correcting_vel>100.0</contact_max_correcting_vel>\n      <contact_surface_layer>0.001</contact_surface_layer>\n    </constraints>\n  </ode>\n</physics>\n'})}),"\n",(0,r.jsx)(n.h4,{id:"2-sensor-noise-models",children:"2. Sensor Noise Models"}),"\n",(0,r.jsx)(n.p,{children:"Adding realistic noise to simulated sensors:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- LiDAR with noise model --\x3e\n<sensor name="lidar" type="ray">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>640</samples>\n        <resolution>1</resolution>\n        <min_angle>-1.570796</min_angle>\n        <max_angle>1.570796</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>10.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="lidar_noise" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <namespace>/lidar</namespace>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n    \x3c!-- Add noise parameters --\x3e\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>\n    </noise>\n  </plugin>\n</sensor>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"unity-high-fidelity-visualization-and-human-robot-interaction",children:"Unity: High-Fidelity Visualization and Human-Robot Interaction"}),"\n",(0,r.jsx)(n.h3,{id:"introduction-to-unity-for-robotics",children:"Introduction to Unity for Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Unity offers:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Photorealistic Rendering"}),": High-quality visuals for realistic perception"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Graphics"}),": Interactive environments for human-robot interaction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Asset Store"}),": Extensive library of 3D models and environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cross-platform Support"}),": Deployment across multiple platforms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scripting"}),": C# scripting for custom behaviors"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"unity-robotics-hub",children:"Unity Robotics Hub"}),"\n",(0,r.jsx)(n.p,{children:"Unity provides specialized tools for robotics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity Robotics Hub"}),": Centralized access to robotics tools"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS-TCP-Connector"}),": Communication bridge between Unity and ROS/ROS 2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity Perception Package"}),": Tools for generating synthetic training data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ML-Agents Toolkit"}),": Reinforcement learning for robotics applications"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"setting-up-unity-for-robotics",children:"Setting Up Unity for Robotics"}),"\n",(0,r.jsx)(n.h4,{id:"basic-robot-in-unity",children:"Basic Robot in Unity"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:"// RobotController.cs\nusing UnityEngine;\nusing System.Collections;\n\npublic class RobotController : MonoBehaviour\n{\n    public float moveSpeed = 5.0f;\n    public float turnSpeed = 100.0f;\n    \n    // ROS communication component (hypothetical)\n    private ROSBridge rosBridge;\n    \n    void Start()\n    {\n        // Initialize ROS connection\n        rosBridge = FindObjectOfType<ROSBridge>();\n    }\n    \n    void Update()\n    {\n        // Process commands from ROS\n        ProcessROSMessages();\n        \n        // Update robot position based on physics\n        UpdateRobotPosition();\n    }\n    \n    void ProcessROSMessages()\n    {\n        // Handle velocity commands from ROS\n        // Subscribe to /cmd_vel topic\n        // Update internal velocity values\n    }\n    \n    void UpdateRobotPosition()\n    {\n        // Apply movement based on velocity\n        // Handle physics interactions\n    }\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"unity-perception-for-synthetic-data",children:"Unity Perception for Synthetic Data"}),"\n",(0,r.jsx)(n.p,{children:"Unity's Perception package enables:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Create labeled training data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization"}),": Vary environments to improve robustness"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gaussian Splatting"}),": Advanced rendering techniques"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Annotation Tools"}),": Automatic labeling of objects in scenes"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:"// PerceptionCamera.cs\nusing UnityEngine;\nusing Unity.Perception.GroundTruth;\nusing Unity.Perception.Randomization;\n\npublic class PerceptionCamera : MonoBehaviour\n{\n    [SerializeField] private Camera m_Camera;\n    [SerializeField] private SemanticSegmentationLabeler m_Labeler;\n    \n    void Start()\n    {\n        // Configure camera for perception data\n        m_Camera = GetComponent<Camera>();\n        \n        // Set up semantic segmentation\n        m_Labeler = GetComponent<SemanticSegmentationLabeler>();\n        \n        // Configure randomization for domain transfer\n        ConfigureRandomization();\n    }\n    \n    void ConfigureRandomization()\n    {\n        // Add domain randomization parameters\n        // Lighting, textures, object positions, etc.\n    }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"sensor-simulation-in-both-platforms",children:"Sensor Simulation in Both Platforms"}),"\n",(0,r.jsx)(n.h3,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,r.jsx)(n.h4,{id:"in-gazebo",children:"In Gazebo:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<sensor name="3d_lidar" type="ray">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1080</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>64</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.261799</min_angle>\n        <max_angle>0.261799</max_angle>\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>120</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n</sensor>\n'})}),"\n",(0,r.jsx)(n.h4,{id:"in-unity",children:"In Unity:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:"// UnityLiDAR.cs\nusing UnityEngine;\nusing System.Collections.Generic;\n\npublic class UnityLiDAR : MonoBehaviour\n{\n    public int horizontalRays = 1080;\n    public int verticalRays = 64;\n    public float maxDistance = 120.0f;\n    public float minDistance = 0.1f;\n    \n    public List<float[]> Scan()\n    {\n        // Perform raycasting to simulate LiDAR\n        List<float[]> ranges = new List<float[]>();\n        \n        for (int v = 0; v < verticalRays; v++)\n        {\n            float[] horizontalScan = new float[horizontalRays];\n            float vAngle = Mathf.Lerp(-0.261799f, 0.261799f, (float)v / verticalRays);\n            \n            for (int h = 0; h < horizontalRays; h++)\n            {\n                float hAngle = Mathf.Lerp(-Mathf.PI, Mathf.PI, (float)h / horizontalRays);\n                \n                Vector3 direction = new Vector3(\n                    Mathf.Cos(vAngle) * Mathf.Cos(hAngle),\n                    Mathf.Cos(vAngle) * Mathf.Sin(hAngle),\n                    Mathf.Sin(vAngle)\n                );\n                \n                RaycastHit hit;\n                if (Physics.Raycast(transform.position, direction, out hit, maxDistance))\n                {\n                    horizontalScan[h] = hit.distance;\n                }\n                else\n                {\n                    horizontalScan[h] = maxDistance;\n                }\n            }\n            ranges.Add(horizontalScan);\n        }\n        \n        return ranges;\n    }\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,r.jsx)(n.h4,{id:"in-gazebo-1",children:"In Gazebo:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n</sensor>\n'})}),"\n",(0,r.jsx)(n.h4,{id:"in-unity-1",children:"In Unity:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:"// DepthCamera.cs\nusing UnityEngine;\n\npublic class DepthCamera : MonoBehaviour\n{\n    public RenderTexture depthTexture;\n    public Camera depthCam;\n    private float[] depthValues;\n    \n    void Start()\n    {\n        // Create depth texture\n        depthTexture = new RenderTexture(640, 480, 24);\n        depthTexture.format = RenderTextureFormat.Depth;\n        \n        // Get or create camera\n        depthCam = GetComponent<Camera>();\n        depthCam.targetTexture = depthTexture;\n    }\n    \n    public float[] GetDepthValues()\n    {\n        // Read depth values from texture\n        // Convert to range values for robotics application\n        return depthValues;\n    }\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"sim-to-real-transfer-challenges",children:"Sim-to-Real Transfer Challenges"}),"\n",(0,r.jsx)(n.h3,{id:"the-reality-gap-problem",children:"The Reality Gap Problem"}),"\n",(0,r.jsx)(n.p,{children:'The primary challenge in using digital twins is the "reality gap" - the difference between simulated and real environments:'}),"\n",(0,r.jsx)(n.h4,{id:"1-visual-domain-gap",children:"1. Visual Domain Gap"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Lighting conditions differ between simulation and reality"}),"\n",(0,r.jsx)(n.li,{children:"Material properties and textures vary"}),"\n",(0,r.jsx)(n.li,{children:"Sensor noise patterns may not match"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-physical-domain-gap",children:"2. Physical Domain Gap"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Friction and contact models differ"}),"\n",(0,r.jsx)(n.li,{children:"Motor dynamics and delays vary"}),"\n",(0,r.jsx)(n.li,{children:"Environmental disturbances"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-sensor-domain-gap",children:"3. Sensor Domain Gap"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Noise characteristics may not match"}),"\n",(0,r.jsx)(n.li,{children:"Resolution and field of view differences"}),"\n",(0,r.jsx)(n.li,{children:"Calibration parameters vary"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"solutions-for-sim-to-real-transfer",children:"Solutions for Sim-to-Real Transfer"}),"\n",(0,r.jsx)(n.h4,{id:"1-domain-randomization",children:"1. Domain Randomization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# domain_randomization_example.py\nimport random\nimport numpy as np\n\nclass DomainRandomizer:\n    def __init__(self):\n        self.lighting_range = (0.5, 2.0)\n        self.friction_range = (0.1, 1.0)\n        self.color_range = (0.0, 1.0)\n        \n    def randomize_environment(self):\n        randomized_params = {\n            # Randomize lighting\n            'light_intensity': random.uniform(*self.lighting_range),\n            # Randomize friction\n            'surface_friction': random.uniform(*self.friction_range),\n            # Randomize colors\n            'object_colors': [\n                random.uniform(*self.color_range),\n                random.uniform(*self.color_range), \n                random.uniform(*self.color_range)\n            ]\n        }\n        return randomized_params\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-system-identification",children:"2. System Identification"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# system_identification.py\nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass SystemIdentifier:\n    def __init__(self, robot_model):\n        self.model = robot_model\n        self.sim_params = {}\n        self.real_params = {}\n        \n    def identify_parameters(self, sim_data, real_data):\n        """Identify parameters that minimize sim-to-real gap"""\n        def objective(params):\n            # Adjust model parameters\n            self.model.update_params(params)\n            # Simulate with new parameters\n            sim_output = self.model.simulate()\n            # Calculate difference with real data\n            error = np.mean((sim_output - real_data) ** 2)\n            return error\n            \n        # Optimize to minimize error\n        result = minimize(objective, x0=self.model.get_params())\n        return result.x\n'})}),"\n",(0,r.jsx)(n.h4,{id:"3-progressive-domain-transfer",children:"3. Progressive Domain Transfer"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# progressive_transfer.py\nclass ProgressiveTransfer:\n    def __init__(self):\n        self.transfer_stages = [\n            "simple_sim",\n            "randomized_sim", \n            "sim_with_noise",\n            "real_world"\n        ]\n        self.current_stage = 0\n        \n    def advance_stage(self):\n        if self.current_stage < len(self.transfer_stages) - 1:\n            self.current_stage += 1\n            self.update_environment()\n            \n    def update_environment(self):\n        """Update simulation environment based on current stage"""\n        stage = self.transfer_stages[self.current_stage]\n        if stage == "simple_sim":\n            self.apply_simple_physics()\n        elif stage == "randomized_sim":\n            self.apply_domain_randomization()\n        elif stage == "sim_with_noise":\n            self.add_realistic_noise()\n        # Real world stage - no simulation changes needed\n'})}),"\n",(0,r.jsx)(n.h2,{id:"nvidia-isaac-sim-the-next-generation",children:"NVIDIA Isaac Sim: The Next Generation"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Sim represents the cutting edge of robotics simulation, built on NVIDIA's Omniverse platform:"}),"\n",(0,r.jsx)(n.h3,{id:"key-features-of-isaac-sim",children:"Key Features of Isaac Sim"}),"\n",(0,r.jsx)(n.h4,{id:"1-photorealistic-rendering",children:"1. Photorealistic Rendering"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"RTX ray tracing for realistic lighting"}),"\n",(0,r.jsx)(n.li,{children:"Physically-based rendering (PBR) materials"}),"\n",(0,r.jsx)(n.li,{children:"Advanced shading and reflections"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-synthetic-data-generation",children:"2. Synthetic Data Generation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Massive scale data generation"}),"\n",(0,r.jsx)(n.li,{children:"Automatic annotation"}),"\n",(0,r.jsx)(n.li,{children:"Domain randomization tools"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-ai-integration",children:"3. AI Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Native support for Isaac ROS packages"}),"\n",(0,r.jsx)(n.li,{children:"GPU-accelerated perception pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Integration with NVIDIA's AI frameworks"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"isaac-sim-architecture",children:"Isaac Sim Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# isaac_sim_example.py\nfrom omni.isaac.kit import SimulationApp\nimport omni.isaac.core.utils.prims as prim_utils\nfrom omni.isaac.core import World\nfrom omni.isaac.core.robots import Robot\n\n# Start Isaac Sim application\nconfig = {"headless": False}\nsimulation_app = SimulationApp(config)\n\n# Create simulation world\nworld = World(stage_units_in_meters=1.0)\n\n# Add robot to simulation\nrobot = Robot(\n    prim_path="/World/Robot",\n    name="Robot",\n    usd_path="/Isaac/Robots/Carter/carter.usd"\n)\n\n# Add objects to environment\nprim_utils.define_prim("/World/Box", "Cube")\n\n# Simulation loop\nfor i in range(1000):\n    # Apply actions to robot\n    # Get sensor data\n    # Update world\n    world.step(render=True)\n\nsimulation_app.close()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"practical-implementation-creating-a-digital-twin",children:"Practical Implementation: Creating a Digital Twin"}),"\n",(0,r.jsx)(n.h3,{id:"complete-gazebo-simulation-example",children:"Complete Gazebo Simulation Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- advanced_humanoid.sdf --\x3e\n<?xml version="1.0" ?>\n<sdf version="1.7">\n  <world name="humanoid_world">\n    \x3c!-- Physics --\x3e\n    <physics type="ode">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <gravity>0 0 -9.8</gravity>\n    </physics>\n    \n    \x3c!-- Environment --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    <include>\n      <uri>model://sun</uri>\n    </include>\n    \n    \x3c!-- Simple humanoid model --\x3e\n    <model name="simple_humanoid">\n      <pose>0 0 1.0 0 0 0</pose>\n      \n      \x3c!-- Torso --\x3e\n      <link name="torso">\n        <inertial>\n          <mass>10.0</mass>\n          <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" izz="1.0"/>\n        </inertial>\n        <collision name="collision">\n          <geometry>\n            <box size="0.3 0.3 0.5"/>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box size="0.3 0.3 0.5"/>\n          </geometry>\n        </visual>\n      </link>\n      \n      \x3c!-- Head with camera --\x3e\n      <link name="head">\n        <pose>0 0 0.3 0 0 0</pose>\n        <inertial>\n          <mass>2.0</mass>\n          <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.1"/>\n        </inertial>\n        <visual name="visual">\n          <geometry>\n            <sphere radius="0.15"/>\n          </geometry>\n        </visual>\n        \n        \x3c!-- Camera sensor --\x3e\n        <sensor name="camera" type="camera">\n          <camera>\n            <horizontal_fov>1.047</horizontal_fov>\n            <image>\n              <width>640</width>\n              <height>480</height>\n              <format>R8G8B8</format>\n            </image>\n            <clip>\n              <near>0.1</near>\n              <far>300</far>\n            </clip>\n          </camera>\n          <always_on>1</always_on>\n          <update_rate>30</update_rate>\n          <visualize>true</visualize>\n        </sensor>\n      </link>\n      \n      \x3c!-- Joint connecting head to torso --\x3e\n      <joint name="neck_joint" type="revolute">\n        <parent>torso</parent>\n        <child>head</child>\n        <axis>\n          <xyz>0 1 0</xyz>\n          <limit>\n            <lower>-0.5</lower>\n            <upper>0.5</upper>\n          </limit>\n        </axis>\n      </joint>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-integration-with-gazebo-1",children:"ROS 2 Integration with Gazebo"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# humanoid_gazebo_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, JointState\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nimport tf2_ros\nimport numpy as np\n\nclass HumanoidGazeboBridge(Node):\n    def __init__(self):\n        super().__init__('humanoid_gazebo_bridge')\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.joint_pub = self.create_publisher(JointState, '/joint_states', 10)\n        \n        # Subscribers\n        self.image_sub = self.create_subscription(Image, '/camera/image_raw', self.image_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\n        self.odom_sub = self.create_subscription(Odometry, '/odom', self.odom_callback, 10)\n        \n        # TF broadcaster\n        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)\n        \n        # Control timer\n        self.control_timer = self.create_timer(0.02, self.control_callback)\n        \n        self.get_logger().info('Humanoid Gazebo Bridge initialized')\n\n    def image_callback(self, msg):\n        # Process camera image from simulation\n        self.get_logger().info('Received camera image')\n\n    def imu_callback(self, msg):\n        # Process IMU data from simulation\n        self.get_logger().info(f'IMU Orientation: ({msg.orientation.x}, {msg.orientation.y}, {msg.orientation.z})')\n\n    def odom_callback(self, msg):\n        # Process odometry data\n        self.get_logger().info(f'Robot Position: ({msg.pose.pose.position.x}, {msg.pose.pose.position.y})')\n\n    def control_callback(self):\n        # Implement humanoid control logic\n        # This would connect to your AI decision-making system\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bridge = HumanoidGazeboBridge()\n    rclpy.spin(bridge)\n    bridge.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"visualization-and-debugging-tools",children:"Visualization and Debugging Tools"}),"\n",(0,r.jsx)(n.h3,{id:"gazebo-tools",children:"Gazebo Tools"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"gzclient"}),": 3D visualization client"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"gzservers"}),": Physics simulation server"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Database"}),": Repository of pre-built robot models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Terrain Tools"}),": Environment creation utilities"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"unity-tools",children:"Unity Tools"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Omniverse Viewport"}),": Real-time rendering"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception Tools"}),": Synthetic data generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ML-Agents Interface"}),": Reinforcement learning dashboard"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physics Debugger"}),": Collision and joint visualization"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization-strategies",children:"Performance Optimization Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"1-simulation-efficiency",children:"1. Simulation Efficiency"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# simulation_optimization.py\nclass SimulationOptimizer:\n    def __init__(self):\n        self.sim_step_size = 0.001  # Balance between accuracy and speed\n        self.render_every_n_steps = 1  # Skip rendering for faster simulation\n        self.collision_detection = "fast_approx"  # Trade accuracy for speed\n        \n    def optimize_for_training(self):\n        """Optimize simulation for AI training (speed priority)"""\n        self.sim_step_size = 0.01  # Larger steps, faster simulation\n        self.render_every_n_steps = 10  # Render less frequently\n        self.collision_detection = "broad_phase"  # Fast collision detection\n        \n    def optimize_for_validation(self):\n        """Optimize simulation for validation (accuracy priority)"""\n        self.sim_step_size = 0.001  # Smaller steps, higher accuracy\n        self.render_every_n_steps = 1  # Render every step\n        self.collision_detection = "narrow_phase"  # Accurate collision detection\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-resource-management",children:"2. Resource Management"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Level of Detail (LOD)"}),": Reduce complexity for distant objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Occlusion Culling"}),": Skip rendering of hidden objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Texture Streaming"}),": Load textures on-demand"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physics Optimization"}),": Simplified collision meshes"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"digital-twin-validation",children:"Digital Twin Validation"}),"\n",(0,r.jsx)(n.h3,{id:"metrics-for-simulation-quality",children:"Metrics for Simulation Quality"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kinematic Accuracy"}),": How well the simulated robot matches real kinematics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic Fidelity"}),": How well simulated physics matches real physics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Precision"}),": How well simulated sensors match real sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Timing Accuracy"}),": How well simulation timing matches real-time operation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"validation-methodology",children:"Validation Methodology"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# validation_framework.py\nclass DigitalTwinValidator:\n    def __init__(self):\n        self.metrics = {\n            'kinematic_error': [],\n            'dynamic_error': [],\n            'sensor_error': [],\n            'timing_error': []\n        }\n    \n    def validate_kinematics(self, real_joints, sim_joints):\n        \"\"\"Validate joint position accuracy\"\"\"\n        error = np.mean(np.abs(real_joints - sim_joints))\n        self.metrics['kinematic_error'].append(error)\n        return error\n    \n    def validate_dynamics(self, real_forces, sim_forces):\n        \"\"\"Validate dynamic response\"\"\"\n        error = np.mean(np.abs(real_forces - sim_forces))\n        self.metrics['dynamic_error'].append(error)\n        return error\n    \n    def generate_report(self):\n        \"\"\"Generate validation report\"\"\"\n        report = {\n            'kinematic_accuracy': np.mean(self.metrics['kinematic_error']),\n            'dynamic_accuracy': np.mean(self.metrics['dynamic_error']),\n            'sensor_accuracy': np.mean(self.metrics['sensor_error']),\n            'timing_accuracy': np.mean(self.metrics['timing_error'])\n        }\n        return report\n"})}),"\n",(0,r.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,r.jsx)(n.p,{children:"Digital twins are fundamental to Physical AI development, providing safe, efficient, and cost-effective environments for testing and training robot systems. Gazebo excels at physics-based simulation with realistic sensor models, while Unity provides high-fidelity visualization and human-robot interaction capabilities. NVIDIA Isaac Sim represents the next generation, combining photorealistic rendering with AI integration for advanced Physical AI applications. Understanding simulation challenges, particularly the sim-to-real transfer problem, is crucial for developing robust Physical AI systems that operate effectively in the real world."}),"\n",(0,r.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Digital Twin"}),": Virtual replica of a physical robot system"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gazebo"}),": 3D simulation environment with physics and sensor simulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unity"}),": Game engine platform for high-fidelity visualization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA Isaac Sim"}),": Advanced simulation platform on Omniverse"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sim-to-Real Transfer"}),": Process of adapting simulation-trained models to real robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization"}),": Technique to improve sim-to-real transfer by randomizing simulation parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Simulation"}),": Modeling of real sensors in virtual environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synthetic Data"}),": Artificially generated data for AI training"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physics Simulation"}),": Modeling of physical laws in virtual environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS Integration"}),": Connecting simulation to ROS/ROS 2 communication framework"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"practice-questions",children:"Practice Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simulation Setup"}),": Design a Gazebo world file for a humanoid robot learning to walk on uneven terrain. Include appropriate physics parameters, sensors, and environmental obstacles."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion"}),": Create a ROS 2 node that combines data from a simulated camera, LiDAR, and IMU in Gazebo to estimate robot pose. Discuss the challenges and solutions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sim-to-Real Transfer"}),": Propose a domain randomization strategy to improve the transfer of a navigation model from simulation to reality. What parameters would you randomize?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance Optimization"}),": Design a simulation optimization strategy for two different use cases: (a) AI training with thousands of parallel simulations, and (b) detailed validation of robot control algorithms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Unity Integration"}),": Explain how you would integrate Unity perception tools with ROS 2 to generate synthetic training data for a computer vision model. What are the advantages and challenges?"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"reflection-questions",children:"Reflection Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"How do the trade-offs between simulation accuracy and computational speed affect the effectiveness of digital twins in Physical AI?"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"What are the ethical considerations when training AI systems entirely in simulation before deploying them in the real world?"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"How might advances in simulation technology change the development process for Physical AI systems in the next decade?"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Continue to ",(0,r.jsx)(n.a,{href:"/docs/chapter-4/ai-robot-brain-isaac",children:"Chapter 4: The AI-Robot Brain (NVIDIA Isaac\u2122)"})]})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},7561:(e,n,i)=>{i.d(n,{A:()=>s});var t=i(6540),r=i(4746),a=i(4848);const s=({chapterId:e,userId:n="1"})=>{const[i,s]=(0,t.useState)(!1),[o,l]=(0,t.useState)(!1),[c,d]=(0,t.useState)(""),[m,h]=(0,t.useState)(0),[u,p]=(0,t.useState)(""),[g,f]=(0,t.useState)(null),[x,y]=(0,t.useState)("intermediate"),b=(0,t.useRef)(null),v=(0,t.useRef)(null);(0,t.useEffect)(()=>{if("undefined"==typeof window)return;const e=()=>{const e=[".theme-doc-markdown","article",".markdown","main .container",'[class*="docItemContainer"]'];for(const n of e){const e=document.querySelector(n);if(e&&e.textContent&&e.textContent.length>50)return void(b.current=e)}};e();const n=new MutationObserver(()=>{b.current||e()}),i=document.querySelector("body");return i&&n.observe(i,{childList:!0,subtree:!0}),()=>{n.disconnect(),v.current&&v.current.abort()}},[e]);const j=()=>{const e=b.current;e&&c&&(e.style.transition="opacity 0.3s ease-in-out",e.style.opacity="0",setTimeout(()=>{e.innerHTML=c,e.style.opacity="1",l(!1),f(null)},300))},_=()=>{switch(x){case"beginner":return"#10b981";case"advanced":return"#8b5cf6";default:return"#3b82f6"}},z=()=>{switch(x){case"beginner":return"\ud83c\udf31";case"advanced":return"\ud83d\ude80";default:return"\ud83d\udcd6"}};return(0,a.jsxs)("div",{className:"custom-button-container",style:{margin:"1.5rem auto",padding:"1.5rem",border:"2px solid #e5e7eb",borderRadius:"16px",backgroundColor:"linear-gradient(-45deg,  #ff0055,  #7000ff,  #009c98, #000000)",boxShadow:"0 4px 6px -1px rgba(0, 0, 0, 0.1)",textAlign:"center",maxWidth:"700px",fontFamily:"system-ui, sans-serif"},children:[(0,a.jsxs)("div",{style:{display:"flex",flexDirection:"column",alignItems:"center",gap:"1rem"},children:[(0,a.jsxs)("div",{style:{display:"flex",gap:"1rem",alignItems:"center",flexWrap:"wrap",justifyContent:"center"},children:[(0,a.jsx)("label",{style:{fontSize:"0.875rem",fontWeight:"600",color:"#374151"},children:"Target Level:"}),(0,a.jsxs)("select",{value:x,onChange:e=>{y(e.target.value),o&&j()},disabled:i,style:{padding:"0.6rem 1rem",borderRadius:"8px",border:"2px solid "+(i?"#e5e7eb":"#d1d5db"),outline:"none",cursor:i?"not-allowed":"pointer"},children:[(0,a.jsx)("option",{value:"beginner",children:"\ud83c\udf31 Beginner (Simple)"}),(0,a.jsx)("option",{value:"intermediate",children:"\ud83d\udcd6 Intermediate (Standard)"}),(0,a.jsx)("option",{value:"advanced",children:"\ud83d\ude80 Advanced (Technical)"})]})]}),(0,a.jsx)("button",{onClick:async()=>{if(o)return void j();const i=b.current;if(i){s(!0),f(null),h(0),p(`Preparing to personalize for ${x}...`),c||d(i.innerHTML);try{const a=(e=>{const n=3e3,i=[];if(e.children.length>0){let t="";Array.from(e.children).forEach(e=>{const r=e.outerHTML;t.length+r.length>n&&t.length>0&&(i.push(t),t=""),t+=r}),t&&i.push(t)}else{const t=e.innerHTML;for(let e=0;e<t.length;e+=n)i.push(t.substring(e,e+n))}return i})(i);if(console.log(`Personalizing in ${a.length} chunks`),0===a.length)throw new Error("Page content is empty");let o="";v.current&&v.current.abort(),v.current=new AbortController;for(let i=0;i<a.length;i++){const s=a[i],l=Math.round(i/a.length*100);h(l),p(`Adapting section ${i+1} of ${a.length}...`);try{const i=await r.u.post("/api/v1/personalize",{content:s,user_id:n?parseInt(n):1,learning_level:x,chapter_context:e,content_type:"partial_chapter"});o+=i.personalized_content,await new Promise(e=>setTimeout(e,150))}catch(t){console.error(`Error in chunk ${i}:`,t),o+=s}}h(100),p("Finalizing changes..."),setTimeout(()=>{i&&(i.style.transition="opacity 0.3s ease-in-out",i.style.opacity="0",setTimeout(()=>{const e=`\n              <div class="alert alert--success margin-bottom--md" style="padding: 1rem; border-radius: 8px; background-color: ${_()}20; border: 1px solid ${_()}; margin-bottom: 20px;">\n                <strong>${z()} Content Adapted: ${x.charAt(0).toUpperCase()+x.slice(1)} Level</strong>\n                <div style="font-size: 0.85em; margin-top: 5px;">Content has been simplified and examples adjusted for your level.</div>\n              </div>\n            `;i.innerHTML=e+o,i.style.opacity="1"},300)),l(!0),s(!1),p("")},500)}catch(g){console.error("Personalization Error:",g);const n=g.response?.data?.detail||g.message||"Process failed";f(`Failed: ${n}`),s(!1),h(0)}}else f("Content to personalize not found.")},disabled:i,style:{padding:"1rem 2rem",backgroundColor:o?"#059669":_(),color:"white",border:"none",borderRadius:"12px",cursor:i?"wait":"pointer",fontWeight:"700",fontSize:"1.05rem",display:"flex",alignItems:"center",gap:"0.75rem",transition:"all 0.2s",opacity:i?.8:1,boxShadow:"0 2px 4px rgba(0,0,0,0.1)"},children:i?(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)("span",{style:{width:"18px",height:"18px",border:"3px solid rgba(255,255,255,0.3)",borderTop:"3px solid white",borderRadius:"50%",animation:"spin 1s linear infinite",display:"inline-block"}}),(0,a.jsx)("span",{children:"Processing..."})]}):o?(0,a.jsx)(a.Fragment,{children:"\u21a9\ufe0f Restore Original"}):(0,a.jsx)(a.Fragment,{children:"\u2728 Personalize Content"})}),i&&(0,a.jsxs)("div",{style:{width:"100%",marginTop:"0.5rem"},children:[(0,a.jsxs)("div",{style:{display:"flex",justifyContent:"space-between",fontSize:"0.9rem",color:"#666",marginBottom:"4px"},children:[(0,a.jsx)("span",{children:u}),(0,a.jsxs)("span",{children:[m,"%"]})]}),(0,a.jsx)("div",{style:{width:"100%",height:"6px",backgroundColor:"#e5e7eb",borderRadius:"3px",overflow:"hidden"},children:(0,a.jsx)("div",{style:{width:`${m}%`,height:"100%",backgroundColor:_(),transition:"width 0.3s ease"}})})]}),!i&&(0,a.jsx)("p",{style:{margin:0,fontSize:"0.9rem",color:g?"#dc2626":"#6b7280",marginTop:"0.5rem"},children:g?`\u26a0\ufe0f ${g}`:o?"\u2705 Content updated based on your preferences.":"AI will rewrite the content to match your selected difficulty level."})]}),(0,a.jsx)("style",{children:"\n        @keyframes spin { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }\n      "})]})}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var t=i(6540);const r={},a=t.createContext(r);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(a.Provider,{value:n},e.children)}},9815:(e,n,i)=>{i(6540),i(9345),i(4848)}}]);